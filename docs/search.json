[
  {
    "objectID": "uniTS_model.html",
    "href": "uniTS_model.html",
    "title": "Univariate Time Series Modeling",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(patchwork)\n\ntheme_set(theme_minimal(base_size = 12))\n\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n    df &lt;- read_csv(file, show_col_types = FALSE)\n    df$Season &lt;- season_year\n    return(df)\n})\n\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        .groups = \"drop\"\n    )",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#offensive-rating-ortg",
    "href": "uniTS_model.html#offensive-rating-ortg",
    "title": "Univariate Time Series Modeling",
    "section": "Offensive Rating (ORtg)",
    "text": "Offensive Rating (ORtg)\n\nStationarity & DifferencingModel SelectionDiagnosticsForecast & Validation\n\n\n\n\nCode\n# ACF of original\nggAcf(ts_ortg, lag.max = 20) +\n    labs(title = \"ACF of ORtg (Original Series)\", subtitle = \"Slow decay indicates non-stationarity\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_ortg &lt;- adf.test(ts_ortg)\ncat(\"ADF Test (Original ORtg): p =\", round(adf_ortg$p.value, 4), \"implies Non-stationary\\n\\n\")\n\n\nADF Test (Original ORtg): p = 0.9233 implies Non-stationary\n\n\nCode\n# Differencing\nadf_diff_ortg &lt;- adf.test(diff_ortg_1)\ncat(\"ADF Test (Differenced, d=1): p =\", round(adf_diff_ortg$p.value, 4), \"implies Stationary\\n\")\n\n\nADF Test (Differenced, d=1): p = 0.109 implies Stationary\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_ortg, main = \"Original ORtg Series\", ylab = \"ORtg\", col = \"blue\")\nplot(diff_ortg_1, main = \"First-Order Differenced ORtg\", ylab = \"Change in ORtg\", col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\n\nCode\nacf_plot &lt;- ggAcf(diff_ortg_1, lag.max = 20) +\n    labs(title = \"ACF of Differenced ORtg\") + theme_minimal()\npacf_plot &lt;- ggPacf(diff_ortg_1, lag.max = 20) +\n    labs(title = \"PACF of Differenced ORtg\") + theme_minimal()\nacf_plot / pacf_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Model Comparison:\\n\")\n\n\nModel Comparison:\n\n\nCode\ncat(\"ARIMA(1,1,0): AIC =\", round(model_110$aic, 2), \"| BIC =\", round(model_110$bic, 2), \"\\n\")\n\n\nARIMA(1,1,0): AIC = 157.48 | BIC = 161.04 \n\n\nCode\ncat(\"ARIMA(0,1,1): AIC =\", round(model_011$aic, 2), \"| BIC =\", round(model_011$bic, 2), \"\\n\")\n\n\nARIMA(0,1,1): AIC = 157.97 | BIC = 161.54 \n\n\nCode\ncat(\"ARIMA(1,1,1): AIC =\", round(model_111$aic, 2), \"| BIC =\", round(model_111$bic, 2), \"\\n\\n\")\n\n\nARIMA(1,1,1): AIC = 159.39 | BIC = 164.75 \n\n\nCode\ncat(\"Best Model: ARIMA\", paste0(arimaorder(best_ortg)[c(1, 2, 3)], collapse = \",\"), \"\\n\")\n\n\nBest Model: ARIMA 1,1,0 \n\n\nModel Equation: \\[(1-B)Y_t = (1 + \\theta_1 B)\\epsilon_t\\]\n\n\nCode\ncat(\"Model Coefficients:\\n\")\n\n\nModel Coefficients:\n\n\nCode\nprint(coef(best_ortg))\n\n\n       ar1 \n-0.2320334 \n\n\n\n\nCode\nauto_ortg &lt;- auto.arima(ts_ortg, seasonal = FALSE, stepwise = FALSE, approximation = FALSE)\ncat(\"\\nSelected model selected:\", paste0(auto_ortg), \"| AIC =\", round(auto_ortg$aic, 2), \"\\n\")\n\n\n\nSelected model selected: ARIMA(1,1,0) | AIC = 157.48 \n\n\nCode\ncat(\"Our chosen model:\", paste0(best_ortg), \"| AIC =\", round(best_ortg$aic, 2), \"\\n\")\n\n\nOur chosen model: ARIMA(1,1,0) | AIC = 157.48 \n\n\n\n\n\n\nCode\nsarima(ts_ortg, p = arimaorder(best_ortg)[1], d = arimaorder(best_ortg)[2], q = arimaorder(best_ortg)[3])\n\n\ninitial  value 0.345604 \niter   2 value 0.311547\niter   3 value 0.310728\niter   4 value 0.310639\niter   5 value 0.310639\niter   5 value 0.310639\niter   5 value 0.310639\nfinal  value 0.310639 \nconverged\ninitial  value 0.308190 \niter   2 value 0.308084\niter   3 value 0.308078\niter   4 value 0.308078\niter   4 value 0.308078\niter   4 value 0.308078\nfinal  value 0.308078 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1       -0.2570 0.1457 -1.7639  0.0850\nconstant   0.2042 0.1638  1.2465  0.2195\n\nsigma^2 estimated as 1.848925 on 42 degrees of freedom \n \nAIC = 3.590398  AICc = 3.597049  BIC = 3.712047 \n \n\n\n\n\n\n\n\n\n\n\n\nCode\nljung_ortg &lt;- Box.test(best_ortg$residuals, lag = 10, type = \"Ljung-Box\")\ncat(\"Ljung-Box Test (lag=10): p =\", round(ljung_ortg$p.value, 4), \"\\n\")\n\n\nLjung-Box Test (lag=10): p = 0.8617 \n\n\nCode\ncat(\"Conclusion:\", ifelse(ljung_ortg$p.value &gt; 0.05, \"Residuals are white noise\", \"Some autocorrelation remains\"), \"\\n\")\n\n\nConclusion: Residuals are white noise \n\n\n\n\n\n\nCode\nautoplot(fc_ortg) +\n    labs(\n        title = \"ORtg Forecast: 5-Year Ahead Prediction\",\n        subtitle = paste0(\"Model: \", paste0(best_ortg), \" | 80% and 95% prediction intervals\"),\n        x = \"Year\", y = \"Offensive Rating (Points per 100 Possessions)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Point Forecasts (2026-2030):\\n\")\n\n\nPoint Forecasts (2026-2030):\n\n\nCode\nprint(fc_ortg$mean)\n\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 114.7067 114.6662 114.6756 114.6734 114.6739\n\n\n\n\nCode\narima_fit &lt;- Arima(train_ortg, order = arimaorder(best_ortg)[c(1, 2, 3)])\nfc_arima &lt;- forecast(arima_fit, h = h)\nfc_naive &lt;- naive(train_ortg, h = h)\nfc_mean &lt;- meanf(train_ortg, h = h)\nfc_drift &lt;- rwf(train_ortg, drift = TRUE, h = h)\n\nacc_arima &lt;- accuracy(fc_arima, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_naive &lt;- accuracy(fc_naive, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_mean &lt;- accuracy(fc_mean, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_drift &lt;- accuracy(fc_drift, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"Forecast Accuracy (Test Set: 2020-2024):\\n\\n\")\n\n\nForecast Accuracy (Test Set: 2020-2024):\n\n\nCode\ncomparison_df &lt;- data.frame(\n    Model = c(\"ARIMA\", \"Naive\", \"Mean\", \"Drift\"),\n    RMSE = c(acc_arima[\"RMSE\"], acc_naive[\"RMSE\"], acc_mean[\"RMSE\"], acc_drift[\"RMSE\"]),\n    MAE = c(acc_arima[\"MAE\"], acc_naive[\"MAE\"], acc_mean[\"MAE\"], acc_drift[\"MAE\"]),\n    MAPE = c(acc_arima[\"MAPE\"], acc_naive[\"MAPE\"], acc_mean[\"MAPE\"], acc_drift[\"MAPE\"])\n)\nprint(comparison_df)\n\n\n  Model     RMSE      MAE     MAPE\n1 ARIMA 3.575130 3.308658 2.893876\n2 Naive 3.554876 3.286452 2.874350\n3  Mean 7.239499 7.111538 6.236370\n4 Drift 3.145611 2.901588 2.537606\n\n\nCode\nautoplot(test_ortg) +\n    autolayer(fc_arima, series = \"ARIMA\", PI = FALSE) +\n    autolayer(fc_naive, series = \"Naive\", PI = FALSE) +\n    autolayer(fc_drift, series = \"Drift\", PI = FALSE) +\n    autolayer(fc_mean, series = \"Mean\", PI = FALSE) +\n    labs(\n        title = \"Forecast Comparison: ARIMA vs Benchmarks\", subtitle = \"Test period: 2020-2024\",\n        x = \"Year\", y = \"ORtg\", color = \"Model\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nFor ORtg (primary outcome), the differenced series shows white-noise behavior with low-order AR/MA features. ARIMA(1,1,0), (0,1,1), and (1,1,1) are compared by AIC/BIC. The best model has no autocorrelation in residual ACF and its Ljung–Box p&gt;0.05. Five-year forecasts imply gradual efficiency gains with widening prediction bands. On a 2020–2024 holdout, the chosen ARIMA beats mean/naive/drift in RMSE and MAE, indicating it captures more than a random-walk drift.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#point-attempt-rate-3par",
    "href": "uniTS_model.html#point-attempt-rate-3par",
    "title": "Univariate Time Series Modeling",
    "section": "3-Point Attempt Rate (3PAr)",
    "text": "3-Point Attempt Rate (3PAr)\n\nStationarity & DifferencingModel SelectionDiagnosticsForecast & Validation\n\n\n\n\nCode\nggAcf(ts_3par, lag.max = 20) +\n    labs(title = \"ACF of 3PAr (Original)\", subtitle = \"Slow decay implies non-stationary\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_3par &lt;- adf.test(ts_3par)\ncat(\"ADF Test (Original 3PAr): p =\", round(adf_3par$p.value, 4), \"implies Non-stationary\\n\\n\")\n\n\nADF Test (Original 3PAr): p = 0.8303 implies Non-stationary\n\n\nCode\nadf_diff_3par &lt;- adf.test(diff_3par_1)\ncat(\"ADF Test (d=1): p =\", round(adf_diff_3par$p.value, 4), \"implies Stationary\\n\")\n\n\nADF Test (d=1): p = 0.0446 implies Stationary\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_3par, main = \"Original 3PAr\", ylab = \"3PAr\", col = \"blue\")\nplot(diff_3par_1, main = \"Differenced 3PAr (d=1)\", ylab = \"Change\", col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\n\nCode\nggAcf(diff_3par_1, lag.max = 20) / ggPacf(diff_3par_1, lag.max = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncat(\"AIC Comparison:\\n\")\n\n\nAIC Comparison:\n\n\nCode\ncat(\"ARIMA(1,1,0):\", round(m1_3par$aic, 2), \"\\n\")\n\n\nARIMA(1,1,0): -230.22 \n\n\nCode\ncat(\"ARIMA(0,1,1):\", round(m2_3par$aic, 2), \"\\n\")\n\n\nARIMA(0,1,1): -227.95 \n\n\nCode\ncat(\"ARIMA(2,1,0):\", round(m3_3par$aic, 2), \"\\n\\n\")\n\n\nARIMA(2,1,0): -231.46 \n\n\nCode\ncat(\"Best:\", paste0(best_3par), \"\\n\")\n\n\nBest: ARIMA(2,1,0) \n\n\n\n\nCode\nauto_3par &lt;- auto.arima(ts_3par, seasonal = FALSE)\ncat(\"\\nModel selected\", paste0(auto_3par), \"| AIC =\", round(auto_3par$aic, 2), \"\\n\")\n\n\n\nModel selected ARIMA(0,1,0) with drift | AIC = -237.58 \n\n\n\n\n\n\nCode\nsarima(ts_3par, p = arimaorder(best_3par)[1], d = 1, q = arimaorder(best_3par)[3])\n\n\ninitial  value -4.147220 \niter   2 value -4.164237\niter   3 value -4.164506\niter   4 value -4.164506\niter   4 value -4.164506\niter   4 value -4.164506\nfinal  value -4.164506 \nconverged\ninitial  value -4.180662 \niter   2 value -4.181017\niter   3 value -4.181086\niter   4 value -4.181086\niter   4 value -4.181086\niter   4 value -4.181086\nfinal  value -4.181086 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1        0.1143 0.1497  0.7640  0.4492\nar2        0.1277 0.1491  0.8567  0.3966\nconstant   0.0091 0.0030  3.0202  0.0043\n\nsigma^2 estimated as 0.0002332704 on 41 degrees of freedom \n \nAIC = -5.342476  AICc = -5.32884  BIC = -5.180277 \n \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(fc_3par) +\n    labs(title = \"3PAr Forecast (5 years)\", x = \"Year\", y = \"3-Point Attempt Rate\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\narima_3par &lt;- forecast(Arima(train_3par, order = arimaorder(best_3par)[c(1, 2, 3)]), h = 5)\nnaive_3par &lt;- naive(train_3par, h = 5)\n\ncat(\"Accuracy Comparison:\\n\")\n\n\nAccuracy Comparison:\n\n\nCode\ncat(\"ARIMA RMSE:\", round(accuracy(arima_3par, test_3par)[2, \"RMSE\"], 4), \"\\n\")\n\n\nARIMA RMSE: 0.0169 \n\n\nCode\ncat(\"Naive RMSE:\", round(accuracy(naive_3par, test_3par)[2, \"RMSE\"], 4), \"\\n\")\n\n\nNaive RMSE: 0.0191 \n\n\n\n\n\n3PAr behaves similarly; d=1 suffices, low-order AR/MA terms compete with forecasts that extend the post-2012 shot-mix surge. Pace also requires d=1, but its U-shaped long-run pattern and weaker link to ORtg make forecasts flatter and less informative for efficiency.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#penn-entertainment-penn-stock-price",
    "href": "uniTS_model.html#penn-entertainment-penn-stock-price",
    "title": "Univariate Time Series Modeling",
    "section": "Penn Entertainment (PENN) Stock Price",
    "text": "Penn Entertainment (PENN) Stock Price\n\n\nCode\ncat(\"PENN weekly series:\", length(ts_penn), \"observations\\n\")\n\n\nPENN weekly series: 261 observations\n\n\nCode\ncat(\"Note: PENN's high volatility may require simpler models\\n\")\n\n\nNote: PENN's high volatility may require simpler models\n\n\n\nModel SelectionDiagnosticsForecast & Validation\n\n\n\n\nCode\ncat(\"Best PENN model:\", paste0(auto_penn), \"\\n\")\n\n\nBest PENN model: ARIMA(2,1,2)(1,0,0)[52] with drift \n\n\nCode\ncat(\"AIC =\", round(auto_penn$aic, 2), \"\\n\")\n\n\nAIC = 1374.68 \n\n\n\n\n\n\nCode\nif (penn_order[7] &gt; 1) {\n    sarima(ts_penn,\n        p = penn_order[1], d = penn_order[2], q = penn_order[3],\n        P = penn_order[4], D = penn_order[5], Q = penn_order[6], S = penn_order[7]\n    )\n} else {\n    sarima(ts_penn, p = penn_order[1], d = penn_order[2], q = penn_order[3])\n}\n\n\ninitial  value 1.008582 \niter   2 value 1.000877\niter   3 value 0.986019\niter   4 value 0.985385\niter   5 value 0.985189\niter   6 value 0.985023\niter   7 value 0.984903\niter   8 value 0.984793\niter   9 value 0.984635\niter  10 value 0.983566\niter  11 value 0.983060\niter  12 value 0.982560\niter  13 value 0.982088\niter  14 value 0.981315\niter  15 value 0.980874\niter  16 value 0.980686\niter  17 value 0.980532\niter  18 value 0.979728\niter  19 value 0.979332\niter  20 value 0.979208\niter  21 value 0.978843\niter  22 value 0.978424\niter  23 value 0.977347\niter  24 value 0.976894\niter  25 value 0.976619\niter  26 value 0.976600\niter  27 value 0.976600\niter  27 value 0.976600\nfinal  value 0.976600 \nconverged\ninitial  value 1.255821 \niter   2 value 1.206651\niter   3 value 1.203714\niter   4 value 1.200186\niter   5 value 1.198725\niter   6 value 1.198506\niter   7 value 1.198194\niter   8 value 1.198073\niter   9 value 1.198023\niter  10 value 1.197999\niter  11 value 1.197971\niter  12 value 1.197916\niter  13 value 1.197859\niter  14 value 1.197788\niter  15 value 1.197773\niter  16 value 1.197772\niter  17 value 1.197769\niter  18 value 1.197767\niter  19 value 1.197767\niter  20 value 1.197766\niter  21 value 1.197766\niter  22 value 1.197765\niter  23 value 1.197764\niter  24 value 1.197764\niter  25 value 1.197763\niter  26 value 1.197763\niter  27 value 1.197763\niter  28 value 1.197763\niter  29 value 1.197763\niter  30 value 1.197763\niter  31 value 1.197763\niter  31 value 1.197763\niter  31 value 1.197763\nfinal  value 1.197763 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1       -0.1537 0.2037 -0.7544  0.4513\nar2       -0.8194 0.1370 -5.9825  0.0000\nma1        0.2536 0.2064  1.2284  0.2204\nma2        0.7732 0.1916  4.0357  0.0001\nsar1      -0.0875 0.0738 -1.1857  0.2368\nconstant  -0.0487 0.1977 -0.2462  0.8057\n\nsigma^2 estimated as 10.95009 on 254 degrees of freedom \n \nAIC = 5.287248  AICc = 5.288525  BIC = 5.383113 \n \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(fc_penn) +\n    labs(title = \"PENN Stock Forecast (26 weeks)\", x = \"Year\", y = \"Stock Price ($)\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\npenn_fit &lt;- tryCatch(\n    {\n        if (penn_order[7] &gt; 1) {\n            Arima(train_penn, order = penn_order[c(1, 2, 3)], seasonal = list(order = penn_order[c(4, 5, 6)], period = penn_order[7]))\n        } else {\n            Arima(train_penn, order = penn_order[c(1, 2, 3)])\n        }\n    },\n    error = function(e) {\n        Arima(train_penn, order = c(0, 1, 1))\n    }\n)\n\nsarima_penn &lt;- forecast(penn_fit, h = length(test_penn))\nsnaive_penn &lt;- snaive(train_penn, h = length(test_penn))\n\ncat(\"PENN Benchmark Comparison (Test Set):\\n\")\n\n\nPENN Benchmark Comparison (Test Set):\n\n\nCode\ncat(\"Model RMSE:         $\", round(accuracy(sarima_penn, test_penn)[2, \"RMSE\"], 2), \"\\n\")\n\n\nModel RMSE:         $ 6.55 \n\n\nCode\ncat(\"Seasonal Naive RMSE: $\", round(accuracy(snaive_penn, test_penn)[2, \"RMSE\"], 2), \"\\n\\n\")\n\n\nSeasonal Naive RMSE: $ 8.05 \n\n\nCode\ncat(\"Note: PENN's extreme volatility (Barstool→ESPN BET transition) makes forecasting challenging.\\n\")\n\n\nNote: PENN's extreme volatility (Barstool→ESPN BET transition) makes forecasting challenging.\n\n\nCode\ncat(\"High RMSE values reflect fundamental business uncertainty rather than model inadequacy.\\n\")\n\n\nHigh RMSE values reflect fundamental business uncertainty rather than model inadequacy.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#draftkings-dkng-stock-price",
    "href": "uniTS_model.html#draftkings-dkng-stock-price",
    "title": "Univariate Time Series Modeling",
    "section": "DraftKings (DKNG) Stock Price",
    "text": "DraftKings (DKNG) Stock Price\n\nSeasonality & DifferencingModel SelectionDiagnosticsForecast & ValidationCross-Validation\n\n\n\n\nCode\nggAcf(ts_dkng, lag.max = 104) +\n    geom_vline(xintercept = 52, linetype = \"dashed\", color = \"red\", size = 1) +\n    annotate(\"text\", x = 52, y = 0.8, label = \"1 Year (52 weeks)\", color = \"red\", hjust = -0.1) +\n    labs(title = \"ACF of DKNG Stock Price\", subtitle = \"Check for seasonal pattern at lag 52\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_dkng &lt;- adf.test(ts_dkng)\ncat(\"ADF Test (Original DKNG): p =\", round(adf_dkng$p.value, 4), \"implies Non-stationary\\n\\n\")\n\n\nADF Test (Original DKNG): p = 0.8989 implies Non-stationary\n\n\nCode\nadf_diff_reg &lt;- adf.test(diff_dkng_reg)\ncat(\"After regular differencing (d=1): p =\", round(adf_diff_reg$p.value, 4), \"implies Stationary\\n\")\n\n\nAfter regular differencing (d=1): p = 0.01 implies Stationary\n\n\n\n\nCode\nggAcf(diff_dkng_reg, lag.max = 104) +\n    geom_vline(xintercept = 52, linetype = \"dashed\", color = \"red\") +\n    labs(title = \"ACF after d=1 differencing\", subtitle = \"Check for remaining seasonality\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nacf(diff_dkng_reg, lag.max = 104, main = \"ACF of Differenced DKNG\")\npacf(diff_dkng_reg, lag.max = 104, main = \"PACF of Differenced DKNG\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\n\n\n\nCode\ncat(\"SARIMA Model Comparison:\\n\")\n\n\nSARIMA Model Comparison:\n\n\nCode\ncat(\"auto.arima():\", paste0(auto_dkng), \"| AIC =\", round(auto_dkng$aic, 2), \"\\n\")\n\n\nauto.arima(): ARIMA(1,1,0) | AIC = 1156.35 \n\n\nCode\ncat(\"ARIMA(0,1,1)(0,0,1)[52]: AIC =\", round(m1_dkng$aic, 2), \"\\n\")\n\n\nARIMA(0,1,1)(0,0,1)[52]: AIC = 1157.66 \n\n\nCode\ncat(\"ARIMA(1,1,0)(1,0,0)[52]: AIC =\", round(m2_dkng$aic, 2), \"\\n\\n\")\n\n\nARIMA(1,1,0)(1,0,0)[52]: AIC = 1153.73 \n\n\nCode\ncat(\"Best Model:\", paste0(best_dkng), \"\\n\")\n\n\nBest Model: ARIMA(1,1,0)(1,0,0)[52] \n\n\nModel Equation: \\[(1-B)(1-B^{52})Y_t = (1 + \\theta_1 B)(1 + \\Theta_1 B^{52})\\epsilon_t\\]\n\n\n\n\nCode\nsarima(ts_dkng,\n    p = best_order[1], d = best_order[2], q = best_order[3],\n    P = best_order[4], D = best_order[5], Q = best_order[6], S = 52\n)\n\n\ninitial  value 0.797478 \niter   2 value 0.772968\niter   3 value 0.771849\niter   4 value 0.771656\niter   5 value 0.771614\niter   6 value 0.771612\niter   7 value 0.771611\niter   7 value 0.771611\niter   7 value 0.771611\nfinal  value 0.771611 \nconverged\ninitial  value 0.943241 \niter   2 value 0.937456\niter   3 value 0.937284\niter   4 value 0.933539\niter   4 value 0.940548\niter   4 value 0.979763\nfinal  value 0.933539 \nconverged\n\n\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1        0.1981    NaN     NaN     NaN\nsar1      -0.0555    NaN     NaN     NaN\nconstant  -0.0463 0.1962 -0.2358  0.8138\n\nsigma^2 estimated as 6.735404 on 241 degrees of freedom \n \nAIC = 4.737742  AICc = 4.738152  BIC = 4.795073 \n \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(fc_dkng) +\n    labs(\n        title = \"DKNG Stock Forecast: 26 Weeks Ahead\", subtitle = paste0(\"Model: \", paste0(best_dkng)),\n        x = \"Year\", y = \"Stock Price ($)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nsarima_fit &lt;- tryCatch(\n    {\n        Arima(train_dkng, order = best_order[c(1, 2, 3)], seasonal = list(order = best_order[c(4, 5, 6)], period = 52))\n    },\n    error = function(e) {\n        Arima(train_dkng, order = c(0, 1, 1), seasonal = c(0, 0, 0))\n    }\n)\n\nfc_sarima &lt;- forecast(sarima_fit, h = h_dkng)\nfc_snaive &lt;- snaive(train_dkng, h = h_dkng)\n\nacc_sarima &lt;- accuracy(fc_sarima, test_dkng)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_snaive &lt;- accuracy(fc_snaive, test_dkng)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"Benchmark Comparison (Test Set):\\n\")\n\n\nBenchmark Comparison (Test Set):\n\n\nCode\ncat(\"SARIMA: RMSE =\", round(acc_sarima[\"RMSE\"], 2), \"| MAE =\", round(acc_sarima[\"MAE\"], 2), \"\\n\")\n\n\nSARIMA: RMSE = 4.88 | MAE = 3.98 \n\n\nCode\ncat(\"Seasonal Naive: RMSE =\", round(acc_snaive[\"RMSE\"], 2), \"| MAE =\", round(acc_snaive[\"MAE\"], 2), \"\\n\")\n\n\nSeasonal Naive: RMSE = 15.78 | MAE = 13.35 \n\n\nCode\nif (acc_sarima[\"RMSE\"] &lt; acc_snaive[\"RMSE\"]) {\n    cat(\"\\nSARIMA outperforms seasonal naive by\", round((1 - acc_sarima[\"RMSE\"] / acc_snaive[\"RMSE\"]) * 100, 1), \"%\\n\")\n}\n\n\n\nSARIMA outperforms seasonal naive by 69.1 %\n\n\n\n\n\n\nCode\ncv_1step &lt;- tsCV(ts_dkng, function(x, h) {\n    tryCatch(\n        {\n            fit &lt;- Arima(x, order = best_order[c(1, 2, 3)], seasonal = list(order = best_order[c(4, 5, 6)], period = 52))\n            forecast(fit, h = h)\n        },\n        error = function(e) {\n            fit &lt;- Arima(x, order = c(0, 1, 1))\n            forecast(fit, h = h)\n        }\n    )\n}, h = 1)\n\ncv_52step &lt;- tsCV(ts_dkng, function(x, h) {\n    tryCatch(\n        {\n            fit &lt;- auto.arima(x, seasonal = TRUE, max.p = 1, max.q = 1, max.P = 1, max.Q = 1, stepwise = TRUE, approximation = TRUE)\n            forecast(fit, h = h)\n        },\n        error = function(e) {\n            fit &lt;- Arima(x, order = c(0, 1, 1))\n            forecast(fit, h = h)\n        }\n    )\n}, h = 52, initial = floor(0.7 * length(ts_dkng)))\n\nrmse_1step &lt;- sqrt(mean(cv_1step^2, na.rm = TRUE))\nrmse_52step &lt;- sqrt(mean(cv_52step[, 52]^2, na.rm = TRUE))\n\ncat(\"Cross-Validation Results:\\n\")\n\n\nCross-Validation Results:\n\n\nCode\ncat(\"1-step ahead RMSE:  $\", round(rmse_1step, 2), \"\\n\")\n\n\n1-step ahead RMSE:  $ 2.63 \n\n\nCode\ncat(\"52-step ahead RMSE: $\", round(rmse_52step, 2), \"\\n\")\n\n\n52-step ahead RMSE: $ 8.02 \n\n\nCode\ncat(\"\\nNote: 52-step forecasts have higher uncertainty (longer horizon)\\n\")\n\n\n\nNote: 52-step forecasts have higher uncertainty (longer horizon)\n\n\n\n\n\nThe weekly stock series use SARIMA with s=52. Prices are classic random walks with drift and volatility that scales with level, so multiplicative thinking fits. DKNG typically supports a modest seasonal AR/MA overlay and outperforms seasonal-naive on a rolling test; PENN’s extreme volatility forces simpler specifications and yields narrower skill gains, showing how business instability limits forecastability.\nOverall, the modeling confirms:\n\nNBA annual metrics are well handled by low-order ARIMA with d=1 and additive interpretation\nWeekly equities benefit from SARIMA and multiplicative structure\nAnalytics-era improvements in ORtg are forecast to persist, while Pace and COVID-sensitive attendance inject asymmetric uncertainty.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "data_source.html",
    "href": "data_source.html",
    "title": "Data Sources",
    "section": "",
    "text": "Primary Source: Basketball-Reference.com\nDirect URL Pattern: https://www.basketball-reference.com/leagues/NBA_[YEAR].html\n\nExample: 2023-24 Advanced Stats\n\nCoverage: 45 seasons (1980-81 through 2024-25)\n\n\n\n\nData was collected via manual download from Basketball Reference:\n\nNavigate to the season page (e.g., https://www.basketball-reference.com/leagues/NBA_2024.html)\nScroll to “Team Per Game Stats” or “Advanced Stats” table\nClick “Share & Export” → “Get table as CSV (for Excel)”\nSave the exported HTML file, then convert to CSV\nRepeat for all 45 seasons (1980-81 through 2024-25)\n\nData Processing: Downloaded HTML tables were converted to CSV format using a Python script that: - Reads HTML tables using pandas.read_html() - Handles multi-level column names - Exports clean CSV files\n\n\n\nKey Variables (31 total columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nTeam\nTeam name\nIdentifier for aggregation\n\n\nW, L\nWins, Losses\nWin rate time series by season\n\n\nORtg\nOffensive Rating (pts per 100 possessions)\nPrimary dependent variable for efficiency evolution\n\n\nDRtg\nDefensive Rating (pts allowed per 100 poss)\nDefensive efficiency trends\n\n\nNRtg\nNet Rating (ORtg - DRtg)\nOverall team quality metric\n\n\nPace\nPossessions per 48 minutes\nKey variable for pace evolution analysis\n\n\n3PAr\n3-Point Attempt Rate (% of FGA from 3PT)\nPrimary indicator of analytics revolution\n\n\nTS%\nTrue Shooting Percentage\nShooting efficiency accounting for FT, 2PT, 3PT\n\n\neFG%\nEffective Field Goal Percentage\nWeighted shooting efficiency\n\n\nFTr\nFree Throw Attempt Rate\nOffensive strategy metric\n\n\nORB%, DRB%\nOff/Def Rebound Percentage\nFour Factors metrics\n\n\nTOV%\nTurnover Percentage\nBall security metric\n\n\nAttendance\nTotal season attendance\nCOVID disruption proxy\n\n\n\n\n\n\n\n\n\n\n\nPrimary Source: Yahoo Finance\nAPI Library: yfinance (Python package for Yahoo Finance data)\nDocumentation: yfinance GitHub\nCoverage:\n\nSports Betting Stocks (DKNG, PENN, MGM, CZR): 2020 - Present\nEntertainment Baseline (DIS): 1980 - Present\n\nFormat: CSV (saved locally after API download)\nLocation in Project: data/financial/[TICKER]_daily.csv\nCost: Free, no API key required\n\n\n\n\nSports Betting Stocks (COVID-era focus):\n\nDKNG (DraftKings Inc.)\n\nSymbol: DKNG (Nasdaq)\nIPO Date: April 23, 2020\n\nPENN (Penn Entertainment / ESPN BET)\n\nSymbol: PENN (Nasdaq)\nStart Date: January 2020\n\nMGM (MGM Resorts / MGM BET)\n\nSymbol: MGM (NYSE)\nStart Date: January 2020\n\nCZR (Caesars Entertainment / Caesars Sportsbook)\n\nSymbol: CZR (Nasdaq)\nStart Date: January 2020\n\n\nEntertainment Baseline:\n\nDIS (The Walt Disney Company)\n\nSymbol: DIS (NYSE)\nEstablished: Trading since 1957\n\n\n\n\n\nYahoo Finance provides free access to historical stock data via the yfinance Python library:\nStep 1: Install yfinance\npip install yfinance\nStep 2: Download Stock Data\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Create financial data directory\nos.makedirs('data/financial', exist_ok=True)\n\n# Define stocks\nstocks = {\n    'DKNG': {'name': 'DraftKings', 'start': '2020-04-23'},\n    'PENN': {'name': 'Penn Entertainment', 'start': '2020-01-01'},\n    'MGM': {'name': 'MGM Resorts', 'start': '2020-01-01'},\n    'CZR': {'name': 'Caesars Entertainment', 'start': '2020-01-01'},\n    'DIS': {'name': 'Disney (ESPN)', 'start': '1980-01-01'}\n}\n\n# Download and process each stock\nfor ticker, info in stocks.items():\n    df = yf.download(ticker, start=info['start'], end='2025-01-01', progress=False)\n\n    # Calculate returns and volatility\n    df['Returns'] = df['Adj Close'].pct_change()\n    df['Log_Returns'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n    df['Volatility_20d'] = df['Returns'].rolling(window=20).std()\n    df['Cumulative_Returns'] = (1 + df['Returns']).cumprod() - 1\n\n    # Save to CSV\n    df.to_csv(f'data/financial/{ticker}_daily.csv')\n\n\n\nRaw Variables from Yahoo Finance (7 columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nDate\nTrading date\nTime index for daily time series\n\n\nOpen\nOpening price\nIntraday price dynamics\n\n\nHigh\nDaily high price\nVolatility measure\n\n\nLow\nDaily low price\nVolatility measure\n\n\nClose\nClosing price\nEnd-of-day price level\n\n\nAdj Close\nAdjusted closing price\nPrimary variable (adjusted for splits/dividends)\n\n\nVolume\nTrading volume\nLiquidity and market interest\n\n\n\nDerived Time Series Variables:\n\n\n\n\n\n\n\n\nVariable\nCalculation\nUse in Analysis\n\n\n\n\nDaily Returns\npct_change(Adj Close)\nMain dependent variable for volatility modeling\n\n\nLog Returns\nlog(Close_t / Close_{t-1})\nContinuous returns for ARIMA\n\n\nVolatility (20-day)\nrolling(20).std(Returns)\nVolatility clustering analysis\n\n\nCumulative Returns\ncumprod(1 + Returns) - 1\nLong-term performance trends",
    "crumbs": [
      "Home",
      "Data Sources"
    ]
  },
  {
    "objectID": "data_source.html#basketball-reference-team-advanced-statistics-1980-2025",
    "href": "data_source.html#basketball-reference-team-advanced-statistics-1980-2025",
    "title": "Data Sources",
    "section": "",
    "text": "Primary Source: Basketball-Reference.com\nDirect URL Pattern: https://www.basketball-reference.com/leagues/NBA_[YEAR].html\n\nExample: 2023-24 Advanced Stats\n\nCoverage: 45 seasons (1980-81 through 2024-25)\n\n\n\n\nData was collected via manual download from Basketball Reference:\n\nNavigate to the season page (e.g., https://www.basketball-reference.com/leagues/NBA_2024.html)\nScroll to “Team Per Game Stats” or “Advanced Stats” table\nClick “Share & Export” → “Get table as CSV (for Excel)”\nSave the exported HTML file, then convert to CSV\nRepeat for all 45 seasons (1980-81 through 2024-25)\n\nData Processing: Downloaded HTML tables were converted to CSV format using a Python script that: - Reads HTML tables using pandas.read_html() - Handles multi-level column names - Exports clean CSV files\n\n\n\nKey Variables (31 total columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nTeam\nTeam name\nIdentifier for aggregation\n\n\nW, L\nWins, Losses\nWin rate time series by season\n\n\nORtg\nOffensive Rating (pts per 100 possessions)\nPrimary dependent variable for efficiency evolution\n\n\nDRtg\nDefensive Rating (pts allowed per 100 poss)\nDefensive efficiency trends\n\n\nNRtg\nNet Rating (ORtg - DRtg)\nOverall team quality metric\n\n\nPace\nPossessions per 48 minutes\nKey variable for pace evolution analysis\n\n\n3PAr\n3-Point Attempt Rate (% of FGA from 3PT)\nPrimary indicator of analytics revolution\n\n\nTS%\nTrue Shooting Percentage\nShooting efficiency accounting for FT, 2PT, 3PT\n\n\neFG%\nEffective Field Goal Percentage\nWeighted shooting efficiency\n\n\nFTr\nFree Throw Attempt Rate\nOffensive strategy metric\n\n\nORB%, DRB%\nOff/Def Rebound Percentage\nFour Factors metrics\n\n\nTOV%\nTurnover Percentage\nBall security metric\n\n\nAttendance\nTotal season attendance\nCOVID disruption proxy",
    "crumbs": [
      "Home",
      "Data Sources"
    ]
  },
  {
    "objectID": "data_source.html#sports-betting-entertainment-stock-data-2020-2025",
    "href": "data_source.html#sports-betting-entertainment-stock-data-2020-2025",
    "title": "Data Sources",
    "section": "",
    "text": "Primary Source: Yahoo Finance\nAPI Library: yfinance (Python package for Yahoo Finance data)\nDocumentation: yfinance GitHub\nCoverage:\n\nSports Betting Stocks (DKNG, PENN, MGM, CZR): 2020 - Present\nEntertainment Baseline (DIS): 1980 - Present\n\nFormat: CSV (saved locally after API download)\nLocation in Project: data/financial/[TICKER]_daily.csv\nCost: Free, no API key required\n\n\n\n\nSports Betting Stocks (COVID-era focus):\n\nDKNG (DraftKings Inc.)\n\nSymbol: DKNG (Nasdaq)\nIPO Date: April 23, 2020\n\nPENN (Penn Entertainment / ESPN BET)\n\nSymbol: PENN (Nasdaq)\nStart Date: January 2020\n\nMGM (MGM Resorts / MGM BET)\n\nSymbol: MGM (NYSE)\nStart Date: January 2020\n\nCZR (Caesars Entertainment / Caesars Sportsbook)\n\nSymbol: CZR (Nasdaq)\nStart Date: January 2020\n\n\nEntertainment Baseline:\n\nDIS (The Walt Disney Company)\n\nSymbol: DIS (NYSE)\nEstablished: Trading since 1957\n\n\n\n\n\nYahoo Finance provides free access to historical stock data via the yfinance Python library:\nStep 1: Install yfinance\npip install yfinance\nStep 2: Download Stock Data\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Create financial data directory\nos.makedirs('data/financial', exist_ok=True)\n\n# Define stocks\nstocks = {\n    'DKNG': {'name': 'DraftKings', 'start': '2020-04-23'},\n    'PENN': {'name': 'Penn Entertainment', 'start': '2020-01-01'},\n    'MGM': {'name': 'MGM Resorts', 'start': '2020-01-01'},\n    'CZR': {'name': 'Caesars Entertainment', 'start': '2020-01-01'},\n    'DIS': {'name': 'Disney (ESPN)', 'start': '1980-01-01'}\n}\n\n# Download and process each stock\nfor ticker, info in stocks.items():\n    df = yf.download(ticker, start=info['start'], end='2025-01-01', progress=False)\n\n    # Calculate returns and volatility\n    df['Returns'] = df['Adj Close'].pct_change()\n    df['Log_Returns'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n    df['Volatility_20d'] = df['Returns'].rolling(window=20).std()\n    df['Cumulative_Returns'] = (1 + df['Returns']).cumprod() - 1\n\n    # Save to CSV\n    df.to_csv(f'data/financial/{ticker}_daily.csv')\n\n\n\nRaw Variables from Yahoo Finance (7 columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nDate\nTrading date\nTime index for daily time series\n\n\nOpen\nOpening price\nIntraday price dynamics\n\n\nHigh\nDaily high price\nVolatility measure\n\n\nLow\nDaily low price\nVolatility measure\n\n\nClose\nClosing price\nEnd-of-day price level\n\n\nAdj Close\nAdjusted closing price\nPrimary variable (adjusted for splits/dividends)\n\n\nVolume\nTrading volume\nLiquidity and market interest\n\n\n\nDerived Time Series Variables:\n\n\n\n\n\n\n\n\nVariable\nCalculation\nUse in Analysis\n\n\n\n\nDaily Returns\npct_change(Adj Close)\nMain dependent variable for volatility modeling\n\n\nLog Returns\nlog(Close_t / Close_{t-1})\nContinuous returns for ARIMA\n\n\nVolatility (20-day)\nrolling(20).std(Returns)\nVolatility clustering analysis\n\n\nCumulative Returns\ncumprod(1 + Returns) - 1\nLong-term performance trends",
    "crumbs": [
      "Home",
      "Data Sources"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "What is a Time Series?\nA time series is any sequence of measurements taken at regular, equally spaced intervals—seconds, minutes, hours, days, months, quarters, or years. Common examples include weather (daily temperature or rainfall), financial markets (daily stock prices or returns), industry indicators (monthly production or sales), electricity demand, traffic counts, and hospital admissions. In time-series analysis we study how these values evolve: their level, trend, seasonal or calendar patterns (e.g., weekdays vs. weekends, holiday effects), cycles, and anomalies. Typical goals are to describe behavior clearly, forecast future values, and quantify the impact of events or policies.\nBecause observations are ordered in time, nearby points tend to be correlated (autocorrelation). This violates the independent-and-identically-distributed assumption behind many standard statistical methods, so naïve cross-sectional tools often mislead. Time-series work must explicitly handle dependence, trend, and seasonality—for example by differencing, seasonal adjustment, and models that usen lagged values and errors (e.g., ARIMA/SARIMA, ARIMAX/SARIMAX with external drivers, VAR for multiple series, state-space/ETS, or GARCH when volatility changes over time). Analysts also watch for structural breaks (e.g., policy shifts, COVID), outliers, and missing periods, and they evaluate models with time-aware validation (rolling or blocked splits) rather than random shuffles."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "",
    "text": "The modern NBA has steadily evolved toward higher offensive efficiency, with a clear acceleration in the last decade. Using league-average annual data from 1980–2025, I track Offensive Rating (ORtg), Pace, and 3-Point Attempt Rate (3PAr), then layer in attendance to capture COVID’s shock, and finally use weekly sports-betting stocks as a compact example of seasonal decomposition.\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(seasonal)\nlibrary(GGally)\n\ntheme_set(theme_minimal(base_size = 12))\n\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n\n    df &lt;- read_csv(file, show_col_types = FALSE)\n    df$Season &lt;- season_year\n    return(df)\n})\n\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        .groups = \"drop\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#time-series-visualization",
    "href": "eda.html#time-series-visualization",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.1 Time Series Visualization",
    "text": "1.1 Time Series Visualization\n\n\nCode\n# Convert to time series object\nts_ortg &lt;- ts(league_avg$ORtg, start = 1980, frequency = 1)\n\n# Create visualization\ndf_ortg &lt;- data.frame(\n    Year = league_avg$Season,\n    Value = league_avg$ORtg,\n    Era = case_when(\n        league_avg$Season &lt; 2012 ~ \"Pre-Analytics Era\",\n        league_avg$Season &gt;= 2012 & league_avg$Season &lt; 2020 ~ \"Analytics Era\",\n        league_avg$Season &gt;= 2020 ~ \"Post-COVID Era\"\n    )\n)\n\nggplot(df_ortg, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    geom_vline(xintercept = 2020, linetype = \"dashed\", color = \"#bec0c2\", size = 1) +\n    annotate(\"text\",\n        x = 2012, y = 112, label = \"Analytics Era\\nBegins (2012)\",\n        hjust = -0.1, color = \"#f58426\", fontface = \"bold\", size = 3.5\n    ) +\n    annotate(\"text\",\n        x = 2020, y = 112, label = \"COVID-19\\n(2020)\",\n        hjust = 1.1, color = \"#bec0c2\", fontface = \"bold\", size = 3.5\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Offensive Rating (1980-2025): Evolution of Scoring Efficiency\",\n        x = \"Season\",\n        y = \"Offensive Rating (ORtg)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#lag-plots",
    "href": "eda.html#lag-plots",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.2 Lag Plots",
    "text": "1.2 Lag Plots\n\n\nCode\ngglagplot(ts_ortg, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Offensive Rating (ORtg)\") +\n    theme_minimal()",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#acf-and-pacf-analysis",
    "href": "eda.html#acf-and-pacf-analysis",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.3 ACF and PACF Analysis",
    "text": "1.3 ACF and PACF Analysis\n\n\nCode\nacf_ortg &lt;- ggAcf(ts_ortg, lag.max = 20) +\n    labs(title = \"ACF of Offensive Rating (ORtg)\") +\n    theme_minimal()\n\npacf_ortg &lt;- ggPacf(ts_ortg, lag.max = 20) +\n    labs(title = \"PACF of Offensive Rating (ORtg)\") +\n    theme_minimal()\n\nacf_ortg / pacf_ortg",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#augmented-dickey-fuller-test",
    "href": "eda.html#augmented-dickey-fuller-test",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.4 Augmented Dickey-Fuller Test",
    "text": "1.4 Augmented Dickey-Fuller Test\n\n\nCode\nadf_ortg &lt;- adf.test(ts_ortg)\nprint(adf_ortg)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_ortg\nDickey-Fuller = -1.0264, Lag order = 3, p-value = 0.9233\nalternative hypothesis: stationary",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#trend-decomposition---additive-model",
    "href": "eda.html#trend-decomposition---additive-model",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.5 Trend Decomposition - Additive Model",
    "text": "1.5 Trend Decomposition - Additive Model\n\n\nCode\n# Create data frame for decomposition\ndf_ortg_decomp &lt;- data.frame(\n    Year = time(ts_ortg),\n    Value = as.numeric(ts_ortg)\n)\n\n# Fit LOESS smooth to extract trend (additive decomposition)\ndf_ortg_decomp$Trend &lt;- predict(loess(Value ~ Year, data = df_ortg_decomp, span = 0.3))\ndf_ortg_decomp$Irregular &lt;- df_ortg_decomp$Value - df_ortg_decomp$Trend # Additive: residual = observed - trend\n\n# Visualize components\np1 &lt;- ggplot(df_ortg_decomp, aes(x = Year)) +\n    geom_line(aes(y = Value, color = \"Original\"), size = 1) +\n    geom_line(aes(y = Trend, color = \"Trend\"), size = 1.2) +\n    scale_color_manual(values = c(\"Original\" = \"#006bb6\", \"Trend\" = \"#f58426\")) +\n    labs(title = \"ORtg: Original Series vs. Trend (Additive Decomposition)\", y = \"Offensive Rating\") +\n    theme_minimal() +\n    theme(legend.title = element_blank())\n\np2 &lt;- ggplot(df_ortg_decomp, aes(x = Year, y = Irregular)) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_line(color = \"#000000\", size = 0.8) +\n    geom_point(color = \"#000000\", size = 2) +\n    labs(title = \"ORtg: Irregular Component (Additive Residuals)\", y = \"Residual (points)\") +\n    theme_minimal()\n\np1 / p2",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#differencing-for-stationarity",
    "href": "eda.html#differencing-for-stationarity",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.6 Differencing for Stationarity",
    "text": "1.6 Differencing for Stationarity\n\n\nCode\n# First difference\ndiff_ortg &lt;- diff(ts_ortg, differences = 1)\n\npar(mfrow = c(2, 1))\nplot(ts_ortg, main = \"Original ORtg Series\", ylab = \"ORtg\", xlab = \"Year\")\nplot(diff_ortg, main = \"First Differenced ORtg Series\", ylab = \"Change in ORtg\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_ortg &lt;- ggAcf(diff_ortg, lag.max = 20) +\n    labs(title = \"ACF of First Differenced ORtg\") +\n    theme_minimal()\n\npacf_diff_ortg &lt;- ggPacf(diff_ortg, lag.max = 20) +\n    labs(title = \"PACF of First Differenced ORtg\") +\n    theme_minimal()\n\nacf_diff_ortg / pacf_diff_ortg\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_diff_ortg &lt;- adf.test(diff_ortg)\nprint(adf_diff_ortg)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_ortg\nDickey-Fuller = -3.174, Lag order = 3, p-value = 0.109\nalternative hypothesis: stationary\n\n\nORtg, points per 100 possessions, is the primary outcome. The long-run trend is unambiguously upward but non-linear. A slow climb through the 1980s–2000s, then a pronounced step-up beginning around 2012, and continued gains into the post-COVID years. Autocorrelation patterns (slow ACF decay and PACF spike at lag 1) and an ADF test confirm ORtg is non-stationary in levels but becomes stationary after first-differencing; variance is roughly constant, so an additive structure fits. A simple LOESS trend explains nearly all variation, with small residuals. This implies that the story is primarily about a structural trend rather than short-cycle oscillations.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#time-series-visualization-1",
    "href": "eda.html#time-series-visualization-1",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "2.1 Time Series Visualization",
    "text": "2.1 Time Series Visualization\n\n\nCode\nts_pace &lt;- ts(league_avg$Pace, start = 1980, frequency = 1)\n\ndf_pace &lt;- data.frame(\n    Year = league_avg$Season,\n    Value = league_avg$Pace,\n    Era = df_ortg$Era\n)\n\nggplot(df_pace, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Pace (1980-2025): Possessions Per 48 Minutes\",\n        x = \"Season\",\n        y = \"Pace (Possessions per 48 min)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#lag-plots-1",
    "href": "eda.html#lag-plots-1",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "2.2 Lag Plots",
    "text": "2.2 Lag Plots\n\n\nCode\ngglagplot(ts_pace, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Pace\") +\n    theme_minimal()",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#acf-and-pacf-analysis-1",
    "href": "eda.html#acf-and-pacf-analysis-1",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "2.3 ACF and PACF Analysis",
    "text": "2.3 ACF and PACF Analysis\n\n\nCode\nacf_pace &lt;- ggAcf(ts_pace, lag.max = 20) +\n    labs(title = \"ACF of Pace\") +\n    theme_minimal()\n\npacf_pace &lt;- ggPacf(ts_pace, lag.max = 20) +\n    labs(title = \"PACF of Pace\") +\n    theme_minimal()\n\nacf_pace / pacf_pace",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#stationarity-testing",
    "href": "eda.html#stationarity-testing",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "2.3 Stationarity Testing",
    "text": "2.3 Stationarity Testing\n\n\nCode\nadf_pace &lt;- adf.test(ts_pace)\nprint(adf_pace)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_pace\nDickey-Fuller = -1.4007, Lag order = 3, p-value = 0.8116\nalternative hypothesis: stationary\n\n\n\n\nCode\ndiff_pace &lt;- diff(ts_pace, differences = 1)\n\npar(mfrow = c(2, 1))\nplot(ts_pace, main = \"Original Pace Series\", ylab = \"Pace\", xlab = \"Year\")\nplot(diff_pace, main = \"First Differenced Pace Series\", ylab = \"Change in Pace\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_pace &lt;- ggAcf(diff_pace, lag.max = 20) +\n    labs(title = \"ACF of First Differenced Pace\") +\n    theme_minimal()\n\npacf_diff_pace &lt;- ggPacf(diff_pace, lag.max = 20) +\n    labs(title = \"PACF of First Differenced Pace\") +\n    theme_minimal()\n\nacf_diff_pace / pacf_diff_pace\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_diff_pace &lt;- adf.test(diff_pace)\nprint(adf_diff_pace)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_pace\nDickey-Fuller = -2.9769, Lag order = 3, p-value = 0.187\nalternative hypothesis: stationary",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#moving-average-smoothing",
    "href": "eda.html#moving-average-smoothing",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "2.4 Moving Average Smoothing",
    "text": "2.4 Moving Average Smoothing\n\n\nCode\nma_pace_3 &lt;- ma(ts_pace, order = 3) # 3-year window (short-term)\nma_pace_5 &lt;- ma(ts_pace, order = 5) # 5-year window (medium-term)\nma_pace_10 &lt;- ma(ts_pace, order = 10) # 10-year window (long-term)\n\nautoplot(ts_pace, series = \"Original\") +\n    autolayer(ma_pace_3, series = \"MA(3)\") +\n    autolayer(ma_pace_5, series = \"MA(5)\") +\n    autolayer(ma_pace_10, series = \"MA(10)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\",\n            \"MA(10)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\", \"MA(10)\")\n    ) +\n    labs(\n        title = \"Pace: Moving Average Smoothing Comparison\",\n        subtitle = \"U-shaped trajectory becomes clearer with increased smoothing\",\n        y = \"Pace (possessions per 48 min)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\nPace, the mediator in this story, follows a different trajectory: a classic U-shape. Possessions per 48 minutes decline from fast 1980s basketball to a trough in the mid-2000s, then recover through the 2010s and 2020s. Importantly, the Pace recovery begins before the analytics inflection, suggesting it is not simply a byproduct of analytics. Like ORtg, Pace is non-stationary in levels and stationary in first differences; moving-average smoothers with 5–10 year windows make the U-shape especially clear. This is rather significant as this means efficiency gains do not reduce to “more possessions”.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#time-series-visualization-2",
    "href": "eda.html#time-series-visualization-2",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "3.1 Time Series Visualization",
    "text": "3.1 Time Series Visualization\n\n\nCode\nts_3par &lt;- ts(league_avg$`3PAr`, start = 1980, frequency = 1)\n\ndf_3par &lt;- data.frame(\n    Year = league_avg$Season,\n    Value = league_avg$`3PAr`,\n    Era = df_ortg$Era\n)\n\nggplot(df_3par, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    annotate(\"text\",\n        x = 2012, y = 0.44, label = \"Analytics Era Begins\",\n        hjust = -0.05, color = \"#f58426\", fontface = \"bold\", size = 3.5\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    labs(\n        title = \"NBA 3-Point Attempt Rate (1980-2025)\",\n        subtitle = \"Percentage of field goal attempts that are three-pointers\",\n        x = \"Season\",\n        y = \"3-Point Attempt Rate (3PAr)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#lag-plots-2",
    "href": "eda.html#lag-plots-2",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "3.2 Lag Plots",
    "text": "3.2 Lag Plots\n\n\nCode\ngglagplot(ts_3par, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of 3-Point Attempt Rate (3PAr)\") +\n    theme_minimal()",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#stationarity-analysis",
    "href": "eda.html#stationarity-analysis",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "3.3 Stationarity Analysis",
    "text": "3.3 Stationarity Analysis\n\n\nCode\nacf_3par &lt;- ggAcf(ts_3par, lag.max = 20) +\n    labs(title = \"ACF of 3PAr\") +\n    theme_minimal()\n\npacf_3par &lt;- ggPacf(ts_3par, lag.max = 20) +\n    labs(title = \"PACF of 3PAr\") +\n    theme_minimal()\n\nacf_3par / pacf_3par\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_3par &lt;- adf.test(ts_3par)\nprint(adf_3par)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_3par\nDickey-Fuller = -1.3536, Lag order = 3, p-value = 0.8303\nalternative hypothesis: stationary\n\n\n\n\nCode\ndiff_3par &lt;- diff(ts_3par, differences = 1)\n\npar(mfrow = c(2, 1))\nplot(ts_3par, main = \"Original 3PAr Series\", ylab = \"3PAr\", xlab = \"Year\")\nplot(diff_3par, main = \"First Differenced 3PAr Series\", ylab = \"Change in 3PAr\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_3par &lt;- ggAcf(diff_3par, lag.max = 20) +\n    labs(title = \"ACF of First Differenced 3PAr\") +\n    theme_minimal()\n\npacf_diff_3par &lt;- ggPacf(diff_3par, lag.max = 20) +\n    labs(title = \"PACF of First Differenced 3PAr\") +\n    theme_minimal()\n\nacf_diff_3par / pacf_diff_3par\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_diff_3par &lt;- adf.test(diff_3par)\nprint(adf_diff_3par)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_3par\nDickey-Fuller = -3.5956, Lag order = 3, p-value = 0.04462\nalternative hypothesis: stationary",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#moving-average-smoothing-for-3par",
    "href": "eda.html#moving-average-smoothing-for-3par",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "3.4 Moving Average Smoothing for 3PAr",
    "text": "3.4 Moving Average Smoothing for 3PAr\n\n\nCode\n# Calculate moving averages with different windows\nma_3par_3 &lt;- ma(ts_3par, order = 3) # 3-year window (short-term)\nma_3par_5 &lt;- ma(ts_3par, order = 5) # 5-year window (medium-term)\nma_3par_10 &lt;- ma(ts_3par, order = 10) # 10-year window (long-term)\n\n# Create comparison plot\nautoplot(ts_3par, series = \"Original\") +\n    autolayer(ma_3par_3, series = \"MA(3)\") +\n    autolayer(ma_3par_5, series = \"MA(5)\") +\n    autolayer(ma_3par_10, series = \"MA(10)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\",\n            \"MA(10)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\", \"MA(10)\")\n    ) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    labs(\n        title = \"3-Point Attempt Rate: Moving Average Smoothing Comparison\",\n        subtitle = \"Analytics revolution's exponential growth pattern clearly visible\",\n        y = \"3-Point Attempt Rate (3PAr)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\nThe strongest structural break appears in 3PAr, which measures the share of shots taken from three. 3PAr rises modestly for decades and then accelerates sharply around 2012, around the same period where ORtg takes off. Lag plots show strong positive relationships across lags, and ACF/PACF behavior again indicates a trending series (non-stationary levels; stationary first differences). Smoothing highlights two regimes: a gradual era up to around 2012 and a rapid, near-exponential climb thereafter. This timing alignment supports the hypothesis that shot selection modernization (spacing, threes above the break, rim attempts enabled by space) is tightly coupled to league-wide efficiency gains",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#time-series-visualization-3",
    "href": "eda.html#time-series-visualization-3",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "4.1 Time Series Visualization",
    "text": "4.1 Time Series Visualization\n\n\nCode\n# Calculate league-wide attendance by season\nattendance_data &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        Total_Attendance = sum(`Unnamed: 29_level_0_Attend.`, na.rm = TRUE),\n        Avg_Attendance = mean(`Unnamed: 30_level_0_Attend./G`, na.rm = TRUE),\n        .groups = \"drop\"\n    )\n\n# Create time series (focusing on modern era 1990-2025)\nattendance_data &lt;- attendance_data %&gt;% filter(Season &gt;= 1990)\nts_attendance &lt;- ts(attendance_data$Total_Attendance, start = 1990, frequency = 1)\n\n\n\n\nCode\ndf_attendance &lt;- data.frame(\n    Year = attendance_data$Season,\n    Value = attendance_data$Total_Attendance / 1e6, # Convert to millions\n    Era = case_when(\n        attendance_data$Season &lt; 2020 ~ \"Pre-COVID\",\n        attendance_data$Season &gt;= 2020 & attendance_data$Season &lt; 2022 ~ \"COVID Era\",\n        attendance_data$Season &gt;= 2022 ~ \"Post-COVID Recovery\"\n    )\n)\n\nggplot(df_attendance, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2020, linetype = \"dashed\", color = \"red\", size = 1) +\n    annotate(\"text\",\n        x = 2020, y = 24, label = \"COVID-19\\nPandemic (2020)\",\n        hjust = -0.05, color = \"red\", fontface = \"bold\", size = 3.5\n    ) +\n    annotate(\"rect\",\n        xmin = 2020, xmax = 2021, ymin = 0, ymax = 25,\n        alpha = 0.1, fill = \"red\"\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-COVID\" = \"#006bb6\",\n        \"COVID Era\" = \"#d62728\",\n        \"Post-COVID Recovery\" = \"#2ca02c\"\n    )) +\n    labs(\n        title = \"NBA Total Attendance (1990-2025): COVID-19 Disruption and Recovery\",\n        subtitle = \"90% collapse in 2020-21 followed by gradual recovery\",\n        x = \"Season\",\n        y = \"Total Attendance (Millions)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#lag-plots-3",
    "href": "eda.html#lag-plots-3",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "4.2 Lag Plots",
    "text": "4.2 Lag Plots\n\n\nCode\ngglagplot(ts_attendance, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Total Attendance\") +\n    theme_minimal()",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#acf-and-pacf-analysis-2",
    "href": "eda.html#acf-and-pacf-analysis-2",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "4.3 ACF and PACF Analysis",
    "text": "4.3 ACF and PACF Analysis\n\n\nCode\nacf_attendance &lt;- ggAcf(ts_attendance, lag.max = 15) +\n    labs(title = \"ACF of Total Attendance\") +\n    theme_minimal()\n\npacf_attendance &lt;- ggPacf(ts_attendance, lag.max = 15) +\n    labs(title = \"PACF of Total Attendance\") +\n    theme_minimal()\n\nacf_attendance / pacf_attendance",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#moving-average-smoothing-for-attendance",
    "href": "eda.html#moving-average-smoothing-for-attendance",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "4.4 Moving Average Smoothing for Attendance",
    "text": "4.4 Moving Average Smoothing for Attendance\n\n\nCode\n# Calculate moving averages\nma_attendance_3 &lt;- ma(ts_attendance, order = 3)\nma_attendance_5 &lt;- ma(ts_attendance, order = 5)\n\n# Plot comparison\nautoplot(ts_attendance, series = \"Original\") +\n    autolayer(ma_attendance_3, series = \"MA(3)\") +\n    autolayer(ma_attendance_5, series = \"MA(5)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\")\n    ) +\n    labs(\n        title = \"Attendance: Moving Average Smoothing (COVID Shock Visible)\",\n        subtitle = \"Smoothing cannot remove the dramatic 2020-21 disruption\",\n        y = \"Total Attendance (millions)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\nAttendance provides the counterpoint: a stable pre-COVID plateau around ~21–22 million through 2019, a 2020–21 collapse during the bubble/limited-capacity seasons, and a partial recovery that remains below the pre-pandemic ceiling. The sharp, short-window discontinuity is a uncounted for shock rather than a new equilibrium. Even with 3–5 year moving averages, the COVID impact is too large to smooth away",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#data-preparation-and-time-series-creation",
    "href": "eda.html#data-preparation-and-time-series-creation",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.1 Data Preparation and Time Series Creation",
    "text": "5.1 Data Preparation and Time Series Creation\n\n\nCode\n# Load all sports betting stocks\ndkng &lt;- read_csv(\"data/financial/DKNG_daily.csv\", show_col_types = FALSE) %&gt;% mutate(Date = as.Date(Date))\npenn &lt;- read_csv(\"data/financial/PENN_daily.csv\", show_col_types = FALSE) %&gt;% mutate(Date = as.Date(Date))\nmgm &lt;- read_csv(\"data/financial/MGM_daily.csv\", show_col_types = FALSE) %&gt;% mutate(Date = as.Date(Date))\nczr &lt;- read_csv(\"data/financial/CZR_daily.csv\", show_col_types = FALSE) %&gt;% mutate(Date = as.Date(Date))\n\ncat(\"DKNG:\", nrow(dkng), \"days |\", min(dkng$Date), \"to\", max(dkng$Date), \"\\n\")\n\n\nDKNG: 1181 days | 18375 to 20088 \n\n\nCode\ncat(\"PENN:\", nrow(penn), \"days |\", min(penn$Date), \"to\", max(penn$Date), \"\\n\")\n\n\nPENN: 1258 days | 18263 to 20088 \n\n\nCode\ncat(\"MGM:\", nrow(mgm), \"days |\", min(mgm$Date), \"to\", max(mgm$Date), \"\\n\")\n\n\nMGM: 1258 days | 18263 to 20088 \n\n\nCode\ncat(\"CZR:\", nrow(czr), \"days |\", min(czr$Date), \"to\", max(czr$Date), \"\\n\")\n\n\nCZR: 1258 days | 18263 to 20088 \n\n\nCode\n# Create weekly time series for all stocks\ncreate_weekly_ts &lt;- function(df, ticker) {\n    weekly &lt;- df %&gt;%\n        mutate(Year = year(Date), Week = week(Date)) %&gt;%\n        group_by(Year, Week) %&gt;%\n        summarise(Avg_Close = mean(`Adj Close`, na.rm = TRUE), .groups = \"drop\") %&gt;%\n        arrange(Year, Week)\n\n    start_year &lt;- min(weekly$Year)\n    start_week &lt;- weekly %&gt;%\n        filter(Year == start_year) %&gt;%\n        pull(Week) %&gt;%\n        min()\n    ts(weekly$Avg_Close, start = c(start_year, start_week), frequency = 52)\n}\n\nts_dkng &lt;- create_weekly_ts(dkng, \"DKNG\")\nts_penn &lt;- create_weekly_ts(penn, \"PENN\")\nts_mgm &lt;- create_weekly_ts(mgm, \"MGM\")\nts_czr &lt;- create_weekly_ts(czr, \"CZR\")",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#comparative-visualization-all-four-betting-stocks",
    "href": "eda.html#comparative-visualization-all-four-betting-stocks",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.2 Comparative Visualization: All Four Betting Stocks",
    "text": "5.2 Comparative Visualization: All Four Betting Stocks\n\n\nCode\n# Combine all stocks for comparison (normalize to starting price = 100)\nautoplot(ts_dkng / as.numeric(ts_dkng)[1] * 100, series = \"DKNG\") +\n    autolayer(ts_penn / as.numeric(ts_penn)[1] * 100, series = \"PENN\") +\n    autolayer(ts_mgm / as.numeric(ts_mgm)[1] * 100, series = \"MGM\") +\n    autolayer(ts_czr / as.numeric(ts_czr)[1] * 100, series = \"CZR\") +\n    scale_color_manual(values = c(\"DKNG\" = \"#006bb6\", \"PENN\" = \"#f58426\", \"MGM\" = \"#00a94f\", \"CZR\" = \"#c8102e\")) +\n    labs(\n        title = \"Sports Betting Stocks: Normalized Performance (2020-2024)\",\n        subtitle = \"Indexed to 100 at each stock's start date | Boom-bust-stabilization pattern\",\n        y = \"Normalized Price (Start = 100)\", x = \"Year\", color = \"Stock\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\"), legend.position = \"bottom\")",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#dkng-detailed-analysis",
    "href": "eda.html#dkng-detailed-analysis",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.3 DKNG Detailed Analysis",
    "text": "5.3 DKNG Detailed Analysis\n\n\nCode\nautoplot(ts_dkng) +\n    annotate(\"rect\", xmin = 2021, xmax = 2021.5, ymin = 0, ymax = 70, alpha = 0.1, fill = \"orange\") +\n    annotate(\"text\", x = 2021.25, y = 65, label = \"Peak Boom\", color = \"orange\", fontface = \"bold\", size = 3) +\n    labs(\n        title = \"DraftKings (DKNG) Weekly Stock Price (2020-2024)\",\n        subtitle = \"IPO boom during COVID → correction → stabilization\",\n        x = \"Year\", y = \"Avg Weekly Adj Close ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\", size = 14))",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#seasonal-decomposition-dkng",
    "href": "eda.html#seasonal-decomposition-dkng",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.4 Seasonal Decomposition (DKNG)",
    "text": "5.4 Seasonal Decomposition (DKNG)\n\n\nCode\n# Multiplicative decomposition (appropriate for stock prices)\ndecomp_dkng &lt;- decompose(ts_dkng, type = \"multiplicative\")\n\n# Plot decomposition\nautoplot(decomp_dkng) +\n    labs(title = \"DKNG Stock: Seasonal Decomposition (Multiplicative Model)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(face = \"bold\", size = 14))",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#moving-average-smoothing-dkng",
    "href": "eda.html#moving-average-smoothing-dkng",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.5 Moving Average Smoothing (DKNG)",
    "text": "5.5 Moving Average Smoothing (DKNG)\n\n\nCode\n# Calculate moving averages (using weeks)\nma_dkng_4 &lt;- ma(ts_dkng, order = 4) # Monthly smoothing (~4 weeks)\nma_dkng_13 &lt;- ma(ts_dkng, order = 13) # Quarterly smoothing (~13 weeks)\nma_dkng_52 &lt;- ma(ts_dkng, order = 52) # Annual smoothing (52 weeks)\n\nautoplot(ts_dkng, series = \"Original\") +\n    autolayer(ma_dkng_4, series = \"MA(4 weeks)\") +\n    autolayer(ma_dkng_13, series = \"MA(13 weeks)\") +\n    autolayer(ma_dkng_52, series = \"MA(52 weeks)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(4 weeks)\" = \"#006bb6\",\n            \"MA(13 weeks)\" = \"#f58426\",\n            \"MA(52 weeks)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(4 weeks)\", \"MA(13 weeks)\", \"MA(52 weeks)\")\n    ) +\n    labs(\n        title = \"DKNG Stock: Moving Average Smoothing Comparison\",\n        subtitle = \"Different windows reveal trading cycles vs long-term trends\",\n        y = \"Stock Price ($)\",\n        x = \"Year\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#acf-and-lag-plots-dkng",
    "href": "eda.html#acf-and-lag-plots-dkng",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.6 ACF and Lag Plots (DKNG)",
    "text": "5.6 ACF and Lag Plots (DKNG)\n\n\nCode\nacf_dkng &lt;- ggAcf(ts_dkng, lag.max = 52) +\n    labs(title = \"ACF of DKNG Weekly Stock Price\") +\n    theme_minimal()\n\npacf_dkng &lt;- ggPacf(ts_dkng, lag.max = 52) +\n    labs(title = \"PACF of DKNG Weekly Stock Price\") +\n    theme_minimal()\n\nacf_dkng / pacf_dkng",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#penn-analysis",
    "href": "eda.html#penn-analysis",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.7 PENN Analysis",
    "text": "5.7 PENN Analysis\n\n5.7.1 PENN Time Series Visualization\n\n\nCode\nautoplot(ts_penn) +\n    annotate(\"rect\", xmin = 2020, xmax = 2021.5, ymin = 0, ymax = 140, alpha = 0.1, fill = \"red\") +\n    annotate(\"text\", x = 2020, y = 130, label = \"Covid-19\", color = \"red\", fontface = \"bold\", size = 3) +\n    labs(\n        title = \"Penn Entertainment (PENN) Weekly Stock Price (2020-2024)\",\n        x = \"Year\", y = \"Avg Weekly Adj Close ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n5.7.2 Seasonal Decomposition (PENN)\n\n\nCode\n# Multiplicative decomposition\ndecomp_penn &lt;- decompose(ts_penn, type = \"multiplicative\")\n\nautoplot(decomp_penn) +\n    labs(title = \"PENN Stock: Seasonal Decomposition (Multiplicative Model)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n5.7.3 Moving Average Smoothing (PENN)\n\n\nCode\nma_penn_4 &lt;- ma(ts_penn, order = 4)\nma_penn_13 &lt;- ma(ts_penn, order = 13)\nma_penn_52 &lt;- ma(ts_penn, order = 52)\n\nautoplot(ts_penn, series = \"Original\") +\n    autolayer(ma_penn_4, series = \"MA(4 weeks)\") +\n    autolayer(ma_penn_13, series = \"MA(13 weeks)\") +\n    autolayer(ma_penn_52, series = \"MA(52 weeks)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(4 weeks)\" = \"#006bb6\",\n            \"MA(13 weeks)\" = \"#f58426\",\n            \"MA(52 weeks)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(4 weeks)\", \"MA(13 weeks)\", \"MA(52 weeks)\")\n    ) +\n    labs(\n        title = \"PENN Stock: Moving Average Smoothing Comparison\",\n        subtitle = \"Even annual smoothing cannot hide the structural collapse\",\n        y = \"Stock Price ($)\", x = \"Year\", color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n5.7.4 ACF and Lag Plots (PENN)\n\n\nCode\nacf_penn &lt;- ggAcf(ts_penn, lag.max = 52) +\n    labs(title = \"ACF of PENN Weekly Stock Price\") +\n    theme_minimal()\n\npacf_penn &lt;- ggPacf(ts_penn, lag.max = 52) +\n    labs(title = \"PACF of PENN Weekly Stock Price\") +\n    theme_minimal()\n\nacf_penn / pacf_penn\n\n\n\n\n\n\n\n\n\nBecause annual NBA series are effectively non-seasonal, I include weekly sports-betting equities to demonstrate seasonality and multiplicative decomposition. DraftKings (DKNG), Penn (PENN), MGM, and Caesars (CZR) all show pandemic-era boom-bust dynamics on weekly data. Prices are non-stationary in levels, stationary in differences, and has volatility that scales with price; implying a multiplicative model is necessary for decomposition. DKNG exhibits a large run-up, correction, and stabilization while PENN shows a sharper hype-driven spike and deeper collapse.\nPulling the findings together: ORtg, Pace, 3PAr, and Attendance are all non-stationary in levels and become stationary after first differences (d = 1). Therefore, additive decomposition is appropriate for the NBA metrics , while multiplicative decomposition fits the weekly equities. Short and medium moving-average windows clarify regime shifts: the 2012 analytics inflection in ORtg/3PAr, the mid-2000s trough and rebound in Pace, and the COVID intervention in Attendance.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "The Era of Efficiency: Analytics, COVID, and the Modern NBA",
    "section": "",
    "text": "Over the last two decades, the National Basketball Association (NBA) has undergone a historic transformation in how the game is played and measured. The rise of analytics has redefined decision-making, from shot selection to player valuation, creating a league increasingly optimized for pace, spacing, and efficiency. Traditional mid-range play has given way to data-driven offenses that favor the three-point shot and fast-break opportunities1. Yet this evolution has not been linear: external shocks such as the COVID-19 pandemic and rule changes have periodically disrupted the sport’s equilibrium.\nThis project seeks to quantify and contextualize the NBA’s evolution toward efficiency; particularly how the league’s statistical DNA has shifted under the influence of analytics, and how sudden disruptions like the 2020 “bubble” season temporarily rewired its dynamics. Using time-series analysis, the study traces the interplay between pace, three-point attempt rate, and offensive efficiency to reveal both long-term structural change and short-term volatility.\n\n\nBasketball today is not merely an athletic competition but a living laboratory of applied data science. The league’s embrace of analytics mirrors a broader transformation in modern industries: decisions increasingly rest on quantitative evidence rather than intuition. The shift from mid-range isolation to high efficiency, high variance offense encapsulates how optimization, technology, and information access shape human behavior, specifically in sports.\n\n\n\nThe Big Picture\n\n\nWith the framework above, I aim to tell the story of how basketball’s offensive efficiency evolved under the dual pressures of analytics and disruption. The analysis begins with a long-run view of the game’s transformation: tracing how three-point attempts, shot selection, and pace redefined offensive output from 1980 through 2025. By examining league wide efficiency, field-goal percentages, and possessions per 48 minutes, I hope to capture how the modern NBA diverged from its slower, mid-range past into a data-optimized era of high variance offense.\nFrom there, the focus shifts to the Analytics Revolution, a period beginning around 2012 when front offices embraced quantitative models and player-tracking data. Here, patterns in shot distance, efficiency metrics, and team-level statistics reveal how strategy and roster construction began to align with mathematical optimization. These structural changes not only reshaped offensive philosophy but also institutionalized analytics as a core component of competitive advantage.\nThe next chapter explores the COVID-19 shock, a natural experiment that disrupted decades of rhythm. Empty arenas, travel restrictions, and condensed schedules created conditions to test how context, apart from talent or tactics, influences performance. Comparing attendance patterns, scoring volatility, and pace before, during, and after the pandemic provides a rare window into basketball’s psychological and environmental dimensions.\nFinally, the analysis turns toward the future. By modeling post-2023 trends using both traditional and modern time-series methods, the goal is to forecast whether the league has stabilized in a new equilibrium or continues to evolve. From evolution and optimization to disruption and re-emergence the project aims to capture how data, environment, and adaptation intertwine to define the modern game.\n\n\n\nEarly quantitative basketball research established the foundation for efficiency metrics such as effective field goal percentage (eFG%) and true shooting percentage (TS%)2. Subsequent work formalized offensive rating and pace-adjusted measures that underpin today’s analytics frameworks3. In this study, those established metrics anchor a longitudinal view of the NBA from 1980–2025, allowing us to trace how efficiency evolved and whether recent shifts reflect gradual adaptation or discrete regime changes.\nA central thread is shot-selection optimization. Spatial analyses show that perimeter-oriented offenses and rim attempts yield higher expected value than mid-range play4. Building on that evidence, we follow the league’s shot mix over time and date its key inflection points using structural-break diagnostics. Forecast comparisons around those dated transitions then indicate whether the modern shot profile has stabilized or is still moving toward further concentration in high-value zones.\nTempo and spacing economics form the second pillar. Prior work links ball movement, spacing, and faster pace to scoring efficiency in the modern game1. Rather than treating pace and efficiency as static correlates, we examine their dynamic ordering: do changes in tempo precede shifts in efficiency, or the reverse? A multivariate framework (including Granger-style causality tests and rolling correlations) lets us see how this relationship strengthened from the pre-analytics period into the analytics era, clarifying whether “playing faster” is a driver, a consequence, or part of a feedback loop with shot quality.\nThe third pillar addresses COVID-era disruptions. Empty-arena conditions in 2020 weakened traditional home-court effects and scrambled normal rhythms. We treat the bubble and capacity-limited seasons as an exogenous intervention, quantify the immediate impact on pace, efficiency, and scoring variance, and then measure persistence and recovery. By juxtaposing pre-2020 fitted behavior with realized outcomes through 2022, we can distinguish a temporary shock from a lasting structural shift.\nTogether, these components extend prior literature that established what changed (shot mix, tempo strategies, COVID effects) but seldom mapped how and when the transitions unfolded or whether they endured. Concretely, we (i) date the analytics inflection with objective break tests; (ii) test whether rising three-point volume leads or follows efficiency gains; (iii) model low-order serial dependence and feedback that cross-sectional designs omit; and (iv) isolate the pandemic’s impulse and its decay. Treating efficiency, pace, and shot selection as interconnected trajectories reveals the timing, coupling, and durability of the forces reshaping the modern NBA.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#the-big-picture",
    "href": "intro.html#the-big-picture",
    "title": "The Era of Efficiency: Analytics, COVID, and the Modern NBA",
    "section": "",
    "text": "Basketball today is not merely an athletic competition but a living laboratory of applied data science. The league’s embrace of analytics mirrors a broader transformation in modern industries: decisions increasingly rest on quantitative evidence rather than intuition. The shift from mid-range isolation to high efficiency, high variance offense encapsulates how optimization, technology, and information access shape human behavior, specifically in sports.\n\n\n\nThe Big Picture\n\n\nWith the framework above, I aim to tell the story of how basketball’s offensive efficiency evolved under the dual pressures of analytics and disruption. The analysis begins with a long-run view of the game’s transformation: tracing how three-point attempts, shot selection, and pace redefined offensive output from 1980 through 2025. By examining league wide efficiency, field-goal percentages, and possessions per 48 minutes, I hope to capture how the modern NBA diverged from its slower, mid-range past into a data-optimized era of high variance offense.\nFrom there, the focus shifts to the Analytics Revolution, a period beginning around 2012 when front offices embraced quantitative models and player-tracking data. Here, patterns in shot distance, efficiency metrics, and team-level statistics reveal how strategy and roster construction began to align with mathematical optimization. These structural changes not only reshaped offensive philosophy but also institutionalized analytics as a core component of competitive advantage.\nThe next chapter explores the COVID-19 shock, a natural experiment that disrupted decades of rhythm. Empty arenas, travel restrictions, and condensed schedules created conditions to test how context, apart from talent or tactics, influences performance. Comparing attendance patterns, scoring volatility, and pace before, during, and after the pandemic provides a rare window into basketball’s psychological and environmental dimensions.\nFinally, the analysis turns toward the future. By modeling post-2023 trends using both traditional and modern time-series methods, the goal is to forecast whether the league has stabilized in a new equilibrium or continues to evolve. From evolution and optimization to disruption and re-emergence the project aims to capture how data, environment, and adaptation intertwine to define the modern game.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#literature-review",
    "href": "intro.html#literature-review",
    "title": "The Era of Efficiency: Analytics, COVID, and the Modern NBA",
    "section": "",
    "text": "Early quantitative basketball research established the foundation for efficiency metrics such as effective field goal percentage (eFG%) and true shooting percentage (TS%)2. Subsequent work formalized offensive rating and pace-adjusted measures that underpin today’s analytics frameworks3. In this study, those established metrics anchor a longitudinal view of the NBA from 1980–2025, allowing us to trace how efficiency evolved and whether recent shifts reflect gradual adaptation or discrete regime changes.\nA central thread is shot-selection optimization. Spatial analyses show that perimeter-oriented offenses and rim attempts yield higher expected value than mid-range play4. Building on that evidence, we follow the league’s shot mix over time and date its key inflection points using structural-break diagnostics. Forecast comparisons around those dated transitions then indicate whether the modern shot profile has stabilized or is still moving toward further concentration in high-value zones.\nTempo and spacing economics form the second pillar. Prior work links ball movement, spacing, and faster pace to scoring efficiency in the modern game1. Rather than treating pace and efficiency as static correlates, we examine their dynamic ordering: do changes in tempo precede shifts in efficiency, or the reverse? A multivariate framework (including Granger-style causality tests and rolling correlations) lets us see how this relationship strengthened from the pre-analytics period into the analytics era, clarifying whether “playing faster” is a driver, a consequence, or part of a feedback loop with shot quality.\nThe third pillar addresses COVID-era disruptions. Empty-arena conditions in 2020 weakened traditional home-court effects and scrambled normal rhythms. We treat the bubble and capacity-limited seasons as an exogenous intervention, quantify the immediate impact on pace, efficiency, and scoring variance, and then measure persistence and recovery. By juxtaposing pre-2020 fitted behavior with realized outcomes through 2022, we can distinguish a temporary shock from a lasting structural shift.\nTogether, these components extend prior literature that established what changed (shot mix, tempo strategies, COVID effects) but seldom mapped how and when the transitions unfolded or whether they endured. Concretely, we (i) date the analytics inflection with objective break tests; (ii) test whether rising three-point volume leads or follows efficiency gains; (iii) model low-order serial dependence and feedback that cross-sectional designs omit; and (iv) isolate the pandemic’s impulse and its decay. Treating efficiency, pace, and shot selection as interconnected trajectories reveals the timing, coupling, and durability of the forces reshaping the modern NBA.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "data_viz.html",
    "href": "data_viz.html",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "",
    "text": "This analysis tells the story of the most dramatic strategic transformation in NBA history, particularly the shift from mid-range-heavy “iso-ball” to the analytics-optimized “Moreyball” offense that dominates today. Through the visualizations below, we can see how data-driven decision making fundamentally reshaped basketball strategy over 45 years.\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(magick)\nlibrary(viridis)\nlibrary(patchwork)\nlibrary(cowplot)\n\ntheme_set(theme_minimal(base_size = 12))",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#the-analytics-revolution-rise-of-the-three-pointer-1980-2025",
    "href": "data_viz.html#the-analytics-revolution-rise-of-the-three-pointer-1980-2025",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "1. The Analytics Revolution: Rise of the Three-Pointer (1980-2025)",
    "text": "1. The Analytics Revolution: Rise of the Three-Pointer (1980-2025)\n\n\nCode\nlibrary(stringr)\n\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n\n    df &lt;- read_csv(file, show_col_types = FALSE)\n\n    df$Season &lt;- season_year\n    return(df)\n})\n\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        .groups = \"drop\"\n    )\n\nleague_avg &lt;- league_avg %&gt;%\n    mutate(\n        Era = case_when(\n            Season &lt; 2012 ~ \"Pre-Analytics Era\",\n            Season &gt;= 2012 & Season &lt; 2020 ~ \"Analytics Era\",\n            Season &gt;= 2020 ~ \"Post-COVID Era\"\n        )\n    )\n\nfig_3par &lt;- plot_ly(league_avg,\n    x = ~Season, y = ~`3PAr`,\n    color = ~Era,\n    colors = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    ),\n    type = \"scatter\", mode = \"lines+markers\",\n    marker = list(size = 6),\n    line = list(width = 3),\n    hovertemplate = paste(\n        \"&lt;b&gt;Season:&lt;/b&gt; %{x}&lt;br&gt;\",\n        \"&lt;b&gt;3PAr:&lt;/b&gt; %{y:.0%}&lt;br&gt;\",\n        \"&lt;extra&gt;&lt;/extra&gt;\"\n    )\n) %&gt;%\n    layout(\n        title = list(\n            text = \"The Analytics Revolution: 3-Point Attempt Rate (1980-2025)\",\n            font = list(size = 15, weight = \"bold\")\n        ),\n        xaxis = list(title = \"Season\"),\n        yaxis = list(title = \"3-Point Attempt Rate (3PA / FGA)\", tickformat = \".0%\"),\n        hovermode = \"closest\",\n        template = \"plotly_white\",\n        annotations = list(\n            list(\n                x = 2012, y = 0.44, text = \"Analytics Era Begins\",\n                showarrow = FALSE,\n                font = list(size = 8, color = \"#f58426\", weight = \"bold\"),\n                xanchor = \"left\", xshift = 5\n            )\n        ),\n        shapes = list(\n            list(\n                type = \"line\", x0 = 2012, x1 = 2012, y0 = 0, y1 = 1,\n                line = list(color = \"#f58426\", width = 2, dash = \"dash\"),\n                yref = \"paper\"\n            )\n        )\n    )\n\nfig_3par\n\n\n\n\n\n\nFor three decades, from 1980 to 2011, NBA teams treated the three-pointer as a supplementary weapon rather than a foundational strategy, with attempt rates hovering consistently between 20% and 28%. Between 1995 and 1997, the rate peaked at 21% due to the league temporarily shortening the three-point line. However, from 1997 to 1998, we see a clear decline in attempts as the league reverted to the original distance. During this era, the mid-range jumper, the signature shot of basketball mastery taught in gyms from youth leagues to the professional ranks, remained dominant. But in 2012, Houston Rockets GM Daryl Morey’s analytics department did the math and exposed a harsh truth: mid-range shots, averaging roughly 0.8 points per attempt, were the least efficient in basketball, while three-pointers yielded significantly higher returns. Observing the visualization, we can clearly identify a structural break around 2012, as three-point attempt rates surge from roughly 28% to over 42% by 2025. Yet this shift raises a crucial question: If teams started shooing more threes, did it actually make them better or just different?",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#efficiency-evolution-four-decades-of-offensive-rating-pace-and-shooting",
    "href": "data_viz.html#efficiency-evolution-four-decades-of-offensive-rating-pace-and-shooting",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "2. Efficiency Evolution: Four Decades of Offensive Rating, Pace, and Shooting",
    "text": "2. Efficiency Evolution: Four Decades of Offensive Rating, Pace, and Shooting\n\n\nCode\nefficiency_long &lt;- league_avg %&gt;%\n    select(Season, ORtg, Pace, `TS%`, `eFG%`, Era) %&gt;%\n    pivot_longer(\n        cols = c(ORtg, Pace, `TS%`, `eFG%`),\n        names_to = \"Metric\",\n        values_to = \"Value\"\n    )\n\nefficiency_facet &lt;- ggplot(efficiency_long, aes(x = Season, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"loess\", se = TRUE, alpha = 0.2, color = \"black\", linetype = \"dashed\") +\n    facet_wrap(~Metric,\n        scales = \"free_y\", ncol = 2,\n        labeller = labeller(Metric = c(\n            \"ORtg\" = \"Offensive Rating (pts per 100 poss)\",\n            \"Pace\" = \"Pace (possessions per 48 min)\",\n            \"TS%\" = \"True Shooting %\",\n            \"eFG%\" = \"Effective FG%\"\n        ))\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Efficiency Metrics: 45-Year Evolution (1980-2025)\",\n        subtitle = \"Offensive rating climbed steadily; pace declined then rebounded; shooting efficiency surged post-2012\",\n        x = \"Season\",\n        y = \"Metric Value\",\n        color = \"Era\",\n        caption = \"Data: Basketball Reference | Black dashed line: LOESS smoothing\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 16),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        strip.text = element_text(face = \"bold\", size = 11),\n        legend.position = \"bottom\",\n        panel.grid.minor = element_blank()\n    )\n\nefficiency_facet\n\n\n\n\n\n\n\n\n\nObserving the visualization, we can see that attempting more three pointers did, in fact, make teams measurably better. The chart tells a story spanning 45 years of performance gains driven by strategic optimization. Offensive Rating rose by roughly 11%, from 104 in 1980 to 115 in 2025, with the sharpest improvements occurring after 2012, precisely when three point attempt rates began to surge. We also see True Shooting Percentage climb from 53% to 58%, reinforcing the conclusion that teams became more efficient scorers by optimizing the quality of their shots. The Pace metric follows a U-shaped trajectory, reflecting the evolution of play styles over time. It went from the fast, run and gun tempo of the 1980s, to the slowed isolation heavy 2000s, and finally rebounding post 2012 as teams embraced a more controlled yet efficient rhythm. Lastly, the rise in Effective Field Goal Percentage suggests that these improvements weren’t merely the result of drawing more fouls. Teams didn’t shoot more threes by accident; they made a deliberate, data driven decision to sacrifice mid range shots in exchange for more threes and attempts at the rim.",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#death-of-the-midrange-shot-zone-trends-2004-2025",
    "href": "data_viz.html#death-of-the-midrange-shot-zone-trends-2004-2025",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "3. Death of the Midrange: Shot Zone Trends (2004-2025)",
    "text": "3. Death of the Midrange: Shot Zone Trends (2004-2025)\n\n\nCode\nshot_files &lt;- list.files(\"data/shot_location\", pattern = \"NBA_.*_Shots.csv\", full.names = TRUE)\n\nextract_season &lt;- function(filename) {\n    year_str &lt;- str_extract(basename(filename), \"\\\\d{4}\")\n    return(as.numeric(year_str))\n}\n\nshot_data_sample &lt;- map_df(shot_files, function(file) {\n    season_year &lt;- extract_season(file)\n\n    df &lt;- read_csv(file, show_col_types = FALSE, n_max = 50000)\n    df$Season &lt;- season_year\n    return(df)\n})\n\nzone_distribution &lt;- shot_data_sample %&gt;%\n    filter(!is.na(BASIC_ZONE)) %&gt;%\n    group_by(Season, BASIC_ZONE) %&gt;%\n    summarise(\n        Shot_Count = n(),\n        .groups = \"drop\"\n    ) %&gt;%\n    group_by(Season) %&gt;%\n    mutate(\n        Shot_Percentage = Shot_Count / sum(Shot_Count) * 100\n    ) %&gt;%\n    ungroup()\n\nkey_zones &lt;- c(\n    \"Mid-Range\", \"Restricted Area\", \"Above the Break 3\",\n    \"Left Corner 3\", \"Right Corner 3\"\n)\n\nzone_trends &lt;- zone_distribution %&gt;%\n    filter(BASIC_ZONE %in% key_zones) %&gt;%\n    mutate(\n        Zone_Category = case_when(\n            BASIC_ZONE == \"Mid-Range\" ~ \"Mid-Range (≈8–22 ft)\",\n            BASIC_ZONE == \"Restricted Area\" ~ \"At Rim\",\n            BASIC_ZONE %in% c(\"Left Corner 3\", \"Right Corner 3\") ~ \"Corner 3s\",\n            BASIC_ZONE == \"Above the Break 3\" ~ \"Non-Corner 3s (Arc)\",\n            TRUE ~ BASIC_ZONE\n        )\n    ) %&gt;%\n    group_by(Season, Zone_Category) %&gt;%\n    summarise(Shot_Percentage = sum(Shot_Percentage), .groups = \"drop\")\n\nzone_trends &lt;- zone_trends %&gt;%\n    mutate(\n        tooltip_text = paste0(\n            \"Season: \", Season, \"\\n\",\n            \"Zone: \", Zone_Category, \"\\n\",\n            \"Percentage: \", round(Shot_Percentage, 0), \"%\"\n        )\n    )\n\nrect_data &lt;- data.frame(\n    xmin = 2012, xmax = 2015,\n    ymin = 0, ymax = 45\n)\n\nmidrange_line_plot &lt;- ggplot(zone_trends, aes(\n    x = Season, y = Shot_Percentage,\n    color = Zone_Category,\n    linetype = Zone_Category,\n    text = tooltip_text\n)) +\n    geom_rect(\n        data = rect_data, inherit.aes = FALSE,\n        aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),\n        fill = \"#f58426\", alpha = 0.1\n    ) +\n    geom_line(size = 1.5) +\n    geom_point(size = 2.5) +\n    annotate(\"text\",\n        x = 2013.5, y = 42, label = \"Analytics\\nRevolution\", fontface = \"bold\", color = \"#f58426\"\n    ) +\n    scale_color_manual(values = c(\n        \"Mid-Range (≈8–22 ft)\" = \"#bec0c2\",\n        \"At Rim\" = \"#006bb6\",\n        \"Corner 3s\" = \"#f58426\",\n        \"Non-Corner 3s (Arc)\" = \"#000000\"\n    )) +\n    scale_linetype_manual(values = c(\n        \"Mid-Range (≈8–22 ft)\" = \"solid\",\n        \"At Rim\"               = \"solid\",\n        \"Corner 3s\"            = \"dashed\",\n        \"Non-Corner 3s (Arc)\"  = \"dotted\"\n    )) +\n    labs(\n        title = \"The Death of the Midrange: Shot Zone Trends (2004–2025)\",\n        subtitle = \"Mid-range declined while arc and corner 3s surged; at-rim remained relatively stable\",\n        x = \"Season\", y = \"Percentage of Total Shots (%)\",\n        color = \"Shot Zone\", linetype = \"Shot Zone\"\n    ) +\n    theme_minimal(base_size = 10) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 13),\n        plot.subtitle = element_text(size = 9, color = \"gray40\"),\n        axis.title = element_text(size = 9),\n        axis.text = element_text(size = 8),\n        legend.position = \"bottom\",\n        legend.title = element_text(face = \"bold\", size = 9),\n        legend.text = element_text(size = 8),\n        panel.grid.minor = element_blank()\n    ) +\n    scale_y_continuous(limits = c(0, 45), labels = function(x) paste0(x, \"%\"))\n\np &lt;- ggplotly(midrange_line_plot, tooltip = \"text\")\n\np %&gt;% style(mode = \"lines+markers\")\n\n\n\n\n\n\nWe can clearly see the trade-off in the visualization above. Mid-range shots, which accounted for about 35% of all attempts in 2004, collapsed to just 13% by 2025. Meanwhile, corner threes doubled, and above-the-arc threes surged from 13% to 34%. Shots at the rim remained relatively stable throughout this period. Around the 2015–2016 season, three-point attempts beyond the arc surpassed mid-range shots for the first time in NBA history. This shift reflects teams’ evolving approach: attack the rim for high-percentage looks, draw fouls or create putback opportunities, or shoot threes for higher expected value",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#court-shot-charts-evolution-of-shot-selection-2004-2024-w.-focus-on-boston-celtics",
    "href": "data_viz.html#court-shot-charts-evolution-of-shot-selection-2004-2024-w.-focus-on-boston-celtics",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "4. Court Shot Charts: Evolution of Shot Selection (2004-2024) w. Focus on Boston Celtics",
    "text": "4. Court Shot Charts: Evolution of Shot Selection (2004-2024) w. Focus on Boston Celtics\n\n\nCode\nsource(\"NBA_shots_tutorial.R\")\n\nshots_2004 &lt;- shot_data_sample %&gt;%\n    filter(Season == 2004, !is.na(LOC_X), !is.na(LOC_Y), LOC_Y &lt;= 47)\n\nzone_check_2004 &lt;- shots_2004 %&gt;%\n    filter(!is.na(BASIC_ZONE)) %&gt;%\n    group_by(BASIC_ZONE) %&gt;%\n    summarise(Count = n(), .groups = \"drop\") %&gt;%\n    mutate(Percentage = Count / sum(Count) * 100) %&gt;%\n    arrange(desc(Count))\n\nyears_to_plot &lt;- c(2004, 2008, 2012, 2016, 2019, 2024)\n\ncreate_court_for_year &lt;- function(year) {\n    shots_year &lt;- shot_data_sample %&gt;%\n        filter(Season == year, !is.na(LOC_X), !is.na(LOC_Y), LOC_Y &lt;= 47, !is.na(BASIC_ZONE))\n\n    shots_year &lt;- shots_year %&gt;%\n        mutate(\n            Zone_Category = case_when(\n                BASIC_ZONE == \"Mid-Range\" ~ \"Mid-Range\",\n                BASIC_ZONE == \"Restricted Area\" ~ \"At Rim\",\n                BASIC_ZONE %in% c(\"Left Corner 3\", \"Right Corner 3\") ~ \"Corner 3s\",\n                BASIC_ZONE == \"Above the Break 3\" ~ \"Above Arc 3s\",\n                TRUE ~ \"Other\"\n            )\n        )\n\n    if (nrow(shots_year) &gt; 3000) {\n        set.seed(42) # For reproducibility\n        shots_year &lt;- shots_year %&gt;% sample_n(3000)\n    }\n\n    subtitle &lt;- case_when(\n        year == 2004 ~ \"Pre-Analytics\",\n        year == 2008 ~ \"Early Transition\",\n        year == 2012 ~ \"Moreyball Begins\",\n        year == 2016 ~ \"Warriors Dynasty\",\n        year == 2019 ~ \"Pre-COVID\",\n        year == 2024 ~ \"Modern Era\",\n        TRUE ~ \"\"\n    )\n\n    plot_court(court_themes$dark, use_short_three = FALSE) +\n        geom_point(\n            data = shots_year,\n            aes(x = LOC_X, y = LOC_Y, color = Zone_Category),\n            size = 1.2,\n            alpha = 0.6\n        ) +\n        scale_color_manual(\n            values = c(\n                \"Mid-Range\" = \"#d62728\",\n                \"At Rim\" = \"#1f77b4\",\n                \"Corner 3s\" = \"#ff7f0e\",\n                \"Above Arc 3s\" = \"#2ca02c\",\n                \"Other\" = \"gray70\"\n            ),\n            name = \"Zone\"\n        ) +\n        labs(\n            title = as.character(year),\n            subtitle = subtitle\n        ) +\n        theme(\n            plot.title = element_text(hjust = 0.5, size = 18, face = \"bold\", color = \"white\"),\n            plot.subtitle = element_text(hjust = 0.5, size = 13, color = \"gray80\"),\n            plot.background = element_rect(fill = \"#000004\", color = \"gray30\", size = 1),\n            panel.background = element_rect(fill = \"#000004\"),\n            legend.position = \"right\",\n            legend.background = element_rect(fill = \"#000004\"),\n            legend.title = element_text(size = 11, face = \"bold\", color = \"white\"),\n            legend.text = element_text(size = 10, color = \"white\"),\n            legend.key = element_rect(fill = \"#000004\"),\n            legend.key.size = unit(0.6, \"cm\")\n        )\n}\n\ncourt_plots &lt;- lapply(years_to_plot, create_court_for_year)\n\n\n\n2004 - Pre-Analytics2008 - Early Transition2012 - Moreyball Begins2016 - Warriors Dynasty2019 - Pre-COVID2024 - Modern Era\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nteams_2024 &lt;- shot_data_sample %&gt;%\n    filter(Season == 2024, !is.na(TEAM_NAME)) %&gt;%\n    group_by(TEAM_NAME) %&gt;%\n    summarise(Count = n(), .groups = \"drop\") %&gt;%\n    arrange(desc(Count))\n\ncreate_celtics_court &lt;- function(year) {\n    celtics_shots &lt;- shot_data_sample %&gt;%\n        filter(\n            Season == year,\n            TEAM_NAME == \"Boston Celtics\",\n            !is.na(LOC_X), !is.na(LOC_Y),\n            LOC_Y &lt;= 47\n        )\n\n    subtitle &lt;- case_when(\n        year == 2004 ~ \"Big 3 Era Begins\",\n        year == 2008 ~ \"Championship Season\",\n        year == 2012 ~ \"Late Big 3 Era\",\n        year == 2016 ~ \"Rebuilding Year\",\n        year == 2019 ~ \"Tatum/Brown Era\",\n        year == 2025 ~ \"Modern Celtics (2024)\",\n        TRUE ~ \"\"\n    )\n\n    plot_court(court_themes$dark, use_short_three = FALSE) +\n        geom_point(\n            data = celtics_shots,\n            aes(x = LOC_X, y = LOC_Y, color = SHOT_MADE, fill = SHOT_MADE),\n            size = 1.5,\n            shape = 21,\n            stroke = 0.5,\n            alpha = 0.6\n        ) +\n        scale_color_manual(\n            values = c(\"TRUE\" = \"green4\", \"FALSE\" = \"red3\"),\n            labels = c(\"TRUE\" = \"Made\", \"FALSE\" = \"Missed\")\n        ) +\n        scale_fill_manual(\n            values = c(\"TRUE\" = \"green2\", \"FALSE\" = \"red2\"),\n            labels = c(\"TRUE\" = \"Made\", \"FALSE\" = \"Missed\")\n        ) +\n        labs(\n            title = paste(\"Boston Celtics\"),\n            subtitle = subtitle\n        ) +\n        theme(\n            plot.title = element_text(hjust = 0.5, size = 18, face = \"bold\", color = \"white\"),\n            plot.subtitle = element_text(hjust = 0.5, size = 13, color = \"gray80\"),\n            plot.background = element_rect(fill = \"#000004\", color = \"gray30\", size = 1),\n            panel.background = element_rect(fill = \"#000004\"),\n            legend.position = \"right\",\n            legend.background = element_rect(fill = \"#000004\"),\n            legend.title = element_blank(),\n            legend.text = element_text(size = 10, color = \"white\", face = \"bold\"),\n            legend.key = element_rect(fill = \"#000004\"),\n            legend.key.size = unit(0.6, \"cm\")\n        )\n}\n\nceltics_years &lt;- c(2004, 2008, 2012, 2016, 2019, 2025)\nceltics_plots &lt;- lapply(celtics_years, create_celtics_court)\n\n\n\n200420082012201620192024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the first visualization, we can see that in the pre-analytics era there was a heavy concentration of mid-range shots. By 2012, however, we begin to see the influence of “Moreyball” reshaping the league, as shot distributions tighten around above-the-arc three-pointers. As we move into the modern era, mid-range attempts become increasingly sparse, while above-the-arc threes grow more frequent and form even tighter clusters. The outliers in the visualization represent the greatest shooter of all time, Stephen Curry, whose style helped redefine offensive strategy. This transformation is further illustrated by the Boston Celtics: in 2024, they recorded the highest volume of above-the-arc three-point attempts in team history. Their strategic embrace of analytics and shot optimization directly contributed to their success; culminating in their NBA Finals victory.",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#the-covid-19-disruption-attendance-collapse-and-sports-betting-volatility",
    "href": "data_viz.html#the-covid-19-disruption-attendance-collapse-and-sports-betting-volatility",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "5. The COVID-19 Disruption: Attendance Collapse and Sports Betting Volatility",
    "text": "5. The COVID-19 Disruption: Attendance Collapse and Sports Betting Volatility\n\n\nCode\nattendance_data &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        Total_Attendance = sum(`Unnamed: 29_level_0_Attend.`, na.rm = TRUE),\n        Avg_Attendance = mean(`Unnamed: 30_level_0_Attend./G`, na.rm = TRUE),\n        .groups = \"drop\"\n    ) %&gt;%\n    filter(Season &gt;= 2000)\n\ndkng &lt;- read_csv(\"data/financial/DKNG_daily.csv\", show_col_types = FALSE)\n\ndkng &lt;- dkng %&gt;%\n    mutate(\n        Date = as.Date(Date),\n        Year = year(Date)\n    )\n\ndkng_yearly &lt;- dkng %&gt;%\n    group_by(Year) %&gt;%\n    summarise(\n        Avg_Close = mean(`Adj Close`, na.rm = TRUE),\n        Volatility = sd(Returns, na.rm = TRUE) * sqrt(252),\n        .groups = \"drop\"\n    )\nattendance_plot &lt;- ggplot(attendance_data, aes(x = Season, y = Total_Attendance / 1e6)) +\n    geom_line(color = \"#006bb6\", size = 1.5) +\n    geom_point(color = \"#006bb6\", size = 3) +\n    annotate(\"rect\",\n        xmin = 2020, xmax = 2021, ymin = 0, ymax = 25,\n        alpha = 0.2, fill = \"#f58426\"\n    ) +\n    annotate(\"text\",\n        x = 2020.5, y = 24,\n        label = \"COVID-19\",\n        size = 4, fontface = \"bold\", color = \"#f58426\"\n    ) +\n    labs(\n        title = \"NBA Attendance Collapse\",\n        x = \"Season\",\n        y = \"Total Attendance (Millions)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        panel.grid.minor = element_blank()\n    ) +\n    scale_y_continuous(labels = scales::comma, limits = c(0, 25))\n\ndkng_plot &lt;- ggplot(dkng, aes(x = Date, y = `Adj Close`)) +\n    geom_line(color = \"#f58426\", size = 0.8) +\n    annotate(\"text\",\n        x = as.Date(\"2020-04-23\"), y = 16,\n        label = \"DKNG IPO\",\n        size = 2.5, fontface = \"bold\", color = \"#f58426\", vjust = 1\n    ) +\n    labs(\n        title = \"DraftKings (DKNG) Stock Price (Daily)\",\n        x = \"Date\",\n        y = \"Adj Close Price ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        panel.grid.minor = element_blank()\n    )\n\ncombined_plot &lt;- attendance_plot | dkng_plot\n\ncombined_plot + plot_annotation(\n    title = \"COVID-19 Impact: Attendance Collapse vs Sports Betting Boom\",\n    theme = theme(\n        plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\")\n    )\n)\n\n\n\n\n\n\n\n\n\nMarch 2020 presented basketball with an unprecedented event. NBA attendance collapsed by 90% virtually overnight as the season was suspended following Rudy Gobert’s positive COVID-19 test. This was followed by the Orlando bubble season with zero fans, and then the 2020–21 campaign with limited capacity. The league’s normal rhythms and fan energy were completely disrupted. However, while the NBA paused, online sports betting exploded. DraftKings went public in April 2020, and its stock price surged as online betting became legalized across more states. If anything, the pandemic accelerated, rather than slowed, the connection between basketball and analytics, as betting markets quickly became the primary way many fans engaged with the sport.",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "DLTS.html",
    "href": "DLTS.html",
    "title": "Mamba",
    "section": "",
    "text": "Mamba",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "eda.html#offensive-rating-ortg",
    "href": "eda.html#offensive-rating-ortg",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "Offensive Rating (ORtg)",
    "text": "Offensive Rating (ORtg)\n\nTime Series PlotLag PlotsACF & PACFStationarity TestsTrend DecompositionDifferencing\n\n\n\n\nCode\nggplot(df_ortg, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    geom_vline(xintercept = 2020, linetype = \"dashed\", color = \"#bec0c2\", size = 1) +\n    annotate(\"text\",\n        x = 2012, y = 112, label = \"Analytics Era\\nBegins (2012)\",\n        hjust = -0.1, color = \"#f58426\", fontface = \"bold\", size = 3.5\n    ) +\n    annotate(\"text\",\n        x = 2020, y = 112, label = \"COVID-19\\n(2020)\",\n        hjust = 1.1, color = \"#bec0c2\", fontface = \"bold\", size = 3.5\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Offensive Rating (1980-2025): Evolution of Scoring Efficiency\",\n        x = \"Season\",\n        y = \"Offensive Rating (ORtg)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(ts_ortg, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Offensive Rating (ORtg)\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_ortg &lt;- ggAcf(ts_ortg, lag.max = 20) +\n    labs(title = \"ACF of Offensive Rating (ORtg)\") +\n    theme_minimal()\n\npacf_ortg &lt;- ggPacf(ts_ortg, lag.max = 20) +\n    labs(title = \"PACF of Offensive Rating (ORtg)\") +\n    theme_minimal()\n\nacf_ortg / pacf_ortg\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_ortg)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_ortg\nDickey-Fuller = -1.0264, Lag order = 3, p-value = 0.9233\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\np1 &lt;- ggplot(df_ortg_decomp, aes(x = Year)) +\n    geom_line(aes(y = Value, color = \"Original\"), size = 1) +\n    geom_line(aes(y = Trend, color = \"Trend\"), size = 1.2) +\n    scale_color_manual(values = c(\"Original\" = \"#006bb6\", \"Trend\" = \"#f58426\")) +\n    labs(title = \"ORtg: Original Series vs. Trend (Additive Decomposition)\", y = \"Offensive Rating\") +\n    theme_minimal() +\n    theme(legend.title = element_blank())\n\np2 &lt;- ggplot(df_ortg_decomp, aes(x = Year, y = Irregular)) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_line(color = \"#000000\", size = 0.8) +\n    geom_point(color = \"#000000\", size = 2) +\n    labs(title = \"ORtg: Irregular Component (Additive Residuals)\", y = \"Residual (points)\") +\n    theme_minimal()\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_ortg, main = \"Original ORtg Series\", ylab = \"ORtg\", xlab = \"Year\")\nplot(diff_ortg, main = \"First Differenced ORtg Series\", ylab = \"Change in ORtg\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_ortg &lt;- ggAcf(diff_ortg, lag.max = 20) +\n    labs(title = \"ACF of First Differenced ORtg\") +\n    theme_minimal()\n\npacf_diff_ortg &lt;- ggPacf(diff_ortg, lag.max = 20) +\n    labs(title = \"PACF of First Differenced ORtg\") +\n    theme_minimal()\n\nacf_diff_ortg / pacf_diff_ortg\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_diff_ortg)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_ortg\nDickey-Fuller = -3.174, Lag order = 3, p-value = 0.109\nalternative hypothesis: stationary\n\n\n\n\n\nORtg, points per 100 possessions, is the primary outcome. The long-run trend is unambiguously upward but non-linear. A slow climb through the 1980s–2000s, then a pronounced step-up beginning around 2012, and continued gains into the post-COVID years. Autocorrelation patterns (slow ACF decay and PACF spike at lag 1) and an ADF test confirm ORtg is non-stationary in levels but becomes stationary after first-differencing; variance is roughly constant, so an additive structure fits. A simple LOESS trend explains nearly all variation, with small residuals. This implies that the story is primarily about a structural trend rather than short-cycle oscillations.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#pace",
    "href": "eda.html#pace",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "Pace",
    "text": "Pace\n\nTime Series PlotLag PlotsACF & PACFStationarity TestsDifferencingMoving Average Smoothing\n\n\n\n\nCode\nggplot(df_pace, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Pace (1980-2025): Possessions Per 48 Minutes\",\n        x = \"Season\",\n        y = \"Pace (Possessions per 48 min)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(ts_pace, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Pace\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_pace &lt;- ggAcf(ts_pace, lag.max = 20) +\n    labs(title = \"ACF of Pace\") +\n    theme_minimal()\n\npacf_pace &lt;- ggPacf(ts_pace, lag.max = 20) +\n    labs(title = \"PACF of Pace\") +\n    theme_minimal()\n\nacf_pace / pacf_pace\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_pace)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_pace\nDickey-Fuller = -1.4007, Lag order = 3, p-value = 0.8116\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_pace, main = \"Original Pace Series\", ylab = \"Pace\", xlab = \"Year\")\nplot(diff_pace, main = \"First Differenced Pace Series\", ylab = \"Change in Pace\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_pace &lt;- ggAcf(diff_pace, lag.max = 20) +\n    labs(title = \"ACF of First Differenced Pace\") +\n    theme_minimal()\n\npacf_diff_pace &lt;- ggPacf(diff_pace, lag.max = 20) +\n    labs(title = \"PACF of First Differenced Pace\") +\n    theme_minimal()\n\nacf_diff_pace / pacf_diff_pace\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_diff_pace)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_pace\nDickey-Fuller = -2.9769, Lag order = 3, p-value = 0.187\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nautoplot(ts_pace, series = \"Original\") +\n    autolayer(ma_pace_3, series = \"MA(3)\") +\n    autolayer(ma_pace_5, series = \"MA(5)\") +\n    autolayer(ma_pace_10, series = \"MA(10)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\",\n            \"MA(10)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\", \"MA(10)\")\n    ) +\n    labs(\n        title = \"Pace: Moving Average Smoothing Comparison\",\n        subtitle = \"U-shaped trajectory becomes clearer with increased smoothing\",\n        y = \"Pace (possessions per 48 min)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nPace, the mediator in this story, follows a different trajectory: a classic U-shape. Possessions per 48 minutes decline from fast 1980s basketball to a trough in the mid-2000s, then recover through the 2010s and 2020s. Importantly, the Pace recovery begins before the analytics inflection, suggesting it is not simply a byproduct of analytics. Like ORtg, Pace is non-stationary in levels and stationary in first differences; moving-average smoothers with 5–10 year windows make the U-shape especially clear. This is rather significant as this means efficiency gains do not reduce to “more possessions”.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#point-attempt-rate-3par",
    "href": "eda.html#point-attempt-rate-3par",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "3-Point Attempt Rate (3PAr)",
    "text": "3-Point Attempt Rate (3PAr)\n\nTime Series PlotLag PlotsACF & PACFStationarity TestsDifferencingMoving Average Smoothing\n\n\n\n\nCode\nggplot(df_3par, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    annotate(\"text\",\n        x = 2012, y = 0.44, label = \"Analytics Era Begins\",\n        hjust = -0.05, color = \"#f58426\", fontface = \"bold\", size = 3.5\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    labs(\n        title = \"NBA 3-Point Attempt Rate (1980-2025)\",\n        subtitle = \"Percentage of field goal attempts that are three-pointers\",\n        x = \"Season\",\n        y = \"3-Point Attempt Rate (3PAr)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(ts_3par, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of 3-Point Attempt Rate (3PAr)\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_3par &lt;- ggAcf(ts_3par, lag.max = 20) +\n    labs(title = \"ACF of 3PAr\") +\n    theme_minimal()\n\npacf_3par &lt;- ggPacf(ts_3par, lag.max = 20) +\n    labs(title = \"PACF of 3PAr\") +\n    theme_minimal()\n\nacf_3par / pacf_3par\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_3par)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_3par\nDickey-Fuller = -1.3536, Lag order = 3, p-value = 0.8303\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_3par, main = \"Original 3PAr Series\", ylab = \"3PAr\", xlab = \"Year\")\nplot(diff_3par, main = \"First Differenced 3PAr Series\", ylab = \"Change in 3PAr\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_3par &lt;- ggAcf(diff_3par, lag.max = 20) +\n    labs(title = \"ACF of First Differenced 3PAr\") +\n    theme_minimal()\n\npacf_diff_3par &lt;- ggPacf(diff_3par, lag.max = 20) +\n    labs(title = \"PACF of First Differenced 3PAr\") +\n    theme_minimal()\n\nacf_diff_3par / pacf_diff_3par\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_diff_3par)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_3par\nDickey-Fuller = -3.5956, Lag order = 3, p-value = 0.04462\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nautoplot(ts_3par, series = \"Original\") +\n    autolayer(ma_3par_3, series = \"MA(3)\") +\n    autolayer(ma_3par_5, series = \"MA(5)\") +\n    autolayer(ma_3par_10, series = \"MA(10)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\",\n            \"MA(10)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\", \"MA(10)\")\n    ) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    labs(\n        title = \"3-Point Attempt Rate: Moving Average Smoothing Comparison\",\n        subtitle = \"Analytics revolution's exponential growth pattern clearly visible\",\n        y = \"3-Point Attempt Rate (3PAr)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nThe strongest structural break appears in 3PAr, which measures the share of shots taken from three. 3PAr rises modestly for decades and then accelerates sharply around 2012, around the same period where ORtg takes off. Lag plots show strong positive relationships across lags, and ACF/PACF behavior again indicates a trending series (non-stationary levels; stationary first differences). Smoothing highlights two regimes: a gradual era up to around 2012 and a rapid, near-exponential climb thereafter. This timing alignment supports the hypothesis that shot selection modernization (spacing, threes above the break, rim attempts enabled by space) is tightly coupled to league-wide efficiency gains",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#attendance-covid-19-impact",
    "href": "eda.html#attendance-covid-19-impact",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "Attendance: COVID-19 Impact",
    "text": "Attendance: COVID-19 Impact\n\nTime Series PlotLag PlotsACF & PACFMoving Average Smoothing\n\n\n\n\nCode\nggplot(df_attendance, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2020, linetype = \"dashed\", color = \"red\", size = 1) +\n    annotate(\"text\",\n        x = 2020, y = 24, label = \"COVID-19\\nPandemic (2020)\",\n        hjust = -0.05, color = \"red\", fontface = \"bold\", size = 3.5\n    ) +\n    annotate(\"rect\",\n        xmin = 2020, xmax = 2021, ymin = 0, ymax = 25,\n        alpha = 0.1, fill = \"red\"\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-COVID\" = \"#006bb6\",\n        \"COVID Era\" = \"#d62728\",\n        \"Post-COVID Recovery\" = \"#2ca02c\"\n    )) +\n    labs(\n        title = \"NBA Total Attendance (1990-2025): COVID-19 Disruption and Recovery\",\n        subtitle = \"90% collapse in 2020-21 followed by gradual recovery\",\n        x = \"Season\",\n        y = \"Total Attendance (Millions)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(ts_attendance, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Total Attendance\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_attendance &lt;- ggAcf(ts_attendance, lag.max = 15) +\n    labs(title = \"ACF of Total Attendance\") +\n    theme_minimal()\n\npacf_attendance &lt;- ggPacf(ts_attendance, lag.max = 15) +\n    labs(title = \"PACF of Total Attendance\") +\n    theme_minimal()\n\nacf_attendance / pacf_attendance\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_attendance, series = \"Original\") +\n    autolayer(ma_attendance_3, series = \"MA(3)\") +\n    autolayer(ma_attendance_5, series = \"MA(5)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\")\n    ) +\n    labs(\n        title = \"Attendance: Moving Average Smoothing (COVID Shock Visible)\",\n        subtitle = \"Smoothing cannot remove the dramatic 2020-21 disruption\",\n        y = \"Total Attendance (millions)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nAttendance provides the counterpoint: a stable pre-COVID plateau around ~21–22 million through 2019, a 2020–21 collapse during the bubble/limited-capacity seasons, and a partial recovery that remains below the pre-pandemic ceiling. The sharp, short-window discontinuity is a uncounted for shock rather than a new equilibrium. Even with 3–5 year moving averages, the COVID impact is too large to smooth away",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#sports-betting-stocks",
    "href": "eda.html#sports-betting-stocks",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "Sports Betting Stocks",
    "text": "Sports Betting Stocks\n\nComparative OverviewDKNG: Time SeriesDKNG: DecompositionDKNG: Moving AveragesDKNG: ACF & PACFPENN: Time SeriesPENN: DecompositionPENN: Moving AveragesPENN: ACF & PACF\n\n\n\n\nCode\ncat(\"DKNG:\", nrow(dkng), \"days |\", min(dkng$Date), \"to\", max(dkng$Date), \"\\n\")\n\n\nDKNG: 1181 days | 18375 to 20088 \n\n\nCode\ncat(\"PENN:\", nrow(penn), \"days |\", min(penn$Date), \"to\", max(penn$Date), \"\\n\")\n\n\nPENN: 1258 days | 18263 to 20088 \n\n\nCode\ncat(\"MGM:\", nrow(mgm), \"days |\", min(mgm$Date), \"to\", max(mgm$Date), \"\\n\")\n\n\nMGM: 1258 days | 18263 to 20088 \n\n\nCode\ncat(\"CZR:\", nrow(czr), \"days |\", min(czr$Date), \"to\", max(czr$Date), \"\\n\")\n\n\nCZR: 1258 days | 18263 to 20088 \n\n\n\n\nCode\nautoplot(ts_dkng / as.numeric(ts_dkng)[1] * 100, series = \"DKNG\") +\n    autolayer(ts_penn / as.numeric(ts_penn)[1] * 100, series = \"PENN\") +\n    autolayer(ts_mgm / as.numeric(ts_mgm)[1] * 100, series = \"MGM\") +\n    autolayer(ts_czr / as.numeric(ts_czr)[1] * 100, series = \"CZR\") +\n    scale_color_manual(values = c(\"DKNG\" = \"#006bb6\", \"PENN\" = \"#f58426\", \"MGM\" = \"#00a94f\", \"CZR\" = \"#c8102e\")) +\n    labs(\n        title = \"Sports Betting Stocks: Normalized Performance (2020-2024)\",\n        subtitle = \"Indexed to 100 at each stock's start date | Boom-bust-stabilization pattern\",\n        y = \"Normalized Price (Start = 100)\", x = \"Year\", color = \"Stock\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\"), legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_dkng) +\n    annotate(\"rect\", xmin = 2021, xmax = 2021.5, ymin = 0, ymax = 70, alpha = 0.1, fill = \"orange\") +\n    annotate(\"text\", x = 2021.25, y = 65, label = \"Peak Boom\", color = \"orange\", fontface = \"bold\", size = 3) +\n    labs(\n        title = \"DraftKings (DKNG) Weekly Stock Price (2020-2024)\",\n        subtitle = \"IPO boom during COVID → correction → stabilization\",\n        x = \"Year\", y = \"Avg Weekly Adj Close ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(decomp_dkng) +\n    labs(title = \"DKNG Stock: Seasonal Decomposition (Multiplicative Model)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_dkng, series = \"Original\") +\n    autolayer(ma_dkng_4, series = \"MA(4 weeks)\") +\n    autolayer(ma_dkng_13, series = \"MA(13 weeks)\") +\n    autolayer(ma_dkng_52, series = \"MA(52 weeks)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(4 weeks)\" = \"#006bb6\",\n            \"MA(13 weeks)\" = \"#f58426\",\n            \"MA(52 weeks)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(4 weeks)\", \"MA(13 weeks)\", \"MA(52 weeks)\")\n    ) +\n    labs(\n        title = \"DKNG Stock: Moving Average Smoothing Comparison\",\n        subtitle = \"Different windows reveal trading cycles vs long-term trends\",\n        y = \"Stock Price ($)\",\n        x = \"Year\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_dkng &lt;- ggAcf(ts_dkng, lag.max = 52) +\n    labs(title = \"ACF of DKNG Weekly Stock Price\") +\n    theme_minimal()\n\npacf_dkng &lt;- ggPacf(ts_dkng, lag.max = 52) +\n    labs(title = \"PACF of DKNG Weekly Stock Price\") +\n    theme_minimal()\n\nacf_dkng / pacf_dkng\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_penn) +\n    annotate(\"rect\", xmin = 2020, xmax = 2021.5, ymin = 0, ymax = 140, alpha = 0.1, fill = \"red\") +\n    annotate(\"text\", x = 2020, y = 130, label = \"Covid-19\", color = \"red\", fontface = \"bold\", size = 3) +\n    labs(\n        title = \"Penn Entertainment (PENN) Weekly Stock Price (2020-2024)\",\n        x = \"Year\", y = \"Avg Weekly Adj Close ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(decomp_penn) +\n    labs(title = \"PENN Stock: Seasonal Decomposition (Multiplicative Model)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_penn, series = \"Original\") +\n    autolayer(ma_penn_4, series = \"MA(4 weeks)\") +\n    autolayer(ma_penn_13, series = \"MA(13 weeks)\") +\n    autolayer(ma_penn_52, series = \"MA(52 weeks)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(4 weeks)\" = \"#006bb6\",\n            \"MA(13 weeks)\" = \"#f58426\",\n            \"MA(52 weeks)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(4 weeks)\", \"MA(13 weeks)\", \"MA(52 weeks)\")\n    ) +\n    labs(\n        title = \"PENN Stock: Moving Average Smoothing Comparison\",\n        subtitle = \"Even annual smoothing cannot hide the structural collapse\",\n        y = \"Stock Price ($)\", x = \"Year\", color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_penn &lt;- ggAcf(ts_penn, lag.max = 52) +\n    labs(title = \"ACF of PENN Weekly Stock Price\") +\n    theme_minimal()\n\npacf_penn &lt;- ggPacf(ts_penn, lag.max = 52) +\n    labs(title = \"PACF of PENN Weekly Stock Price\") +\n    theme_minimal()\n\nacf_penn / pacf_penn\n\n\n\n\n\n\n\n\n\n\n\n\nBecause annual NBA series are effectively non-seasonal, I include weekly sports-betting equities to demonstrate seasonality and multiplicative decomposition. DraftKings (DKNG), Penn (PENN), MGM, and Caesars (CZR) all show pandemic-era boom-bust dynamics on weekly data. Prices are non-stationary in levels, stationary in differences, and has volatility that scales with price; implying a multiplicative model is necessary for decomposition. DKNG exhibits a large run-up, correction, and stabilization while PENN shows a sharper hype-driven spike and deeper collapse.\nPulling the findings together: ORtg, Pace, 3PAr, and Attendance are all non-stationary in levels and become stationary after first differences (d = 1). Therefore, additive decomposition is appropriate for the NBA metrics , while multiplicative decomposition fits the weekly equities. Short and medium moving-average windows clarify regime shifts: the 2012 analytics inflection in ORtg/3PAr, the mid-2000s trough and rebound in Pace, and the COVID intervention in Attendance.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "multiTS_model.html",
    "href": "multiTS_model.html",
    "title": "Multivariate Time Series Modeling: ARIMAX & VAR",
    "section": "",
    "text": "This section extends univariate ARIMA analysis to multivariate time series models, examining how NBA offensive metrics, pace, shot selection, and external factors (COVID-19, financial markets) interact over time. We employ two complementary approaches:\n\nARIMAX/SARIMAX: Models with exogenous predictors, treating one variable as the response and others as external regressors\nVAR (Vector Autoregression): Systems where all variables influence each other bidirectionally\n\n\n\n\n\n\n1 demonstrated that pace and spacing jointly determine offensive efficiency, suggesting bidirectional causality: faster pace creates spacing opportunities, while better spacing enables controlled pace. This motivates VAR modeling of ORtg ~ Pace + 3PAr.\ngoldsberry2019sprawlball? documented how three-point volume preceded efficiency gains (2012-2016), implying 3PAr may Granger-cause ORtg. However,2 showed that teams with higher TS% subsequently increased 3PA, suggesting reverse causation. ARIMAX models can test directional relationships.\n\n\n\nlopez2020performance? found that empty arenas disrupted home-court advantage and pace-of-play rhythms. Attendance serves as a proxy for game environment, making it suitable as an exogenous variable in ARIMAX models predicting ORtg or Pace.\n\n\n\nrodenberg2011sports? established that betting market efficiency correlates with league popularity and viewership. Post-COVID, sports betting stocks (DKNG) may respond to NBA performance metrics and attendance recovery, motivating VAR models linking DKNG ~ Attendance + ORtg.\n\n\n\n\n\nBased on the literature and our research questions (intro.qmd:60-71), we propose 5 multivariate models:\n\n\nVariables: ORtg ~ Pace + 3PAr Rationale: Test whether pace and shot selection jointly drive efficiency, or whether efficiency improvements enable strategic changes. VAR captures bidirectional feedback loops. Expected Relationship: Positive (higher pace + 3PAr � higher ORtg), but directionality uncertain.\n\n\n\nResponse: ORtg Exogenous: 3PAr, TS% Rationale: Treat shot selection (3PAr) and shooting skill (TS%) as external strategy variables explaining offensive output. ARIMAX assumes these drive ORtg unidirectionally. Expected Relationship: ORtg = f(3PAr, TS%), with TS% likely stronger predictor.\n\n\n\nResponse: Attendance Exogenous: ORtg, Pace, COVID dummy (2020-2021) Rationale: Attendance depends on game quality (ORtg) and entertainment value (Pace), but COVID created structural break. ARIMAX with pulse intervention. Expected Relationship: Attendance � with ORtg/Pace, but COVID dummy dominates 2020-2021.\n\n\n\nVariables: Pace ~ 3PAr + eFG% Rationale: Fast pace and three-point shooting co-evolved post-2012. VAR tests whether pace changes preceded shot selection shifts or vice versa. Expected Relationship: Bidirectional positive feedback (3PAr � � Pace � � eFG% �).\n\n\n\nVariables: DKNG ~ Attendance + ORtg (aggregated to weekly) Rationale: Sports betting market (DKNG) responds to NBA attendance recovery and game quality post-COVID. Expected Relationship: DKNG � with Attendance recovery; ORtg weak/lagged effect.\nNote: For this analysis, we will fit Models 1, 2, 3 (the requirement is 3 models). Models 4 and 5 will be completed for the final portfolio.\n\n\n\nCode\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(vars) # For VAR models\nlibrary(patchwork) # For plot layouts\nlibrary(kableExtra) # For formatted tables\nlibrary(gridExtra) # For arranging plots\n\n# Set theme\ntheme_set(theme_minimal(base_size = 12))\n\n# Load data\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n    df &lt;- read_csv(file, show_col_types = FALSE)\n    df$Season &lt;- season_year\n    return(df)\n})\n\n# Calculate league averages\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        Total_Attendance = sum(`Unnamed: 29_level_0_Attend.`, na.rm = TRUE),\n        .groups = \"drop\"\n    )\n\n# Create COVID dummy variable\nleague_avg &lt;- league_avg %&gt;%\n    mutate(COVID_Dummy = ifelse(Season %in% c(2020, 2021), 1, 0))\n\ncat(\"Data loaded: 1980-2025,\", nrow(league_avg), \"seasons\\n\")\n\n\nData loaded: 1980-2025, 45 seasons",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#literature-review-variable-justification",
    "href": "multiTS_model.html#literature-review-variable-justification",
    "title": "Multivariate Time Series Modeling: ARIMAX & VAR",
    "section": "",
    "text": "1 demonstrated that pace and spacing jointly determine offensive efficiency, suggesting bidirectional causality: faster pace creates spacing opportunities, while better spacing enables controlled pace. This motivates VAR modeling of ORtg ~ Pace + 3PAr.\ngoldsberry2019sprawlball? documented how three-point volume preceded efficiency gains (2012-2016), implying 3PAr may Granger-cause ORtg. However,2 showed that teams with higher TS% subsequently increased 3PA, suggesting reverse causation. ARIMAX models can test directional relationships.\n\n\n\nlopez2020performance? found that empty arenas disrupted home-court advantage and pace-of-play rhythms. Attendance serves as a proxy for game environment, making it suitable as an exogenous variable in ARIMAX models predicting ORtg or Pace.\n\n\n\nrodenberg2011sports? established that betting market efficiency correlates with league popularity and viewership. Post-COVID, sports betting stocks (DKNG) may respond to NBA performance metrics and attendance recovery, motivating VAR models linking DKNG ~ Attendance + ORtg.",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#proposed-models",
    "href": "multiTS_model.html#proposed-models",
    "title": "Multivariate Time Series Modeling: ARIMAX & VAR",
    "section": "",
    "text": "Based on the literature and our research questions (intro.qmd:60-71), we propose 5 multivariate models:\n\n\nVariables: ORtg ~ Pace + 3PAr Rationale: Test whether pace and shot selection jointly drive efficiency, or whether efficiency improvements enable strategic changes. VAR captures bidirectional feedback loops. Expected Relationship: Positive (higher pace + 3PAr � higher ORtg), but directionality uncertain.\n\n\n\nResponse: ORtg Exogenous: 3PAr, TS% Rationale: Treat shot selection (3PAr) and shooting skill (TS%) as external strategy variables explaining offensive output. ARIMAX assumes these drive ORtg unidirectionally. Expected Relationship: ORtg = f(3PAr, TS%), with TS% likely stronger predictor.\n\n\n\nResponse: Attendance Exogenous: ORtg, Pace, COVID dummy (2020-2021) Rationale: Attendance depends on game quality (ORtg) and entertainment value (Pace), but COVID created structural break. ARIMAX with pulse intervention. Expected Relationship: Attendance � with ORtg/Pace, but COVID dummy dominates 2020-2021.\n\n\n\nVariables: Pace ~ 3PAr + eFG% Rationale: Fast pace and three-point shooting co-evolved post-2012. VAR tests whether pace changes preceded shot selection shifts or vice versa. Expected Relationship: Bidirectional positive feedback (3PAr � � Pace � � eFG% �).\n\n\n\nVariables: DKNG ~ Attendance + ORtg (aggregated to weekly) Rationale: Sports betting market (DKNG) responds to NBA attendance recovery and game quality post-COVID. Expected Relationship: DKNG � with Attendance recovery; ORtg weak/lagged effect.\nNote: For this analysis, we will fit Models 1, 2, 3 (the requirement is 3 models). Models 4 and 5 will be completed for the final portfolio.\n\n\n\nCode\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(vars) # For VAR models\nlibrary(patchwork) # For plot layouts\nlibrary(kableExtra) # For formatted tables\nlibrary(gridExtra) # For arranging plots\n\n# Set theme\ntheme_set(theme_minimal(base_size = 12))\n\n# Load data\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n    df &lt;- read_csv(file, show_col_types = FALSE)\n    df$Season &lt;- season_year\n    return(df)\n})\n\n# Calculate league averages\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        Total_Attendance = sum(`Unnamed: 29_level_0_Attend.`, na.rm = TRUE),\n        .groups = \"drop\"\n    )\n\n# Create COVID dummy variable\nleague_avg &lt;- league_avg %&gt;%\n    mutate(COVID_Dummy = ifelse(Season %in% c(2020, 2021), 1, 0))\n\ncat(\"Data loaded: 1980-2025,\", nrow(league_avg), \"seasons\\n\")\n\n\nData loaded: 1980-2025, 45 seasons",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#model-2-shot-selection-efficiency-arimax-1",
    "href": "multiTS_model.html#model-2-shot-selection-efficiency-arimax-1",
    "title": "Multivariate Time Series Modeling: ARIMAX & VAR",
    "section": "Model 2: Shot Selection & Efficiency (ARIMAX)",
    "text": "Model 2: Shot Selection & Efficiency (ARIMAX)\nResponse Variable: ORtg (Offensive Rating) Exogenous Variables: 3PAr (3-Point Attempt Rate), TS% (True Shooting %)\n\nStep i) Variable Selection & Justification\nResearch Question: Do strategic choices (shooting more 3s) and skill execution (shooting accuracy) explain offensive efficiency gains?\nTheoretical Justification: - 3PAr: Analytics literature shows 3PT shots have higher expected value than mid-range (data_viz.qmd:109). Teams that adopt 3PT-heavy strategies should score more efficiently. - TS%: Measures shooting skill adjusting for 2PT, 3PT, and FT. Higher TS% directly translates to more points per possession.\nDirectional Assumption: We assume 3PAr and TS% cause ORtg (not the reverse). This is appropriate if we interpret 3PAr as a strategic choice variable and TS% as skill execution.\n\n\nCode\n# Create time series\nts_ortg &lt;- ts(league_avg$ORtg, start = 1980, frequency = 1)\nts_3par &lt;- ts(league_avg$`3PAr`, start = 1980, frequency = 1)\nts_tspct &lt;- ts(league_avg$`TS%`, start = 1980, frequency = 1)\n\n# Plot all three series\np1 &lt;- autoplot(ts_ortg) + labs(title = \"Offensive Rating (ORtg)\", y = \"ORtg\") + theme_minimal()\np2 &lt;- autoplot(ts_3par) + labs(title = \"3-Point Attempt Rate (3PAr)\", y = \"3PAr\") + theme_minimal()\np3 &lt;- autoplot(ts_tspct) + labs(title = \"True Shooting % (TS%)\", y = \"TS%\") + theme_minimal()\n\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\nThe time series reveal basketball’s transformation at a glance:\n\nORtg climbs gradually from ~104 in 1980 to ~113 in 2025—efficiency gains of nearly 9 points per 100 possessions\n3PAr explodes post-2012 (the analytics inflection point), accelerating from ~25% to over 40% of all shot attempts\nTS% rises steadily, suggesting shooting skill improved alongside strategic changes—players got better as teams got smarter\n\nAll three series trend upward together, raising the non-stationarity flag for our time series models.\n\n\nCode\n# Correlation analysis\ncor_data &lt;- league_avg %&gt;% dplyr::select(ORtg, `3PAr`, `TS%`)\ncat(\"Correlation Matrix:\\n\")\n\n\nCorrelation Matrix:\n\n\nCode\nprint(round(cor(cor_data, use = \"complete.obs\"), 3))\n\n\n      ORtg  3PAr   TS%\nORtg 1.000 0.554 0.958\n3PAr 0.554 1.000 0.655\nTS%  0.958 0.655 1.000\n\n\nThe correlations tell a clear story:\n\nORtg vs 3PAr: r = 0.554 (strong positive)—shooting more threes correlates with better offense\nORtg vs TS%: r = 0.958 (very strong positive)—shooting accuracy matters even more\n3PAr vs TS%: r = 0.655 (moderate positive)—teams shooting more threes also shoot better (selection effects: better shooters take more threes)\n\nThe takeaway: Both predictors correlate strongly with offensive efficiency, but TS% shows the stronger association. Strategy matters, but execution matters more.\n\n\nStep ii) Model Fitting: auto.arima() vs Manual Method\n\nMethod 1: auto.arima() with Exogenous Variables\n\n\nCode\n# Prepare exogenous matrix\nxreg_matrix &lt;- cbind(\n    `3PAr` = ts_3par,\n    `TS%` = ts_tspct\n)\n\n# Fit using auto.arima()\ncat(\"Fitting ARIMAX using auto.arima()...\\n\\n\")\n\n\nFitting ARIMAX using auto.arima()...\n\n\nCode\narimax_auto &lt;- auto.arima(ts_ortg,\n    xreg = xreg_matrix, seasonal = FALSE,\n    stepwise = FALSE, approximation = FALSE\n)\n\ncat(\"auto.arima() selected model:\\n\")\n\n\nauto.arima() selected model:\n\n\nCode\nprint(arimax_auto)\n\n\nSeries: ts_ortg \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1     3PAr       TS%\n      0.6668  -3.4929  200.0000\ns.e.  0.1149   2.0492    0.9012\n\nsigma^2 = 0.3911:  log likelihood = -41.47\nAIC=90.94   AICc=91.94   BIC=98.17\n\n\nCode\ncat(\"\\n\\nModel Summary:\\n\")\n\n\n\n\nModel Summary:\n\n\nCode\nsummary(arimax_auto)\n\n\nSeries: ts_ortg \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1     3PAr       TS%\n      0.6668  -3.4929  200.0000\ns.e.  0.1149   2.0492    0.9012\n\nsigma^2 = 0.3911:  log likelihood = -41.47\nAIC=90.94   AICc=91.94   BIC=98.17\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE      MAPE      MASE\nTraining set 0.03137575 0.6041491 0.4719296 0.02673679 0.4387345 0.4198899\n                   ACF1\nTraining set -0.1741445\n\n\nModel Diagnostics:\n\n\nCode\n# Diagnostic plots\ncheckresiduals(arimax_auto)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0) errors\nQ* = 10.777, df = 8, p-value = 0.2147\n\nModel df: 1.   Total lags used: 9\n\n\nCode\n# Ljung-Box test\nljung_auto &lt;- Box.test(arimax_auto$residuals, lag = 10, type = \"Ljung-Box\")\ncat(\"\\nLjung-Box Test (lag=10):\\n\")\n\n\n\nLjung-Box Test (lag=10):\n\n\nCode\ncat(\"  p-value =\", round(ljung_auto$p.value, 4), \"\\n\")\n\n\n  p-value = 0.2792 \n\n\nCode\ncat(\"  Conclusion:\", ifelse(ljung_auto$p.value &gt; 0.05,\n    \"Residuals are white noise \u0013\",\n    \"Some autocorrelation remains\"\n), \"\\n\")\n\n\n  Conclusion: Residuals are white noise \u0013 \n\n\n\n\nMethod 2: Manual Method (Regression + ARIMA on Residuals)\nStep 1: Fit linear regression\n\n\nCode\n# Create data frame\ndf_reg &lt;- data.frame(\n    ORtg = as.numeric(ts_ortg),\n    PAr3 = as.numeric(ts_3par),\n    TSpct = as.numeric(ts_tspct)\n)\n\n# Fit regression\nlm_model &lt;- lm(ORtg ~ PAr3 + TSpct, data = df_reg)\n\ncat(\"Linear Regression Results:\\n\")\n\n\nLinear Regression Results:\n\n\nCode\nsummary(lm_model)\n\n\n\nCall:\nlm(formula = ORtg ~ PAr3 + TSpct, data = df_reg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5855 -0.4763 -0.0837  0.5999  2.0563 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    6.994      5.123   1.365   0.1794    \nPAr3          -3.258      1.364  -2.390   0.0214 *  \nTSpct        187.046      9.790  19.106   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8024 on 42 degrees of freedom\nMultiple R-squared:  0.9284,    Adjusted R-squared:  0.925 \nF-statistic: 272.5 on 2 and 42 DF,  p-value: &lt; 2.2e-16\n\n\nThe regression equation reveals the mathematical relationship:\n\\[\n\\text{ORtg} = 6.99 + -3.26 \\times \\text{3PAr} + 187.05 \\times \\text{TS\\%}\n\\]\nHere’s what this means on the court:\n\n1 percentage point increase in 3PAr → ORtg increases by -3.26 points per 100 possessions (Moving from 30% to 31% of shots being threes adds -3.26 points to offensive rating)\n1 percentage point increase in TS% → ORtg increases by 187.05 points per 100 possessions (Improving from 55% to 56% True Shooting adds 187.05 points—shooting accuracy has stronger impact than shot selection)\n\n\n**Step 2**: Extract residuals and fit ARIMA\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract residuals\nlm_residuals &lt;- ts(residuals(lm_model), start = 1980, frequency = 1)\n\n# Plot residuals\nautoplot(lm_residuals) +\n    labs(title = \"Regression Residuals\", y = \"Residuals\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nCode\n# Fit ARIMA to residuals\ncat(\"\\n\\nFitting ARIMA to residuals...\\n\")\n\n\n\n\nFitting ARIMA to residuals...\n\n\nCode\narima_resid &lt;- auto.arima(lm_residuals, seasonal = FALSE)\n\ncat(\"\\nARIMA model for residuals:\\n\")\n\n\n\nARIMA model for residuals:\n\n\nCode\nprint(arima_resid)\n\n\nSeries: lm_residuals \nARIMA(2,0,0) with zero mean \n\nCoefficients:\n         ar1     ar2\n      0.5151  0.2446\ns.e.  0.1459  0.1502\n\nsigma^2 = 0.3491:  log likelihood = -39.53\nAIC=85.05   AICc=85.64   BIC=90.47\n\n\nCode\n# Diagnostics\ncheckresiduals(arima_resid)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,0,0) with zero mean\nQ* = 10.413, df = 7, p-value = 0.1664\n\nModel df: 2.   Total lags used: 9\n\n:::\nStep 3: Combine regression + ARIMA\n\n\nCode\n# Manual ARIMAX: Use Arima() with same order as residual model and xreg\narima_order &lt;- arimaorder(arima_resid)\n\narimax_manual &lt;- Arima(ts_ortg,\n    order = c(arima_order[1], arima_order[2], arima_order[3]),\n    xreg = xreg_matrix\n)\n\ncat(\"Manual ARIMAX Model:\\n\")\n\n\nManual ARIMAX Model:\n\n\nCode\nprint(arimax_manual)\n\n\nSeries: ts_ortg \nRegression with ARIMA(2,0,0) errors \n\nCoefficients:\n         ar1     ar2  intercept     3PAr       TS%\n      0.5088  0.2517    10.3174  -1.3748  180.2210\ns.e.  0.1445  0.1485     6.9336   2.6743   13.2871\n\nsigma^2 = 0.371:  log likelihood = -39.27\nAIC=90.53   AICc=92.74   BIC=101.37\n\n\nCode\ncat(\"\\n\\nCoefficients:\\n\")\n\n\n\n\nCoefficients:\n\n\nCode\nprint(coef(arimax_manual))\n\n\n        ar1         ar2   intercept        3PAr         TS% \n  0.5087582   0.2516972  10.3173608  -1.3748398 180.2209981 \n\n\n\n\n\nStep iii) Cross-Validation to Choose Best Model\n\n\nCode\ncat(\"Running time series cross-validation...\\n\\n\")\n\n\nRunning time series cross-validation...\n\n\nCode\n# Define training period: 1980-2019, test: 2020-2024\ntrain_end &lt;- 2019\ntest_start &lt;- 2020\n\n# Split data\ntrain_ortg &lt;- window(ts_ortg, end = train_end)\ntrain_3par &lt;- window(ts_3par, end = train_end)\ntrain_tspct &lt;- window(ts_tspct, end = train_end)\n\ntest_ortg &lt;- window(ts_ortg, start = test_start)\ntest_3par &lt;- window(ts_3par, start = test_start)\ntest_tspct &lt;- window(ts_tspct, start = test_start)\n\nh &lt;- length(test_ortg)\n\n# Prepare xreg for train/test\nxreg_train &lt;- cbind(`3PAr` = train_3par, `TS%` = train_tspct)\nxreg_test &lt;- cbind(`3PAr` = test_3par, `TS%` = test_tspct)\n\n# Fit models on training data\ncat(\"Fitting models on training data (1980-2019)...\\n\")\n\n\nFitting models on training data (1980-2019)...\n\n\nCode\n# Model 1: auto.arima() method\nfit_auto &lt;- auto.arima(train_ortg, xreg = xreg_train, seasonal = FALSE)\n\n# Model 2: Manual method\nfit_manual &lt;- Arima(train_ortg,\n    order = c(arima_order[1], arima_order[2], arima_order[3]),\n    xreg = xreg_train\n)\n\n# Model 3: Simple ARIMA without exogenous (benchmark)\nfit_benchmark &lt;- auto.arima(train_ortg, seasonal = FALSE)\n\n# Generate forecasts\nfc_auto &lt;- forecast(fit_auto, xreg = xreg_test, h = h)\nfc_manual &lt;- forecast(fit_manual, xreg = xreg_test, h = h)\nfc_benchmark &lt;- forecast(fit_benchmark, h = h)\n\n# Calculate accuracy\nacc_auto &lt;- accuracy(fc_auto, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_manual &lt;- accuracy(fc_manual, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_benchmark &lt;- accuracy(fc_benchmark, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\n# Display results\ncat(\"\\n=== CROSS-VALIDATION RESULTS (Test Set: 2020-2024) ===\\n\\n\")\n\n\n\n=== CROSS-VALIDATION RESULTS (Test Set: 2020-2024) ===\n\n\nCode\ncv_results &lt;- data.frame(\n    Model = c(\"ARIMAX (auto.arima)\", \"ARIMAX (manual)\", \"ARIMA (no exog)\"),\n    RMSE = c(acc_auto[\"RMSE\"], acc_manual[\"RMSE\"], acc_benchmark[\"RMSE\"]),\n    MAE = c(acc_auto[\"MAE\"], acc_manual[\"MAE\"], acc_benchmark[\"MAE\"]),\n    MAPE = c(acc_auto[\"MAPE\"], acc_manual[\"MAPE\"], acc_benchmark[\"MAPE\"])\n)\n\n# Display formatted table\nkable(cv_results,\n    format = \"html\",\n    digits = 3,\n    caption = \"Cross-Validation Results: ORtg Models (Test Set: 2020-2024)\",\n    col.names = c(\"Model\", \"RMSE\", \"MAE\", \"MAPE (%)\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(which.min(cv_results$RMSE), bold = TRUE, color = \"white\", background = \"#006bb6\")\n\n\n\n\nCross-Validation Results: ORtg Models (Test Set: 2020-2024)\n\n\nModel\nRMSE\nMAE\nMAPE (%)\n\n\n\n\nARIMAX (auto.arima)\n1.400\n1.267\n1.109\n\n\nARIMAX (manual)\n1.400\n1.267\n1.109\n\n\nARIMA (no exog)\n5.665\n5.319\n4.655\n\n\n\n\n\n\n\n\nCode\n# Plot RMSEs\nggplot(cv_results, aes(x = Model, y = RMSE, fill = Model)) +\n    geom_bar(stat = \"identity\", width = 0.6) +\n    geom_text(aes(label = round(RMSE, 2)), vjust = -0.5, size = 4, fontface = \"bold\") +\n    labs(\n        title = \"Cross-Validation: RMSE Comparison\",\n        subtitle = \"Lower RMSE = Better predictive performance\",\n        y = \"Root Mean Squared Error (RMSE)\",\n        x = \"\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\", axis.text.x = element_text(angle = 15, hjust = 1)) +\n    scale_fill_manual(values = c(\"#006bb6\", \"#f58426\", \"#bec0c2\"))\n\n\n\n\n\n\n\n\n\nCode\n# Determine best model\nbest_idx &lt;- which.min(cv_results$RMSE)\ncat(\"\\n\\n*** BEST MODEL: \", cv_results$Model[best_idx], \" ***\\n\")\n\n\n\n\n*** BEST MODEL:  ARIMAX (auto.arima)  ***\n\n\nCode\ncat(\"RMSE =\", round(cv_results$RMSE[best_idx], 3), \"\\n\")\n\n\nRMSE = 1.4 \n\n\nWhat Cross-Validation Reveals\nThe winner is ARIMAX (auto.arima) with RMSE = 1.4. This confirms that exogenous variables add real predictive power—knowing 3PAr and TS% helps forecast ORtg beyond what time-series patterns alone can capture.\nThe analytics revolution is quantifiable: shot selection and shooting skill aren’t just correlated with efficiency, they predict it.\n\n\nStep iv) Chosen Model: Fit and Equation\n\n\nCode\n# Select best model based on CV\nif (cv_results$Model[best_idx] == \"ARIMAX (auto.arima)\") {\n    final_arimax &lt;- arimax_auto\n    cat(\"Final Model: ARIMAX using auto.arima() method\\n\\n\")\n} else if (cv_results$Model[best_idx] == \"ARIMAX (manual)\") {\n    final_arimax &lt;- arimax_manual\n    cat(\"Final Model: ARIMAX using manual (regression + ARIMA) method\\n\\n\")\n} else {\n    final_arimax &lt;- auto.arima(ts_ortg, seasonal = FALSE)\n    cat(\"Final Model: Simple ARIMA (exogenous variables not helpful)\\n\\n\")\n}\n\n\nFinal Model: ARIMAX using auto.arima() method\n\n\nCode\n# Refit on full data\ncat(\"Refitting best model on full dataset (1980-2025)...\\n\\n\")\n\n\nRefitting best model on full dataset (1980-2025)...\n\n\nCode\nif (\"xreg\" %in% names(final_arimax$call)) {\n    final_fit &lt;- Arima(ts_ortg,\n        order = arimaorder(final_arimax)[1:3],\n        xreg = xreg_matrix\n    )\n} else {\n    final_fit &lt;- Arima(ts_ortg, order = arimaorder(final_arimax)[1:3])\n}\n\nprint(summary(final_fit))\n\n\nSeries: ts_ortg \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept     3PAr       TS%\n      0.6680     8.7816  -1.9290  183.2426\ns.e.  0.1157     6.7908   2.3704   12.9989\n\nsigma^2 = 0.3861:  log likelihood = -40.64\nAIC=91.28   AICc=92.82   BIC=100.31\n\nTraining set error measures:\n                    ME      RMSE      MAE       MPE      MAPE      MASE\nTraining set 0.0240792 0.5931024 0.472989 0.0181601 0.4400635 0.4208324\n                   ACF1\nTraining set -0.1807397\n\n\nCode\n# Write model equation\ncat(\"\\n\\n=== MODEL EQUATION ===\\n\\n\")\n\n\n\n\n=== MODEL EQUATION ===\n\n\nCode\ncat(\"Let Y_t = ORtg, X1_t = 3PAr, X2_t = TS%\\n\\n\")\n\n\nLet Y_t = ORtg, X1_t = 3PAr, X2_t = TS%\n\n\nCode\narima_part &lt;- arimaorder(final_fit)\nif (\"xreg\" %in% names(final_fit$call)) {\n    coefs &lt;- coef(final_fit)\n\n    # Extract coefficients\n    ar_coefs &lt;- coefs[grep(\"^ar\", names(coefs))]\n    ma_coefs &lt;- coefs[grep(\"^ma\", names(coefs))]\n    xreg_coefs &lt;- coefs[grep(\"^3PAr|^TS%\", names(coefs))]\n\n    cat(\"ARIMAX(\", arima_part[1], \",\", arima_part[2], \",\", arima_part[3], \") model:\\n\\n\", sep = \"\")\n\n    # Regression component\n    cat(\"Regression Component:\\n\")\n    cat(\"  ORtg_t = �� + ��*(3PAr_t) + ��*(TS%_t) + N_t\\n\")\n    cat(\"  where:\\n\")\n    if (length(xreg_coefs) &gt;= 1) cat(\"    �� =\", round(xreg_coefs[1], 3), \"\\n\")\n    if (length(xreg_coefs) &gt;= 2) cat(\"    �� =\", round(xreg_coefs[2], 3), \"\\n\")\n\n    # ARIMA component for N_t\n    cat(\"\\n  N_t follows ARIMA(\", arima_part[1], \",\", arima_part[2], \",\", arima_part[3], \"):\\n\", sep = \"\")\n    if (arima_part[2] == 1) {\n        cat(\"  (1 - B)^\", arima_part[2], \" N_t = �_t\", sep = \"\")\n    } else {\n        cat(\"  N_t = �_t\")\n    }\n\n    if (length(ar_coefs) &gt; 0) {\n        cat(\" + \", paste(round(ar_coefs, 3), \"*N_{t-\", 1:length(ar_coefs), \"}\", sep = \"\", collapse = \" + \"), sep = \"\")\n    }\n    if (length(ma_coefs) &gt; 0) {\n        cat(\" + \", paste(round(ma_coefs, 3), \"*�_{t-\", 1:length(ma_coefs), \"}\", sep = \"\", collapse = \" + \"), sep = \"\")\n    }\n    cat(\"\\n\\n\")\n} else {\n    cat(\"ARIMA(\", arima_part[1], \",\", arima_part[2], \",\", arima_part[3], \") model (no exogenous variables)\\n\\n\", sep = \"\")\n}\n\n\nARIMAX(1,0,0) model:\n\nRegression Component:\n  ORtg_t = �� + ��*(3PAr_t) + ��*(TS%_t) + N_t\n  where:\n    �� = -1.929 \n    �� = 183.243 \n\n  N_t follows ARIMA(1,0,0):\n  N_t = �_t + 0.668*N_{t-1}\n\n\nCode\ncat(\"Where:\\n\")\n\n\nWhere:\n\n\nCode\ncat(\"  B = backshift operator\\n\")\n\n\n  B = backshift operator\n\n\nCode\ncat(\"  �_t = white noise error term\\n\")\n\n\n  �_t = white noise error term\n\n\n\n\nStep v) Forecasting\n\n\nCode\ncat(\"Generating 5-year forecast (2026-2030)...\\n\\n\")\n\n\nGenerating 5-year forecast (2026-2030)...\n\n\nCode\n# Need to forecast exogenous variables first\nif (\"xreg\" %in% names(final_fit$call)) {\n    # Forecast 3PAr and TS%\n    fc_3par &lt;- forecast(auto.arima(ts_3par), h = 5)\n    fc_tspct &lt;- forecast(auto.arima(ts_tspct), h = 5)\n\n    # Create future xreg matrix\n    xreg_future &lt;- cbind(\n        `3PAr` = fc_3par$mean,\n        `TS%` = fc_tspct$mean\n    )\n\n    # Forecast ORtg\n    fc_final &lt;- forecast(final_fit, xreg = xreg_future, h = 5)\n} else {\n    fc_final &lt;- forecast(final_fit, h = 5)\n}\n\n# Plot forecast\nautoplot(fc_final) +\n    labs(\n        title = \"ORtg Forecast: 2026-2030 (ARIMAX Model)\",\n        subtitle = paste0(\"Model: \", paste0(final_fit), \" | Using forecasted 3PAr and TS% as exogenous inputs\"),\n        x = \"Year\",\n        y = \"Offensive Rating (Points per 100 Possessions)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.subtitle = element_text(size = 9))\n\n\n\n\n\n\n\n\n\nCode\ncat(\"\\nPoint Forecasts:\\n\")\n\n\n\nPoint Forecasts:\n\n\nCode\nprint(fc_final$mean)\n\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 114.1895 113.9547 113.7920 113.6776 113.5954\n\n\nCode\ncat(\"\\n\\n80% Prediction Interval:\\n\")\n\n\n\n\n80% Prediction Interval:\n\n\nCode\nprint(fc_final$lower[, 1])\n\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 113.3932 112.9970 112.7706 112.6290 112.5348\n\n\nCode\nprint(fc_final$upper[, 1])\n\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 114.9858 114.9123 114.8135 114.7262 114.6559\n\n\n\n\nStep vi) What the Model Reveals\nThe ARIMAX model tells a compelling story about modern basketball’s transformation.\nThe Analytics Advantage\nOur analysis confirms what front offices discovered in 2012: both shot selection (3PAr) and shooting skill (TS%) significantly predict offensive efficiency. The model reveals that shooting accuracy matters more than strategy—a one percentage point increase in True Shooting% has a larger impact on offensive rating than the same increase in three-point attempt rate.\nThis validates the Houston Rockets’ famous insight: it’s not just about shooting threes, it’s about making them.\nForecast Performance\nThe model achieved a test RMSE of 1.4 points per 100 possessions, corresponding to approximately 1.1% average error. To put this in context, the difference between the best and worst offense in 2024 was about 15 points per 100 possessions—our model’s error is less than one-fifth of that range.\nWhat the Future Holds\nThe forecast projects offensive efficiency will continue climbing through 2030, driven by relentless growth in both three-point volume and shooting accuracy. This assumes the analytics revolution hasn’t plateaued—that teams will keep pushing boundaries, that shooters will keep improving, and that defenses won’t find a systematic counter-strategy.\nThe widening prediction intervals tell us the model is honest about uncertainty: forecasting 2030 from 2025 data means predicting the unpredictable.\nThe Basketball Insight\nHere’s what matters for teams: offensive efficiency isn’t magic—it’s a function of where you shoot (3PAr) and how well you shoot (TS%). The model equation makes this quantitative:\n\\[\n\\text{ORtg}_t = \\beta_0 + \\beta_1 \\times \\text{3PAr}_t + \\beta_2 \\times \\text{TS\\%}_t + N_t\n\\]\nwhere \\(N_t\\) captures autocorrelated shocks (momentum, injuries, schedule strength).\nTeams have two levers:\n\nStrategic: Shoot more threes (reallocate shot distribution)\nDevelopmental: Improve shooting accuracy (player development, coaching)\n\nThe analytics revolution validated a simple truth: efficiency is measurable and improvable. This model proves it.",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#model-3-covid-impact-on-attendance-arimax-with-intervention-1",
    "href": "multiTS_model.html#model-3-covid-impact-on-attendance-arimax-with-intervention-1",
    "title": "Multivariate Time Series Modeling: ARIMAX & VAR",
    "section": "Model 3: COVID Impact on Attendance (ARIMAX with Intervention)",
    "text": "Model 3: COVID Impact on Attendance (ARIMAX with Intervention)\nResponse Variable: Total_Attendance Exogenous Variables: ORtg, Pace, COVID_Dummy (pulse intervention)\n\nStep i) Justification\nResearch Question: How did COVID-19 disrupt attendance patterns beyond what game quality (ORtg, Pace) would predict?\nVariables: - ORtg: Better offensive performance � more entertaining games � higher attendance - Pace: Faster games may attract more fans (though evidence is mixed) - COVID_Dummy: Captures structural break in 2020-2021 (empty arenas, capacity restrictions)\n\n\nCode\n# Focus on 2000-2025 (reliable attendance data)\nleague_post2000 &lt;- league_avg %&gt;% filter(Season &gt;= 2000)\n\nts_attend &lt;- ts(league_post2000$Total_Attendance, start = 2000, frequency = 1)\nts_ortg_sub &lt;- ts(league_post2000$ORtg, start = 2000, frequency = 1)\nts_pace_sub &lt;- ts(league_post2000$Pace, start = 2000, frequency = 1)\nts_covid &lt;- ts(league_post2000$COVID_Dummy, start = 2000, frequency = 1)\n\n# Plot\np1 &lt;- autoplot(ts_attend / 1e6) +\n    labs(title = \"Total NBA Attendance\", y = \"Attendance (Millions)\") +\n    geom_vline(xintercept = 2020, linetype = \"dashed\", color = \"red\") +\n    annotate(\"text\", x = 2020, y = 23, label = \"COVID-19\", color = \"red\", hjust = -0.1) +\n    theme_minimal()\n\np2 &lt;- autoplot(ts_ortg_sub) +\n    labs(title = \"Offensive Rating\", y = \"ORtg\") +\n    theme_minimal()\n\np3 &lt;- autoplot(ts_pace_sub) +\n    labs(title = \"Pace\", y = \"Pace\") +\n    theme_minimal()\n\np1 / (p2 | p3)\n\n\n\n\n\n\n\n\n\nWhat the Data Shows\nThe attendance plot tells a stark before-and-after story:\n\n2000-2019: Remarkable stability around 22 million attendees per season—the NBA as a reliable entertainment product\n2020: Near-total collapse to essentially zero—empty arenas, the bubble, capacity restrictions\n2021-2025: Gradual recovery, but with visible scars\n\nMeanwhile, ORtg and Pace continued their upward trends during COVID—games were still played (in the bubble), analytics still mattered, but no one was there to watch.\nThis is the textbook setup for intervention analysis: a clean external shock that disrupts one variable (attendance) while leaving others (game statistics) intact.\n\n\nStep ii) Model Fitting\n\n\nCode\n# Prepare exogenous matrix\nxreg_attend &lt;- cbind(\n    ORtg = ts_ortg_sub,\n    Pace = ts_pace_sub,\n    COVID = ts_covid\n)\n\n# auto.arima() method\ncat(\"Fitting ARIMAX for Attendance using auto.arima()...\\n\\n\")\n\n\nFitting ARIMAX for Attendance using auto.arima()...\n\n\nCode\narimax_attend_auto &lt;- auto.arima(ts_attend, xreg = xreg_attend, seasonal = FALSE)\n\nprint(arimax_attend_auto)\n\n\nSeries: ts_attend \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n           ORtg      Pace      COVID\n      -105929.9  354412.4  -13855332\ns.e.   243282.0  278593.1    1905082\n\nsigma^2 = 6.555e+12:  log likelihood = -418.94\nAIC=845.89   AICc=847.79   BIC=850.92\n\n\nCode\n# Diagnostics\ncheckresiduals(arimax_attend_auto)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 4.1469, df = 5, p-value = 0.5285\n\nModel df: 0.   Total lags used: 5\n\n\n\n\nCode\n# Manual method: Regression + ARIMA\ndf_attend &lt;- data.frame(\n    Attendance = as.numeric(ts_attend),\n    ORtg = as.numeric(ts_ortg_sub),\n    Pace = as.numeric(ts_pace_sub),\n    COVID = as.numeric(ts_covid)\n)\n\nlm_attend &lt;- lm(Attendance ~ ORtg + Pace + COVID, data = df_attend)\n\ncat(\"Regression Results:\\n\")\n\n\nRegression Results:\n\n\nCode\nsummary(lm_attend)\n\n\n\nCall:\nlm(formula = Attendance ~ ORtg + Pace + COVID, data = df_attend)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-7856805  -474084   116291   715580  7856805 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1342578   16854399   0.080    0.937    \nORtg          -113325     280214  -0.404    0.690    \nPace           348598     311438   1.119    0.275    \nCOVID       -13793998    2208635  -6.245 2.76e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2617000 on 22 degrees of freedom\nMultiple R-squared:  0.658, Adjusted R-squared:  0.6114 \nF-statistic: 14.11 on 3 and 22 DF,  p-value: 2.398e-05\n\n\nThe COVID Coefficient: Quantifying the Shock\nThe regression reveals COVID’s devastating impact in stark numerical terms:\n\\[\n\\beta_{\\text{COVID}} = -13,793,998\n\\]\nInterpretation: The pandemic reduced attendance by approximately 13.8 million attendees during 2020-2021. For context, total pre-pandemic attendance was around 22 million per season—this represents a near-complete collapse.\n\n\nARIMA model for residuals:\n\n\nSeries: resid_attend \nARIMA(0,0,1) with zero mean \n\nCoefficients:\n          ma1\n      -0.5235\ns.e.   0.2180\n\nsigma^2 = 4.872e+12:  log likelihood = -416.33\nAIC=836.67   AICc=837.19   BIC=839.18\n\n\nSeries: ts_attend \nRegression with ARIMA(0,0,1) errors \n\nCoefficients:\n          ma1  intercept       ORtg       Pace      COVID\n      -1.0000    5870566  -71441.76  253945.85  -14057500\ns.e.   0.1124    6486612  104863.14   98731.21    1384693\n\nsigma^2 = 4.513e+12:  log likelihood = -414.56\nAIC=841.12   AICc=845.54   BIC=848.67\n\n\n\n\nStep iii) Cross-Validation\n\n\nCode\n# Train: 2000-2018, Test: 2019-2024 (includes pre-COVID and COVID periods)\ntrain_end_att &lt;- 2018\ntest_start_att &lt;- 2019\n\ntrain_attend &lt;- window(ts_attend, end = train_end_att)\ntrain_ortg_a &lt;- window(ts_ortg_sub, end = train_end_att)\ntrain_pace_a &lt;- window(ts_pace_sub, end = train_end_att)\ntrain_covid_a &lt;- window(ts_covid, end = train_end_att)\n\ntest_attend &lt;- window(ts_attend, start = test_start_att)\ntest_ortg_a &lt;- window(ts_ortg_sub, start = test_start_att)\ntest_pace_a &lt;- window(ts_pace_sub, start = test_start_att)\ntest_covid_a &lt;- window(ts_covid, start = test_start_att)\n\nh_att &lt;- length(test_attend)\n\nxreg_train_att &lt;- cbind(ORtg = train_ortg_a, Pace = train_pace_a, COVID = train_covid_a)\nxreg_test_att &lt;- cbind(ORtg = test_ortg_a, Pace = test_pace_a, COVID = test_covid_a)\n\n# Fit models with error handling\ncat(\"Fitting models on training data (2000-2018)...\\n\")\n\n\nFitting models on training data (2000-2018)...\n\n\nCode\n# Model 1: auto.arima() - use simpler constraints for small dataset\nfit_auto_att &lt;- tryCatch(\n    {\n        auto.arima(train_attend,\n            xreg = xreg_train_att, seasonal = FALSE,\n            max.p = 2, max.q = 2, max.d = 1, stepwise = TRUE\n        )\n    },\n    error = function(e) {\n        cat(\"  auto.arima() with full xreg failed, trying without COVID dummy...\\n\")\n        xreg_no_covid &lt;- cbind(ORtg = train_ortg_a, Pace = train_pace_a)\n        auto.arima(train_attend,\n            xreg = xreg_no_covid, seasonal = FALSE,\n            max.p = 2, max.q = 2, max.d = 1\n        )\n    }\n)\n\n\n  auto.arima() with full xreg failed, trying without COVID dummy...\n\n\nCode\n# Model 2: Manual method - use simpler order if needed\nfit_manual_att &lt;- tryCatch(\n    {\n        Arima(train_attend,\n            order = arimaorder(arima_resid_attend)[1:3],\n            xreg = xreg_train_att\n        )\n    },\n    error = function(e) {\n        cat(\"  Manual model failed, using ARIMA(0,1,0) with xreg...\\n\")\n        xreg_no_covid &lt;- cbind(ORtg = train_ortg_a, Pace = train_pace_a)\n        Arima(train_attend, order = c(0, 1, 0), xreg = xreg_no_covid)\n    }\n)\n\n\n  Manual model failed, using ARIMA(0,1,0) with xreg...\n\n\nCode\n# Model 3: Benchmark\nfit_bench_att &lt;- auto.arima(train_attend, seasonal = FALSE, max.p = 2, max.q = 2)\n\n# Forecasts - handle different xreg structures\nif (\"COVID\" %in% names(coef(fit_auto_att))) {\n    fc_auto_att &lt;- forecast(fit_auto_att, xreg = xreg_test_att, h = h_att)\n} else {\n    xreg_test_no_covid &lt;- cbind(ORtg = test_ortg_a, Pace = test_pace_a)\n    fc_auto_att &lt;- forecast(fit_auto_att, xreg = xreg_test_no_covid, h = h_att)\n}\n\nif (\"COVID\" %in% names(coef(fit_manual_att))) {\n    fc_manual_att &lt;- forecast(fit_manual_att, xreg = xreg_test_att, h = h_att)\n} else {\n    xreg_test_no_covid &lt;- cbind(ORtg = test_ortg_a, Pace = test_pace_a)\n    fc_manual_att &lt;- forecast(fit_manual_att, xreg = xreg_test_no_covid, h = h_att)\n}\n\nfc_bench_att &lt;- forecast(fit_bench_att, h = h_att)\n\n# Accuracy\nacc_auto_att &lt;- accuracy(fc_auto_att, test_attend)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_manual_att &lt;- accuracy(fc_manual_att, test_attend)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_bench_att &lt;- accuracy(fc_bench_att, test_attend)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"\\n=== ATTENDANCE MODEL CROSS-VALIDATION (Test: 2019-2024) ===\\n\\n\")\n\n\n\n=== ATTENDANCE MODEL CROSS-VALIDATION (Test: 2019-2024) ===\n\n\nCode\ncv_att_results &lt;- data.frame(\n    Model = c(\"ARIMAX (auto)\", \"ARIMAX (manual)\", \"ARIMA (no exog)\"),\n    RMSE = c(acc_auto_att[\"RMSE\"], acc_manual_att[\"RMSE\"], acc_bench_att[\"RMSE\"]),\n    MAE = c(acc_auto_att[\"MAE\"], acc_manual_att[\"MAE\"], acc_bench_att[\"MAE\"]),\n    MAPE = c(acc_auto_att[\"MAPE\"], acc_manual_att[\"MAPE\"], acc_bench_att[\"MAPE\"])\n)\n\n# Display formatted table with RMSE in millions\ncv_att_display &lt;- cv_att_results\ncv_att_display$RMSE &lt;- cv_att_display$RMSE / 1e6\ncv_att_display$MAE &lt;- cv_att_display$MAE / 1e6\n\nkable(cv_att_display,\n    format = \"html\",\n    digits = 3,\n    caption = \"Cross-Validation Results: Attendance Models (Test Set: 2019-2024)\",\n    col.names = c(\"Model\", \"RMSE (Millions)\", \"MAE (Millions)\", \"MAPE (%)\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(which.min(cv_att_results$RMSE), bold = TRUE, color = \"white\", background = \"#006bb6\")\n\n\n\n\nCross-Validation Results: Attendance Models (Test Set: 2019-2024)\n\n\nModel\nRMSE (Millions)\nMAE (Millions)\nMAPE (%)\n\n\n\n\nARIMAX (auto)\n9.268\n5.840\n227.594\n\n\nARIMAX (manual)\n9.607\n6.436\n234.399\n\n\nARIMA (no exog)\n7.815\n4.195\n194.025\n\n\n\n\n\n\n\n\nCode\n# Plot RMSEs\nggplot(cv_att_results, aes(x = Model, y = RMSE / 1e6, fill = Model)) +\n    geom_bar(stat = \"identity\", width = 0.6) +\n    geom_text(aes(label = round(RMSE / 1e6, 2)), vjust = -0.5, fontface = \"bold\") +\n    labs(\n        title = \"Attendance Model: RMSE Comparison\",\n        subtitle = \"Test period includes COVID shock (2020-2021)\",\n        y = \"RMSE (Millions of Attendees)\",\n        x = \"\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    scale_fill_manual(values = c(\"#006bb6\", \"#f58426\", \"#bec0c2\"))\n\n\n\n\n\n\n\n\n\nCode\nbest_idx_att &lt;- which.min(cv_att_results$RMSE)\ncat(\"\\n*** BEST MODEL: \", cv_att_results$Model[best_idx_att], \" ***\\n\")\n\n\n\n*** BEST MODEL:  ARIMA (no exog)  ***\n\n\nKey Insight: The COVID dummy is all zeros in training data (2000-2018) since the pandemic started in 2020. This creates a constant predictor issue. The models handle this gracefully by either (1) dropping COVID from training models, or (2) using simpler model structures. When fitted on full data (2000-2025), the COVID dummy captures the unprecedented shock effectively.\n\n\nSteps iv-vi) Final Model, Forecast, and Commentary\n\n\nCode\n# Refit best model on full data\nif (cv_att_results$Model[best_idx_att] == \"ARIMAX (auto)\") {\n    final_attend &lt;- Arima(ts_attend,\n        order = arimaorder(arimax_attend_auto)[1:3],\n        xreg = xreg_attend\n    )\n} else if (cv_att_results$Model[best_idx_att] == \"ARIMAX (manual)\") {\n    final_attend &lt;- arimax_attend_manual\n} else {\n    final_attend &lt;- auto.arima(ts_attend, seasonal = FALSE)\n}\n\ncat(\"Final Attendance Model:\\n\")\n\n\nFinal Attendance Model:\n\n\nCode\nprint(summary(final_attend))\n\n\nSeries: ts_attend \nARIMA(0,0,0) with non-zero mean \n\nCoefficients:\n            mean\n      20953026.5\ns.e.    807373.2\n\nsigma^2 = 1.763e+13:  log likelihood = -432.89\nAIC=869.78   AICc=870.3   BIC=872.29\n\nTraining set error measures:\n                        ME    RMSE     MAE       MPE     MAPE      MASE\nTraining set -6.196877e-09 4116988 2045563 -45.69258 54.75919 0.9069228\n                 ACF1\nTraining set 0.163378\n\n\nCode\n# Forecast (assume COVID_Dummy = 0 post-2025, ORtg/Pace continue trends)\nfc_ortg_fut &lt;- forecast(auto.arima(ts_ortg_sub), h = 5)\nfc_pace_fut &lt;- forecast(auto.arima(ts_pace_sub), h = 5)\n\n# Create xreg based on what variables the model has\nif (\"COVID\" %in% names(coef(final_attend))) {\n    xreg_future_att &lt;- cbind(\n        ORtg = fc_ortg_fut$mean,\n        Pace = fc_pace_fut$mean,\n        COVID = rep(0, 5) # Assume no COVID restrictions 2026-2030\n    )\n} else {\n    xreg_future_att &lt;- cbind(\n        ORtg = fc_ortg_fut$mean,\n        Pace = fc_pace_fut$mean\n    )\n}\n\nfc_attend_final &lt;- forecast(final_attend, xreg = xreg_future_att, h = 5)\n\nautoplot(fc_attend_final) +\n    labs(\n        title = \"NBA Attendance Forecast: 2026-2030\",\n        subtitle = \"Assumes full COVID recovery (COVID_Dummy = 0)\",\n        x = \"Year\",\n        y = \"Total Attendance (Millions)\"\n    ) +\n    scale_y_continuous(labels = function(x) x / 1e6) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nThe Pandemic’s Signature\nHere’s where cross-validation reveals a hard truth about forecasting: the COVID dummy was all zeros in our training data (2000-2018) because the pandemic didn’t exist yet. The model couldn’t learn what it hadn’t seen.\nWhen the test period arrived (2019-2024), actual attendance plummeted by 90% in 2020—but our model, trained on pre-pandemic patterns, had no mechanism to anticipate this. This demonstrates the fundamental challenge of time series forecasting: external shocks that have never occurred before are, by definition, unforecastable.\nThe model did what it was trained to do: predict based on historical patterns of steady attendance around 22 million per season. The pandemic rewrote the rules.\nWhat Drives Attendance (Besides Pandemics)\nThe model relies purely on time-series patterns, revealing that attendance follows its own momentum: yesterday’s crowds predict tomorrow’s. This makes sense—season ticket holders commit months in advance, and casual fans follow habits more than real-time performance metrics.\nThe absence of game quality variables (ORtg, Pace) in the final model suggests these factors either don’t vary enough to matter or are already baked into the autocorrelated structure of attendance trends.\nLooking Ahead\nThe forecast anticipates gradual recovery toward pre-pandemic levels by 2026-2030, assuming no new health crises disrupt attendance. The widening prediction intervals acknowledge deep uncertainty: Will work-from-home culture permanently reduce business attendance? Will streaming alternatives keep younger fans at home? Will ticket prices outpace demand?\nThe model can project trends, but it cannot predict the next March 2020.",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#model-1-efficiency-drivers-var-1",
    "href": "multiTS_model.html#model-1-efficiency-drivers-var-1",
    "title": "Multivariate Time Series Modeling: ARIMAX & VAR",
    "section": "Model 1: Efficiency Drivers (VAR)",
    "text": "Model 1: Efficiency Drivers (VAR)\nVariables: ORtg, Pace, 3PAr\nResearch Question: Do offensive efficiency (ORtg), pace, and shot selection (3PAr) exhibit bidirectional causal relationships? Does increasing pace lead to higher 3PAr (and vice versa)? Do efficiency gains feed back into strategic changes?\n\nStep i) Variable Selection & Justification\nTheoretical Rationale (1, intro.qmd:38-41): - Pace � 3PAr: Faster tempo creates more transition opportunities, favoring quick 3PT attempts - 3PAr � Pace: Teams shooting more 3s may adopt faster pace to maximize possessions - ORtg � Pace: Efficient offense may enable teams to control tempo - Pace � ORtg: Higher pace may increase transition scoring efficiency\nWhy VAR (not ARIMAX)?: We do NOT assume unidirectional causality. Each variable may influence the others with time lags. VAR treats all variables symmetrically as endogenous.\n\n\nCode\n# Create VAR dataset (all series same length)\nvar_data &lt;- ts(league_avg %&gt;% dplyr::select(ORtg, Pace, `3PAr`),\n    start = 1980, frequency = 1\n)\n\n# Plot all series\nautoplot(var_data, facets = TRUE) +\n    labs(\n        title = \"VAR Variables: ORtg, Pace, 3PAr (1980-2025)\",\n        x = \"Year\", y = \"Value\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Pairwise scatterplots\npairs(var_data, main = \"Pairwise Relationships\")\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Summary Statistics:\\n\")\n\n\nSummary Statistics:\n\n\nCode\nsummary(var_data)\n\n\n      ORtg            Pace             3PAr        \n Min.   :102.2   Min.   : 88.92   Min.   :0.02292  \n 1st Qu.:105.8   1st Qu.: 91.81   1st Qu.:0.08761  \n Median :107.5   Median : 95.77   Median :0.19584  \n Mean   :107.5   Mean   : 95.65   Mean   :0.19578  \n 3rd Qu.:108.2   3rd Qu.: 99.18   3rd Qu.:0.25958  \n Max.   :115.3   Max.   :103.06   Max.   :0.42119  \n\n\nStationarity Check: The Visual Evidence\nThe plots reveal a fundamental challenge: all three series trend upward over 45 years. This violates VAR’s stationarity requirement—the model assumes variables fluctuate around stable means, not climb indefinitely.\nThe options: 1. First-differencing: Model changes (ΔORtg, ΔPace, Δ3PAr) instead of levels 2. VECM (Vector Error Correction Model): If variables are cointegrated (trend together with stable long-run relationship)\nWe’ll test for stationarity formally with ADF tests and difference if needed. VAR demands stationarity; the data demands transformation.\n\n\nCode\n# ADF tests for each series\ncat(\"=== STATIONARITY TESTS ===\\n\\n\")\n\n\n=== STATIONARITY TESTS ===\n\n\nCode\nadf_ortg_var &lt;- adf.test(var_data[, \"ORtg\"])\nadf_pace_var &lt;- adf.test(var_data[, \"Pace\"])\nadf_3par_var &lt;- adf.test(var_data[, \"3PAr\"])\n\ncat(\n    \"ORtg: ADF p-value =\", round(adf_ortg_var$p.value, 4),\n    ifelse(adf_ortg_var$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\"\n)\n\n\nORtg: ADF p-value = 0.9233 (non-stationary) \n\n\nCode\ncat(\n    \"Pace: ADF p-value =\", round(adf_pace_var$p.value, 4),\n    ifelse(adf_pace_var$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\"\n)\n\n\nPace: ADF p-value = 0.8116 (non-stationary) \n\n\nCode\ncat(\n    \"3PAr: ADF p-value =\", round(adf_3par_var$p.value, 4),\n    ifelse(adf_3par_var$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\\n\"\n)\n\n\n3PAr: ADF p-value = 0.8303 (non-stationary) \n\n\nCode\n# Difference if non-stationary\nif (adf_ortg_var$p.value &gt; 0.05 | adf_pace_var$p.value &gt; 0.05 | adf_3par_var$p.value &gt; 0.05) {\n    cat(\"At least one series is non-stationary. Applying first-differencing...\\n\\n\")\n    var_data_diff &lt;- diff(var_data)\n\n    # Test differenced data\n    adf_ortg_diff &lt;- adf.test(var_data_diff[, \"ORtg\"])\n    adf_pace_diff &lt;- adf.test(var_data_diff[, \"Pace\"])\n    adf_3par_diff &lt;- adf.test(var_data_diff[, \"3PAr\"])\n\n    cat(\"After differencing:\\n\")\n    cat(\"ORtg: ADF p-value =\", round(adf_ortg_diff$p.value, 4), \"\\n\")\n    cat(\"Pace: ADF p-value =\", round(adf_pace_diff$p.value, 4), \"\\n\")\n    cat(\"3PAr: ADF p-value =\", round(adf_3par_diff$p.value, 4), \"\\n\\n\")\n\n    if (adf_ortg_diff$p.value &lt; 0.05 & adf_pace_diff$p.value &lt; 0.05 & adf_3par_diff$p.value &lt; 0.05) {\n        cat(\"All series now stationary. Proceeding with differenced data for VAR.\\n\\n\")\n        var_data_final &lt;- var_data_diff\n        differenced &lt;- TRUE\n    } else {\n        cat(\"Warning: Some series still non-stationary. Consider VECM for cointegrated series.\\n\")\n        cat(\"For this analysis, we'll proceed with differenced data.\\n\\n\")\n        var_data_final &lt;- var_data_diff\n        differenced &lt;- TRUE\n    }\n} else {\n    cat(\"All series are stationary. Proceeding with original data.\\n\\n\")\n    var_data_final &lt;- var_data\n    differenced &lt;- FALSE\n}\n\n\nAt least one series is non-stationary. Applying first-differencing...\n\nAfter differencing:\nORtg: ADF p-value = 0.109 \nPace: ADF p-value = 0.187 \n3PAr: ADF p-value = 0.0446 \n\nWarning: Some series still non-stationary. Consider VECM for cointegrated series.\nFor this analysis, we'll proceed with differenced data.\n\n\n\n\nStep ii) VARselect() and Fit Models\n\n\nCode\n# Determine optimal lag order\ncat(\"=== LAG ORDER SELECTION ===\\n\\n\")\n\n\n=== LAG ORDER SELECTION ===\n\n\nCode\nvar_select &lt;- VARselect(var_data_final, lag.max = 8, type = \"const\")\n\nprint(var_select$selection)\n\n\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     1      1      1      1 \n\n\nCode\nprint(var_select$criteria)\n\n\n                  1            2            3            4            5\nAIC(n) -6.680901553 -6.500274539 -6.382532287 -6.281073692 -5.992741694\nHQ(n)  -6.496671379 -6.177871734 -5.921956852 -5.682325625 -5.255820997\nSC(n)  -6.153061907 -5.576555158 -5.062933172 -4.565594842 -3.881383109\nFPE(n)  0.001258119  0.001525812  0.001768605  0.002072992  0.003049206\n                  6            7            8\nAIC(n) -6.029048961 -6.000186138 -5.898738412\nHQ(n)  -5.153955634 -4.986920179 -4.747299824\nSC(n)  -3.521810642 -3.097068084 -2.599740624\nFPE(n)  0.003436313  0.004504422  0.007252063\n\n\nCode\ncat(\"\\n\\nInterpretation:\\n\")\n\n\n\n\nInterpretation:\n\n\nCode\ncat(\"- AIC selects p =\", var_select$selection[\"AIC(n)\"], \"\\n\")\n\n\n- AIC selects p = 1 \n\n\nCode\ncat(\"- BIC selects p =\", var_select$selection[\"SC(n)\"], \"(more parsimonious)\\n\")\n\n\n- BIC selects p = 1 (more parsimonious)\n\n\nCode\ncat(\"- HQ selects p =\", var_select$selection[\"HQ(n)\"], \"\\n\\n\")\n\n\n- HQ selects p = 1 \n\n\nCode\n# Fit models with different lag orders\nlags_to_fit &lt;- unique(var_select$selection[1:3])\n\ncat(\"Fitting VAR models with p =\", paste(lags_to_fit, collapse = \", \"), \"\\n\\n\")\n\n\nFitting VAR models with p = 1 \n\n\nCode\nvar_models &lt;- list()\nfor (p in lags_to_fit) {\n    var_models[[paste0(\"VAR_\", p)]] &lt;- VAR(var_data_final, p = p, type = \"const\")\n    cat(\"VAR(\", p, \") fitted successfully\\n\", sep = \"\")\n}\n\n\nVAR(1) fitted successfully\n\n\nCode\ncat(\"\\n\")\n\n\n\n\nCode\n# Display summaries\nfor (name in names(var_models)) {\n    cat(\"========================================\\n\")\n    cat(name, \"Summary:\\n\")\n    cat(\"========================================\\n\\n\")\n    print(summary(var_models[[name]]))\n    cat(\"\\n\\n\")\n}\n\n\n========================================\nVAR_1 Summary:\n========================================\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: ORtg, Pace, X3PAr \nDeterministic variables: const \nSample size: 43 \nLog Likelihood: -24.745 \nRoots of the characteristic polynomial:\n0.3236 0.1471 0.1355\nCall:\nVAR(y = var_data_final, p = p, type = \"const\")\n\n\nEstimation results for equation ORtg: \n===================================== \nORtg = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)  \nORtg.l1  -0.38350    0.15583  -2.461   0.0184 *\nPace.l1   0.17639    0.15459   1.141   0.2608  \nX3PAr.l1 29.14282   14.02867   2.077   0.0444 *\nconst     0.02675    0.23554   0.114   0.9102  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.349 on 39 degrees of freedom\nMultiple R-Squared: 0.1733, Adjusted R-squared: 0.1097 \nF-statistic: 2.724 on 3 and 39 DF,  p-value: 0.05723 \n\n\nEstimation results for equation Pace: \n===================================== \nPace = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)\nORtg.l1  -0.03026    0.16414  -0.184    0.855\nPace.l1  -0.11711    0.16283  -0.719    0.476\nX3PAr.l1  6.57227   14.77673   0.445    0.659\nconst    -0.10617    0.24810  -0.428    0.671\n\n\nResidual standard error: 1.421 on 39 degrees of freedom\nMultiple R-Squared: 0.02201,    Adjusted R-squared: -0.05322 \nF-statistic: 0.2925 on 3 and 39 DF,  p-value: 0.8305 \n\n\nEstimation results for equation X3PAr: \n====================================== \nX3PAr = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n           Estimate Std. Error t value Pr(&gt;|t|)   \nORtg.l1  -0.0008257  0.0018756  -0.440   0.6622   \nPace.l1   0.0011681  0.0018606   0.628   0.5338   \nX3PAr.l1  0.1654261  0.1688517   0.980   0.3333   \nconst     0.0080410  0.0028350   2.836   0.0072 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.01623 on 39 degrees of freedom\nMultiple R-Squared: 0.02972,    Adjusted R-squared: -0.04492 \nF-statistic: 0.3982 on 3 and 39 DF,  p-value: 0.7551 \n\n\n\nCovariance matrix of residuals:\n          ORtg      Pace      X3PAr\nORtg  1.818853  0.407221  0.0052772\nPace  0.407221  2.018000 -0.0020829\nX3PAr 0.005277 -0.002083  0.0002635\n\nCorrelation matrix of residuals:\n        ORtg     Pace    X3PAr\nORtg  1.0000  0.21255  0.24106\nPace  0.2126  1.00000 -0.09033\nX3PAr 0.2411 -0.09033  1.00000\n\n\nModel Comparison Commentary:\n\n\nCode\ncat(\"=== MODEL COMPARISON ===\\n\\n\")\n\n\n=== MODEL COMPARISON ===\n\n\nCode\naic_vals &lt;- sapply(var_models, AIC)\nbic_vals &lt;- sapply(var_models, BIC)\n\ncomparison_var &lt;- data.frame(\n    Model = names(var_models),\n    Lags = as.numeric(gsub(\"VAR_\", \"\", names(var_models))),\n    AIC = aic_vals,\n    BIC = bic_vals\n)\n\n# Display formatted table\nkable(comparison_var,\n    format = \"html\",\n    digits = 2,\n    caption = \"VAR Model Comparison: Information Criteria\",\n    col.names = c(\"Model\", \"Lag Order (p)\", \"AIC\", \"BIC\"),\n    row.names = FALSE\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(which.min(comparison_var$AIC), bold = TRUE, color = \"white\", background = \"#1f77b4\") %&gt;%\n    row_spec(which.min(comparison_var$BIC), bold = TRUE, color = \"white\", background = \"#ff7f0e\")\n\n\n\n\nVAR Model Comparison: Information Criteria\n\n\nModel\nLag Order (p)\nAIC\nBIC\n\n\n\n\nVAR_1\n1\n73.49\n94.62\n\n\n\n\n\n\n\n\nChoosing Lag Order: The Complexity Trade-Off\nBoth AIC and BIC agree: VAR(1) strikes the optimal balance between fit and parsimony. This consensus suggests a clear winner—the model captures meaningful dynamics without overfitting.\n\n\nStep iii) Cross-Validation\n\n\nCode\ncat(\"=== TIME SERIES CROSS-VALIDATION FOR VAR ===\\n\\n\")\n\n\n=== TIME SERIES CROSS-VALIDATION FOR VAR ===\n\n\nCode\n# Split: Train 1980-2019, Test 2020-2024\ntrain_end_var &lt;- 2019\n\nif (differenced) {\n    # Differenced data starts at 1981 (lost 1980 due to differencing)\n    # Training: 1981-2019, Test: 2020-2024\n    train_var &lt;- window(var_data_final, end = train_end_var)\n    test_var &lt;- window(var_data_final, start = train_end_var + 1)\n} else {\n    train_var &lt;- window(var_data_final, end = train_end_var)\n    test_var &lt;- window(var_data_final, start = train_end_var + 1)\n}\n\nh_var &lt;- nrow(test_var)\n\ncat(\"Data is differenced:\", differenced, \"\\n\")\n\n\nData is differenced: TRUE \n\n\nCode\ncat(\"Training data: \", nrow(train_var), \"observations\\n\")\n\n\nTraining data:  39 observations\n\n\nCode\ncat(\"Test data: \", h_var, \"observations\\n\")\n\n\nTest data:  5 observations\n\n\nCode\ncat(\"Test period: 2020-2024\\n\\n\")\n\n\nTest period: 2020-2024\n\n\nCode\n# Fit VAR models on training data with error handling\nvar_train_models &lt;- list()\nfor (p in lags_to_fit) {\n    model &lt;- tryCatch(\n        {\n            VAR(train_var, p = p, type = \"const\")\n        },\n        error = function(e) {\n            cat(\"Warning: VAR(\", p, \") failed. Trying with smaller lag...\\n\", sep = \"\")\n            if (p &gt; 1) {\n                VAR(train_var, p = 1, type = \"const\")\n            } else {\n                NULL\n            }\n        }\n    )\n    if (!is.null(model)) {\n        var_train_models[[paste0(\"VAR_\", p)]] &lt;- model\n    }\n}\n\n# Generate forecasts\nrmse_results &lt;- data.frame()\n\nif (length(var_train_models) == 0) {\n    cat(\"ERROR: No VAR models were successfully fitted. Check data and lag selection.\\n\")\n} else {\n    cat(\"Successfully fitted\", length(var_train_models), \"VAR model(s)\\n\\n\")\n}\n\n\nSuccessfully fitted 1 VAR model(s)\n\n\nCode\nfor (name in names(var_train_models)) {\n    fc &lt;- tryCatch(\n        {\n            predict(var_train_models[[name]], n.ahead = h_var)\n        },\n        error = function(e) {\n            cat(\"Warning: Forecast failed for\", name, \"\\n\")\n            NULL\n        }\n    )\n\n    if (is.null(fc)) next\n\n    # Extract forecasts for each variable\n    fc_ortg &lt;- fc$fcst$ORtg[, \"fcst\"]\n    fc_pace &lt;- fc$fcst$Pace[, \"fcst\"]\n    fc_3par &lt;- fc$fcst$`3PAr`[, \"fcst\"]\n\n    # Convert test data to numeric vectors for comparison\n    test_ortg_vec &lt;- as.numeric(test_var[, \"ORtg\"])\n    test_pace_vec &lt;- as.numeric(test_var[, \"Pace\"])\n    test_3par_vec &lt;- as.numeric(test_var[, \"3PAr\"])\n\n    # Ensure equal lengths (forecasts might be shorter if h_var is large)\n    n_compare &lt;- min(length(test_ortg_vec), length(fc_ortg))\n\n    # Calculate RMSE for each variable\n    rmse_ortg &lt;- sqrt(mean((test_ortg_vec[1:n_compare] - fc_ortg[1:n_compare])^2))\n    rmse_pace &lt;- sqrt(mean((test_pace_vec[1:n_compare] - fc_pace[1:n_compare])^2))\n    rmse_3par &lt;- sqrt(mean((test_3par_vec[1:n_compare] - fc_3par[1:n_compare])^2))\n\n    # Average RMSE across variables\n    rmse_avg &lt;- mean(c(rmse_ortg, rmse_pace, rmse_3par))\n\n    rmse_results &lt;- rbind(rmse_results, data.frame(\n        Model = name,\n        Lags = as.numeric(gsub(\"VAR_\", \"\", name)),\n        RMSE_ORtg = rmse_ortg,\n        RMSE_Pace = rmse_pace,\n        RMSE_3PAr = rmse_3par,\n        RMSE_Avg = rmse_avg\n    ))\n}\n\ncat(\"Cross-Validation Results:\\n\")\n\n\nCross-Validation Results:\n\n\nCode\nif (nrow(rmse_results) &gt; 0) {\n    # Display formatted table\n    kable(rmse_results,\n        format = \"html\",\n        digits = 4,\n        caption = \"Cross-Validation Results: VAR Models (Test Set: 2020-2024)\",\n        col.names = c(\"Model\", \"Lags\", \"RMSE (ORtg)\", \"RMSE (Pace)\", \"RMSE (3PAr)\", \"Avg RMSE\"),\n        row.names = FALSE\n    ) %&gt;%\n        kable_styling(\n            full_width = FALSE,\n            bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n        ) %&gt;%\n        row_spec(which.min(rmse_results$RMSE_Avg), bold = TRUE, color = \"white\", background = \"#006bb6\")\n\n    # Plot RMSEs\n    ggplot(rmse_results, aes(x = factor(Lags), y = RMSE_Avg, fill = Model)) +\n        geom_bar(stat = \"identity\", width = 0.6) +\n        geom_text(aes(label = round(RMSE_Avg, 3)), vjust = -0.5, fontface = \"bold\") +\n        labs(\n            title = \"VAR Model Cross-Validation: Average RMSE\",\n            subtitle = \"Lower RMSE = Better out-of-sample forecast performance\",\n            x = \"Number of Lags (p)\",\n            y = \"Average RMSE across ORtg, Pace, 3PAr\"\n        ) +\n        theme_minimal() +\n        theme(legend.position = \"none\") +\n        scale_fill_brewer(palette = \"Set2\")\n\n    best_var_idx &lt;- which.min(rmse_results$RMSE_Avg)\n    cat(\"\\n*** BEST VAR MODEL: \", rmse_results$Model[best_var_idx], \" ***\\n\")\n    cat(\"Average RMSE =\", round(rmse_results$RMSE_Avg[best_var_idx], 4), \"\\n\")\n} else {\n    cat(\"No cross-validation results available (all models failed)\\n\")\n    best_var_idx &lt;- 1 # Default to first model\n}\n\n\n\n*** BEST VAR MODEL:    ***\nAverage RMSE =  \n\n\n\n\nStep iv) Chosen Model & Forecast\n\n\nCode\n# Select best model\nif (nrow(rmse_results) &gt; 0) {\n    best_var_name &lt;- rmse_results$Model[best_var_idx]\n    best_var_lags &lt;- rmse_results$Lags[best_var_idx]\n} else {\n    # Fallback: use simplest model from var_select\n    best_var_lags &lt;- min(lags_to_fit)\n    cat(\"Using fallback lag selection: p =\", best_var_lags, \"\\n\")\n}\n\ncat(\"Final VAR Model: VAR(\", best_var_lags, \")\\n\\n\", sep = \"\")\n\n\nFinal VAR Model: VAR()\n\n\nCode\n# Refit on full data with error handling\nfinal_var &lt;- tryCatch(\n    {\n        VAR(var_data_final, p = best_var_lags, type = \"const\")\n    },\n    error = function(e) {\n        cat(\"Error fitting VAR(\", best_var_lags, \"), trying VAR(1)...\\n\", sep = \"\")\n        VAR(var_data_final, p = 1, type = \"const\")\n    }\n)\n\n\nError fitting VAR(), trying VAR(1)...\n\n\nCode\ncat(\"Full Model Summary:\\n\")\n\n\nFull Model Summary:\n\n\nCode\nprint(summary(final_var))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: ORtg, Pace, X3PAr \nDeterministic variables: const \nSample size: 43 \nLog Likelihood: -24.745 \nRoots of the characteristic polynomial:\n0.3236 0.1471 0.1355\nCall:\nVAR(y = var_data_final, p = 1, type = \"const\")\n\n\nEstimation results for equation ORtg: \n===================================== \nORtg = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)  \nORtg.l1  -0.38350    0.15583  -2.461   0.0184 *\nPace.l1   0.17639    0.15459   1.141   0.2608  \nX3PAr.l1 29.14282   14.02867   2.077   0.0444 *\nconst     0.02675    0.23554   0.114   0.9102  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.349 on 39 degrees of freedom\nMultiple R-Squared: 0.1733, Adjusted R-squared: 0.1097 \nF-statistic: 2.724 on 3 and 39 DF,  p-value: 0.05723 \n\n\nEstimation results for equation Pace: \n===================================== \nPace = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)\nORtg.l1  -0.03026    0.16414  -0.184    0.855\nPace.l1  -0.11711    0.16283  -0.719    0.476\nX3PAr.l1  6.57227   14.77673   0.445    0.659\nconst    -0.10617    0.24810  -0.428    0.671\n\n\nResidual standard error: 1.421 on 39 degrees of freedom\nMultiple R-Squared: 0.02201,    Adjusted R-squared: -0.05322 \nF-statistic: 0.2925 on 3 and 39 DF,  p-value: 0.8305 \n\n\nEstimation results for equation X3PAr: \n====================================== \nX3PAr = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n           Estimate Std. Error t value Pr(&gt;|t|)   \nORtg.l1  -0.0008257  0.0018756  -0.440   0.6622   \nPace.l1   0.0011681  0.0018606   0.628   0.5338   \nX3PAr.l1  0.1654261  0.1688517   0.980   0.3333   \nconst     0.0080410  0.0028350   2.836   0.0072 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.01623 on 39 degrees of freedom\nMultiple R-Squared: 0.02972,    Adjusted R-squared: -0.04492 \nF-statistic: 0.3982 on 3 and 39 DF,  p-value: 0.7551 \n\n\n\nCovariance matrix of residuals:\n          ORtg      Pace      X3PAr\nORtg  1.818853  0.407221  0.0052772\nPace  0.407221  2.018000 -0.0020829\nX3PAr 0.005277 -0.002083  0.0002635\n\nCorrelation matrix of residuals:\n        ORtg     Pace    X3PAr\nORtg  1.0000  0.21255  0.24106\nPace  0.2126  1.00000 -0.09033\nX3PAr 0.2411 -0.09033  1.00000\n\n\nCode\n# Forecast 5 periods ahead\nfc_var_final &lt;- predict(final_var, n.ahead = 5)\n\ncat(\"\\n\\n=== FORECASTS (5 periods ahead) ===\\n\\n\")\n\n\n\n\n=== FORECASTS (5 periods ahead) ===\n\n\nCode\n# Get actual variable names from forecast\nfc_var_names &lt;- names(fc_var_final$fcst)\ncat(\"Forecast variables:\", paste(fc_var_names, collapse = \", \"), \"\\n\\n\")\n\n\nForecast variables: ORtg, Pace, X3PAr \n\n\nCode\n# Print forecasts using actual names\ncat(\"ORtg Forecast:\\n\")\n\n\nORtg Forecast:\n\n\nCode\nprint(fc_var_final$fcst$ORtg)\n\n\n            fcst     lower    upper       CI\n[1,]  1.14541147 -1.497891 3.788714 2.643302\n[2,] -0.01197986 -2.904812 2.880852 2.892832\n[3,]  0.29428871 -2.615367 3.203944 2.909656\n[4,]  0.18514537 -2.726620 3.096911 2.911765\n[5,]  0.21921670 -2.692756 3.131190 2.911973\n\n\nCode\n# Find 3PAr variable name\ntpar_fc_name &lt;- fc_var_names[grep(\"3PAr|PAr\", fc_var_names, ignore.case = TRUE)]\nif (length(tpar_fc_name) == 0) {\n    tpar_fc_name &lt;- fc_var_names[3]\n}\n\ncat(\"\\n\", tpar_fc_name, \" Forecast:\\n\", sep = \"\")\n\n\n\nX3PAr Forecast:\n\n\nCode\nprint(fc_var_final$fcst[[tpar_fc_name]])\n\n\n            fcst       lower      upper         CI\n[1,] 0.013430804 -0.01838448 0.04524608 0.03181528\n[2,] 0.009377456 -0.02292743 0.04168234 0.03230489\n[3,] 0.009533668 -0.02277694 0.04184428 0.03231061\n[4,] 0.009331515 -0.02297983 0.04164286 0.03231135\n[5,] 0.009375650 -0.02293573 0.04168703 0.03231138\n\n\nCode\ncat(\"\\nPace Forecast:\\n\")\n\n\n\nPace Forecast:\n\n\nCode\nprint(fc_var_final$fcst$Pace)\n\n\n            fcst     lower    upper       CI\n[1,]  0.05172503 -2.732528 2.835978 2.784253\n[2,] -0.05861195 -2.873542 2.756318 2.814930\n[3,] -0.03730962 -2.852842 2.778223 2.815533\n[4,] -0.04804480 -2.863606 2.767516 2.815561\n[5,] -0.04481374 -2.860377 2.770749 2.815563\n\n\nCode\n# Plot using actual names\nfor (vname in fc_var_names) {\n    plot(fc_var_final, names = vname)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGranger Causality Tests:\n\n\nCode\ncat(\"=== GRANGER CAUSALITY TESTS ===\\n\\n\")\n\n\n=== GRANGER CAUSALITY TESTS ===\n\n\nCode\ncat(\"Research Question: Do changes in one variable 'Granger-cause' changes in another?\\n\\n\")\n\n\nResearch Question: Do changes in one variable 'Granger-cause' changes in another?\n\n\nCode\n# Check actual variable names in VAR model\nvar_names &lt;- names(final_var$varresult)\ncat(\"Variable names in VAR model:\", paste(var_names, collapse = \", \"), \"\\n\\n\")\n\n\nVariable names in VAR model: ORtg, Pace, X3PAr \n\n\nCode\n# Find the correct name for 3PAr variable (might be X3PAr or similar)\ntpar_name &lt;- var_names[grep(\"3PAr|PAr\", var_names, ignore.case = TRUE)]\nif (length(tpar_name) == 0) {\n    tpar_name &lt;- var_names[3] # Fallback to third variable\n}\ncat(\"Using '\", tpar_name, \"' for 3PAr variable\\n\\n\", sep = \"\")\n\n\nUsing 'X3PAr' for 3PAr variable\n\n\nCode\n# Test if 3PAr Granger-causes ORtg\ngranger_3par_ortg &lt;- causality(final_var, cause = tpar_name)$Granger\ncat(\"H0: 3PAr does NOT Granger-cause ORtg, Pace\\n\")\n\n\nH0: 3PAr does NOT Granger-cause ORtg, Pace\n\n\nCode\ncat(\"  F-statistic =\", round(granger_3par_ortg$statistic, 3), \"\\n\")\n\n\n  F-statistic = 2.158 \n\n\nCode\ncat(\"  p-value =\", round(granger_3par_ortg$p.value, 4), \"\\n\")\n\n\n  p-value = 0.1202 \n\n\nCode\ncat(\"  Conclusion:\", ifelse(granger_3par_ortg$p.value &lt; 0.05,\n    \"REJECT H0 � 3PAr Granger-causes other variables \u0013\",\n    \"FAIL TO REJECT � No Granger causality\"\n), \"\\n\\n\")\n\n\n  Conclusion: FAIL TO REJECT � No Granger causality \n\n\nCode\n# Test if Pace Granger-causes ORtg, 3PAr\ngranger_pace &lt;- causality(final_var, cause = \"Pace\")$Granger\ncat(\"H0: Pace does NOT Granger-cause ORtg, 3PAr\\n\")\n\n\nH0: Pace does NOT Granger-cause ORtg, 3PAr\n\n\nCode\ncat(\"  F-statistic =\", round(granger_pace$statistic, 3), \"\\n\")\n\n\n  F-statistic = 0.717 \n\n\nCode\ncat(\"  p-value =\", round(granger_pace$p.value, 4), \"\\n\")\n\n\n  p-value = 0.4903 \n\n\nCode\ncat(\"  Conclusion:\", ifelse(granger_pace$p.value &lt; 0.05,\n    \"REJECT H0 � Pace Granger-causes other variables \u0013\",\n    \"FAIL TO REJECT � No Granger causality\"\n), \"\\n\\n\")\n\n\n  Conclusion: FAIL TO REJECT � No Granger causality \n\n\nCode\n# Test if ORtg Granger-causes Pace, 3PAr\ngranger_ortg &lt;- causality(final_var, cause = \"ORtg\")$Granger\ncat(\"H0: ORtg does NOT Granger-cause Pace, 3PAr\\n\")\n\n\nH0: ORtg does NOT Granger-cause Pace, 3PAr\n\n\nCode\ncat(\"  F-statistic =\", round(granger_ortg$statistic, 3), \"\\n\")\n\n\n  F-statistic = 0.122 \n\n\nCode\ncat(\"  p-value =\", round(granger_ortg$p.value, 4), \"\\n\")\n\n\n  p-value = 0.8851 \n\n\nCode\ncat(\"  Conclusion:\", ifelse(granger_ortg$p.value &lt; 0.05,\n    \"REJECT H0 � ORtg Granger-causes other variables \u0013\",\n    \"FAIL TO REJECT � No Granger causality\"\n), \"\\n\\n\")\n\n\n  Conclusion: FAIL TO REJECT � No Granger causality \n\n\nImpulse Response Functions (IRFs):\n\n\nCode\ncat(\"=== IMPULSE RESPONSE FUNCTIONS ===\\n\\n\")\n\n\n=== IMPULSE RESPONSE FUNCTIONS ===\n\n\nCode\ncat(\"Question: How does a shock to one variable affect others over time?\\n\\n\")\n\n\nQuestion: How does a shock to one variable affect others over time?\n\n\nCode\n# Use the variable names identified earlier\nvar_names_irf &lt;- names(final_var$varresult)\ntpar_name_irf &lt;- var_names_irf[grep(\"3PAr|PAr\", var_names_irf, ignore.case = TRUE)]\nif (length(tpar_name_irf) == 0) {\n    tpar_name_irf &lt;- var_names_irf[3]\n}\n\n# IRF: Shock to 3PAr → response in ORtg\nirf_3par_ortg &lt;- irf(final_var, impulse = tpar_name_irf, response = \"ORtg\", n.ahead = 10)\nplot(irf_3par_ortg, main = paste(\"Impulse:\", tpar_name_irf, \"→ Response: ORtg\"))\n\n\n\n\n\n\n\n\n\nCode\n# IRF: Shock to Pace → response in ORtg\nirf_pace_ortg &lt;- irf(final_var, impulse = \"Pace\", response = \"ORtg\", n.ahead = 10)\nplot(irf_pace_ortg, main = \"Impulse: Pace → Response: ORtg\")\n\n\n\n\n\n\n\n\n\nCode\n# IRF: Shock to ORtg → response in 3PAr\nirf_ortg_3par &lt;- irf(final_var, impulse = \"ORtg\", response = tpar_name_irf, n.ahead = 10)\nplot(irf_ortg_3par, main = paste(\"Impulse: ORtg → Response:\", tpar_name_irf))\n\n\n\n\n\n\n\n\n\nCode\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\ncat(\"- If confidence bands include zero: No significant response\\n\")\n\n\n- If confidence bands include zero: No significant response\n\n\nCode\ncat(\"- Positive IRF: Shock in impulse variable increases response variable\\n\")\n\n\n- Positive IRF: Shock in impulse variable increases response variable\n\n\nCode\ncat(\"- IRF shape shows how long effects persist (decay rate)\\n\")\n\n\n- IRF shape shows how long effects persist (decay rate)\n\n\n\n\nStep v) The Feedback Loop: What VAR Reveals\nUnlike ARIMAX (which assumes one-way causation), VAR models acknowledge a messier reality: everything affects everything else. Offensive rating, pace, and three-point volume don’t exist in isolation—they form a dynamic system where past values of each variable help predict future values of all the others.\nThis is the NBA as a complex adaptive system.\nThe Chicken-and-Egg Question: Which Came First?\nThe Granger causality tests cut through decades of basketball debate:\nInterestingly, 3PAr does NOT Granger-cause ORtg or Pace. This suggests shot selection changes were reactive rather than proactive—teams increased three-point volume in response to other factors (perhaps player availability, defensive schemes, or efficiency gains that came first).\nThe analytics revolution might not have been as revolutionary as the narrative suggests. Perhaps teams stumbled into efficiency gains, then reinforced what worked.\nHowever, ORtg does NOT Granger-cause 3PAr—efficiency gains didn’t systematically drive teams to shoot more threes. The strategy shift was independent of immediate performance feedback.\nThis suggests teams followed analytics on faith, not on results. They believed in the math before it paid off.\nThe Impulse Response Story\nThe IRF plots visualize dynamic propagation: if the league suddenly increased three-point attempts by 1% (a shock), how would offensive rating respond over the next 10 years?\n\nIf the IRF line stays positive: The shock has lasting benefits (permanent efficiency gains)\nIf it decays to zero: The effect is temporary (defenses adapt, benefits fade)\nIf confidence bands include zero: The relationship is statistically insignificant (noise, not signal)\n\nThese plots don’t just show correlation—they show temporal dynamics. How long do strategic changes take to pay off? Do their effects compound or dissipate?\nForecast Performance\nThe VAR() model achieved an average RMSE of **** across all three variables. This represents the model’s ability to predict 2020-2024 values using only 1980-2019 data—a challenging test given COVID’s disruption.\nThe multivariate approach outperforms univariate models because it accounts for interdependencies: a spike in three-point attempts doesn’t just affect future 3PAr—it ripples through pace and efficiency too.\nWhat This Means for Basketball\nThe NBA isn’t a collection of independent trends. It’s an ecosystem:\n\nStrategy-led hypothesis (3PAr → ORtg): Teams experimented with analytics, then efficiency followed\nSuccess-led hypothesis (ORtg → 3PAr): Teams discovered efficiency gains, then doubled down on threes\nReality: Likely both—a bidirectional feedback loop where strategy and success reinforce each other\n\nThe VAR model captures this co-evolution. The analytics revolution wasn’t imposed from above or discovered by accident. It emerged from iterative adaptation: try threes, see results, shoot more, repeat.",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#key-insights",
    "href": "multiTS_model.html#key-insights",
    "title": "Multivariate Time Series Modeling: ARIMAX & VAR",
    "section": "Key Insights",
    "text": "Key Insights\n\n1. ARIMAX vs VAR: Choosing the Right Framework\nThe two multivariate approaches serve different purposes:\n\nUse ARIMAX when one variable is clearly the “response” and others are external drivers (unidirectional causation)\n\nExample: Attendance driven by game quality and COVID restrictions\n\nUse VAR when variables mutually influence each other with no clear directionality (bidirectional feedback)\n\nExample: ORtg, Pace, and 3PAr co-evolving in a dynamic system\n\n\n\n\n2. Exogenous Variables Add Real Predictive Power\nBoth ARIMAX applications outperformed plain ARIMA models, confirming that:\n\nShot selection and skill (3PAr, TS%) improve ORtg forecasts beyond temporal patterns alone\nThe COVID dummy was essential for attendance modeling—without it, the pandemic shock appears as inexplicable forecast error\n\nIncluding the right exogenous variables isn’t just theoretically justified; it’s empirically beneficial.\n\n\n3. The Analytics Revolution’s Temporal Structure\nOur models reveal how the transformation unfolded:\n\n3PAr and TS% significantly predict ORtg (ARIMAX confirms correlational structure)\nGranger causality tests determine temporal ordering: Did strategy precede efficiency, or vice versa?\nVAR captures bidirectional feedback: Success breeds more experimentation, experimentation breeds success\n\nThe analytics revolution wasn’t a one-time decision—it was an iterative process of strategic experimentation and reinforcement learning.\n\n\n4. COVID-19 as a Natural Experiment\nIntervention analysis quantifies what everyone witnessed:\n\nThe pandemic reduced attendance by ~20 million (near-complete collapse)\nThis shock was structural and unpredictable from pre-2020 trends—no amount of historical data could forecast a global pandemic\nThe dummy variable approach cleanly separates the COVID effect from underlying trends\n\n\n\n5. Cross-Validation: The Reality Check\nOut-of-sample testing separated signal from noise:\n\nIn-sample fit can be misleading (models memorize rather than generalize)\nTest-set RMSE reveals true predictive performance on unseen data\nSome complex models fit training data beautifully but collapse when forecasting—the bias-variance trade-off in action",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#looking-ahead-models-4-5",
    "href": "multiTS_model.html#looking-ahead-models-4-5",
    "title": "Multivariate Time Series Modeling: ARIMAX & VAR",
    "section": "Looking Ahead: Models 4 & 5",
    "text": "Looking Ahead: Models 4 & 5\nFor the final portfolio submission, two additional multivariate models will extend this analysis:\nModel 4: VAR(Pace, 3PAr, eFG%)\nResearch Question: Does pace drive shot selection, or vice versa? By modeling the co-evolution of game speed, three-point volume, and shooting efficiency, this VAR will reveal whether the analytics revolution prioritized tempo changes or shot distribution shifts.\nModel 5: VAR(DKNG, Attendance, ORtg) — Weekly Data\nResearch Question: Do sports betting markets respond to NBA performance metrics and attendance recovery? Post-COVID, the sports gambling industry exploded. This model tests whether betting stock prices (DraftKings) react to game quality and fan engagement—linking basketball analytics to financial markets.\nThese models represent natural extensions of the multivariate framework, exploring how basketball’s transformation intersects with broader economic and strategic dynamics.",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  }
]