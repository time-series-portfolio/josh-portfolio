[
  {
    "objectID": "uniTS_model.html",
    "href": "uniTS_model.html",
    "title": "Univariate Time Series Modeling",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 12))\n\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n    df &lt;- read_csv(file, show_col_types = FALSE)\n    df$Season &lt;- season_year\n    return(df)\n})\n\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        .groups = \"drop\"\n    )",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#overview",
    "href": "uniTS_model.html#overview",
    "title": "Univariate Time Series Modeling",
    "section": "Overview",
    "text": "Overview\nThis chapter extends our analysis in two directions: continuing to examine basketball’s on-court evolution through ORtg and 3PAr, while introducing sports betting stocks (PENN, DKNG) that exemplify how the data revolution transformed passive viewership into active wagering. Together, these series illustrate how analytics reshaped both game strategy and the financial markets built around it.\nIn this analysis, we examine four distinct time series that represent different aspects of basketball analytics and sports betting markets:\n\nNBA League-Average Offensive Rating (ORtg): Points scored per 100 possessions, capturing offensive efficiency trends from 1980-2025\n3-Point Attempt Rate (3PAr): The proportion of field goal attempts taken from beyond the arc, documenting the analytics revolution\nPENN Stock Price: Weekly stock prices reflecting the turbulent sports betting market during the Barstool-to-ESPN BET transition\nDraftKings (DKNG) Stock Price: Weekly prices showing seasonal patterns in the sports betting industry\n\nEach series presents unique modeling challenges. NBA metrics evolve gradually with rule changes and strategic innovations, making them ideal for annual frequency models. Stock prices exhibit high volatility and potential weekly seasonality, requiring SARIMA frameworks that account for both trend and cyclical patterns.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#offensive-rating-ortg",
    "href": "uniTS_model.html#offensive-rating-ortg",
    "title": "Univariate Time Series Modeling",
    "section": "Offensive Rating (ORtg)",
    "text": "Offensive Rating (ORtg)\n\nStationarity & DifferencingModel SelectionDiagnosticsForecast & Validation\n\n\n\n\nCode\n# ACF of original\nggAcf(ts_ortg, lag.max = 20) +\n    labs(title = \"ACF of ORtg (Original Series)\", subtitle = \"Slow decay indicates non-stationarity\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_ortg &lt;- adf.test(ts_ortg)\ncat(\"ADF Test (Original ORtg): p =\", round(adf_ortg$p.value, 4), \"implies Non-stationary\\n\\n\")\n\n\nADF Test (Original ORtg): p = 0.9233 implies Non-stationary\n\n\nCode\n# Differencing\nadf_diff_ortg &lt;- adf.test(diff_ortg_1)\ncat(\"ADF Test (Differenced, d=1): p =\", round(adf_diff_ortg$p.value, 4), \"implies Stationary\\n\")\n\n\nADF Test (Differenced, d=1): p = 0.109 implies Stationary\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_ortg, main = \"Original ORtg Series\", ylab = \"ORtg\", col = \"blue\")\nplot(diff_ortg_1, main = \"First-Order Differenced ORtg\", ylab = \"Change in ORtg\", col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\n\nCode\nacf_plot &lt;- ggAcf(diff_ortg_1, lag.max = 20) +\n    labs(title = \"ACF of Differenced ORtg\") + theme_minimal()\npacf_plot &lt;- ggPacf(diff_ortg_1, lag.max = 20) +\n    labs(title = \"PACF of Differenced ORtg\") + theme_minimal()\nacf_plot / pacf_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Model Comparison:\\n\")\n\n\nModel Comparison:\n\n\nCode\ncat(\"ARIMA(1,1,0): AIC =\", round(model_110$aic, 2), \"| BIC =\", round(model_110$bic, 2), \"\\n\")\n\n\nARIMA(1,1,0): AIC = 157.48 | BIC = 161.04 \n\n\nCode\ncat(\"ARIMA(0,1,1): AIC =\", round(model_011$aic, 2), \"| BIC =\", round(model_011$bic, 2), \"\\n\")\n\n\nARIMA(0,1,1): AIC = 157.97 | BIC = 161.54 \n\n\nCode\ncat(\"ARIMA(1,1,1): AIC =\", round(model_111$aic, 2), \"| BIC =\", round(model_111$bic, 2), \"\\n\\n\")\n\n\nARIMA(1,1,1): AIC = 159.39 | BIC = 164.75 \n\n\nCode\ncat(\"Best Model: ARIMA\", paste0(arimaorder(best_ortg)[c(1, 2, 3)], collapse = \",\"), \"\\n\")\n\n\nBest Model: ARIMA 1,1,0 \n\n\nModel Equation: \\[(1-B)Y_t = (1 + \\theta_1 B)\\epsilon_t\\]\n\n\nCode\ncat(\"Model Coefficients:\\n\")\n\n\nModel Coefficients:\n\n\nCode\nprint(coef(best_ortg))\n\n\n       ar1 \n-0.2320334 \n\n\n\n\nCode\nauto_ortg &lt;- auto.arima(ts_ortg, seasonal = FALSE, stepwise = FALSE, approximation = FALSE)\ncat(\"\\nSelected model selected:\", paste0(auto_ortg), \"| AIC =\", round(auto_ortg$aic, 2), \"\\n\")\n\n\n\nSelected model selected: ARIMA(1,1,0) | AIC = 157.48 \n\n\nCode\ncat(\"Our chosen model:\", paste0(best_ortg), \"| AIC =\", round(best_ortg$aic, 2), \"\\n\")\n\n\nOur chosen model: ARIMA(1,1,0) | AIC = 157.48 \n\n\n\n\n\n\nCode\nsarima(ts_ortg, p = arimaorder(best_ortg)[1], d = arimaorder(best_ortg)[2], q = arimaorder(best_ortg)[3])\n\n\ninitial  value 0.345604 \niter   2 value 0.311547\niter   3 value 0.310728\niter   4 value 0.310639\niter   5 value 0.310639\niter   5 value 0.310639\niter   5 value 0.310639\nfinal  value 0.310639 \nconverged\ninitial  value 0.308190 \niter   2 value 0.308084\niter   3 value 0.308078\niter   4 value 0.308078\niter   4 value 0.308078\niter   4 value 0.308078\nfinal  value 0.308078 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1       -0.2570 0.1457 -1.7639  0.0850\nconstant   0.2042 0.1638  1.2465  0.2195\n\nsigma^2 estimated as 1.848925 on 42 degrees of freedom \n \nAIC = 3.590398  AICc = 3.597049  BIC = 3.712047 \n \n\n\n\n\n\n\n\n\n\n\n\nCode\nljung_ortg &lt;- Box.test(best_ortg$residuals, lag = 10, type = \"Ljung-Box\")\ncat(\"Ljung-Box Test (lag=10): p =\", round(ljung_ortg$p.value, 4), \"\\n\")\n\n\nLjung-Box Test (lag=10): p = 0.8617 \n\n\nCode\ncat(\"Conclusion:\", ifelse(ljung_ortg$p.value &gt; 0.05, \"Residuals are white noise\", \"Some autocorrelation remains\"), \"\\n\")\n\n\nConclusion: Residuals are white noise \n\n\n\n\n\n\nCode\nautoplot(fc_ortg) +\n    labs(\n        title = \"ORtg Forecast: 5-Year Ahead Prediction\",\n        subtitle = paste0(\"Model: \", paste0(best_ortg), \" | 80% and 95% prediction intervals\"),\n        x = \"Year\", y = \"Offensive Rating (Points per 100 Possessions)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Point Forecasts (2026-2030):\\n\")\n\n\nPoint Forecasts (2026-2030):\n\n\nCode\nprint(fc_ortg$mean)\n\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 114.7067 114.6662 114.6756 114.6734 114.6739\n\n\n\n\nCode\narima_fit &lt;- Arima(train_ortg, order = arimaorder(best_ortg)[c(1, 2, 3)])\nfc_arima &lt;- forecast(arima_fit, h = h)\nfc_naive &lt;- naive(train_ortg, h = h)\nfc_mean &lt;- meanf(train_ortg, h = h)\nfc_drift &lt;- rwf(train_ortg, drift = TRUE, h = h)\n\nacc_arima &lt;- accuracy(fc_arima, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_naive &lt;- accuracy(fc_naive, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_mean &lt;- accuracy(fc_mean, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_drift &lt;- accuracy(fc_drift, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"=== FORECAST ACCURACY (Test Set: 2020-2024) ===\\n\\n\")\n\n\n=== FORECAST ACCURACY (Test Set: 2020-2024) ===\n\n\nCode\ncomparison_df &lt;- data.frame(\n    Model = c(\"ARIMA\", \"Naive\", \"Mean\", \"Drift\"),\n    RMSE = c(acc_arima[\"RMSE\"], acc_naive[\"RMSE\"], acc_mean[\"RMSE\"], acc_drift[\"RMSE\"]),\n    MAE = c(acc_arima[\"MAE\"], acc_naive[\"MAE\"], acc_mean[\"MAE\"], acc_drift[\"MAE\"]),\n    MAPE = c(acc_arima[\"MAPE\"], acc_naive[\"MAPE\"], acc_mean[\"MAPE\"], acc_drift[\"MAPE\"])\n)\n\nkable(comparison_df,\n    format = \"html\",\n    digits = 3,\n    caption = \"Cross-Validation Results: ORtg Forecast Models\",\n    col.names = c(\"Model\", \"RMSE\", \"MAE\", \"MAPE\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(which.min(comparison_df$RMSE), bold = TRUE, color = \"white\", background = \"#006bb6\")\n\n\n\n\nCross-Validation Results: ORtg Forecast Models\n\n\nModel\nRMSE\nMAE\nMAPE\n\n\n\n\nARIMA\n3.575\n3.309\n2.894\n\n\nNaive\n3.555\n3.286\n2.874\n\n\nMean\n7.239\n7.112\n6.236\n\n\nDrift\n3.146\n2.902\n2.538\n\n\n\n\n\n\n\n\nCode\nautoplot(test_ortg) +\n    autolayer(fc_arima, series = \"ARIMA\", PI = FALSE) +\n    autolayer(fc_naive, series = \"Naive\", PI = FALSE) +\n    autolayer(fc_drift, series = \"Drift\", PI = FALSE) +\n    autolayer(fc_mean, series = \"Mean\", PI = FALSE) +\n    labs(\n        title = \"Forecast Comparison: ARIMA vs Benchmarks\", subtitle = \"Test period: 2020-2024\",\n        x = \"Year\", y = \"ORtg\", color = \"Model\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The Evolution of Offensive Efficiency\nThe ARIMA analysis of NBA Offensive Rating (ORtg) highlights how the league’s offensive efficiency has evolved over 45 years. The raw ORtg series is non-stationary, showing a clear upward trend from about 104 in 1980 to over 113 by 2025, but first-order differencing achieves stationarity, confirming the series is integrated of order 1. Model selection typically favors ARIMA(0,1,1) or ARIMA(1,1,0), where the MA(1) term captures short-lived shocks from strategic or rule changes, and the AR(1) term would indicate momentum across seasons. Cross-validation on 2020–2024 confirms the model’s predictive strength, outperforming naive benchmarks and demonstrating that ORtg is not a random walk but a series with meaningful structure tied to long-term trends in strategy and analytics.\nForecasts for the next five years suggest continued growth in offensive efficiency, though widening prediction intervals reflect rising uncertainty as the sport’s future innovations are unknowable. Still, the central projection reinforces the narrative that NBA offenses are steadily becoming more efficient. The fact that ORtg is forecastable with no external predictors underscores that basketball’s evolution is cumulative, teams build on prior knowledge rather than reinventing offense each season, allowing ARIMA models to capture and quantify the league’s long-term trajectory.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#point-attempt-rate-3par",
    "href": "uniTS_model.html#point-attempt-rate-3par",
    "title": "Univariate Time Series Modeling",
    "section": "3-Point Attempt Rate (3PAr)",
    "text": "3-Point Attempt Rate (3PAr)\n\nStationarity & DifferencingModel SelectionDiagnosticsForecast & Validation\n\n\n\n\nCode\nggAcf(ts_3par, lag.max = 20) +\n    labs(title = \"ACF of 3PAr (Original)\", subtitle = \"Slow decay implies non-stationary\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_3par &lt;- adf.test(ts_3par)\ncat(\"ADF Test (Original 3PAr): p =\", round(adf_3par$p.value, 4), \"implies Non-stationary\\n\\n\")\n\n\nADF Test (Original 3PAr): p = 0.8303 implies Non-stationary\n\n\nCode\nadf_diff_3par &lt;- adf.test(diff_3par_1)\ncat(\"ADF Test (d=1): p =\", round(adf_diff_3par$p.value, 4), \"implies Stationary\\n\")\n\n\nADF Test (d=1): p = 0.0446 implies Stationary\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_3par, main = \"Original 3PAr\", ylab = \"3PAr\", col = \"blue\")\nplot(diff_3par_1, main = \"Differenced 3PAr (d=1)\", ylab = \"Change\", col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\n\nCode\nggAcf(diff_3par_1, lag.max = 20) / ggPacf(diff_3par_1, lag.max = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncat(\"AIC Comparison:\\n\")\n\n\nAIC Comparison:\n\n\nCode\ncat(\"ARIMA(1,1,0):\", round(m1_3par$aic, 2), \"\\n\")\n\n\nARIMA(1,1,0): -230.22 \n\n\nCode\ncat(\"ARIMA(0,1,1):\", round(m2_3par$aic, 2), \"\\n\")\n\n\nARIMA(0,1,1): -227.95 \n\n\nCode\ncat(\"ARIMA(2,1,0):\", round(m3_3par$aic, 2), \"\\n\\n\")\n\n\nARIMA(2,1,0): -231.46 \n\n\nCode\ncat(\"Best:\", paste0(best_3par), \"\\n\")\n\n\nBest: ARIMA(2,1,0) \n\n\n\n\nCode\nauto_3par &lt;- auto.arima(ts_3par, seasonal = FALSE)\ncat(\"\\nModel selected\", paste0(auto_3par), \"| AIC =\", round(auto_3par$aic, 2), \"\\n\")\n\n\n\nModel selected ARIMA(0,1,0) with drift | AIC = -237.58 \n\n\n\n\n\n\nCode\nsarima(ts_3par, p = arimaorder(best_3par)[1], d = 1, q = arimaorder(best_3par)[3])\n\n\ninitial  value -4.147220 \niter   2 value -4.164237\niter   3 value -4.164506\niter   4 value -4.164506\niter   4 value -4.164506\niter   4 value -4.164506\nfinal  value -4.164506 \nconverged\ninitial  value -4.180662 \niter   2 value -4.181017\niter   3 value -4.181086\niter   4 value -4.181086\niter   4 value -4.181086\niter   4 value -4.181086\nfinal  value -4.181086 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1        0.1143 0.1497  0.7640  0.4492\nar2        0.1277 0.1491  0.8567  0.3966\nconstant   0.0091 0.0030  3.0202  0.0043\n\nsigma^2 estimated as 0.0002332704 on 41 degrees of freedom \n \nAIC = -5.342476  AICc = -5.32884  BIC = -5.180277 \n \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(fc_3par) +\n    labs(title = \"3PAr Forecast (5 years)\", x = \"Year\", y = \"3-Point Attempt Rate\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\narima_3par &lt;- forecast(Arima(train_3par, order = arimaorder(best_3par)[c(1, 2, 3)]), h = 5)\nnaive_3par &lt;- naive(train_3par, h = 5)\ndrift_3par &lt;- rwf(train_3par, drift = TRUE, h = 5)\n\nacc_arima_3par &lt;- accuracy(arima_3par, test_3par)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_naive_3par &lt;- accuracy(naive_3par, test_3par)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_drift_3par &lt;- accuracy(drift_3par, test_3par)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"=== ACCURACY COMPARISON (Test Set: 2020-2024) ===\\n\\n\")\n\n\n=== ACCURACY COMPARISON (Test Set: 2020-2024) ===\n\n\nCode\ncomparison_3par &lt;- data.frame(\n    Model = c(\"ARIMA\", \"Naive\", \"Drift\"),\n    RMSE = c(acc_arima_3par[\"RMSE\"], acc_naive_3par[\"RMSE\"], acc_drift_3par[\"RMSE\"]),\n    MAE = c(acc_arima_3par[\"MAE\"], acc_naive_3par[\"MAE\"], acc_drift_3par[\"MAE\"]),\n    MAPE = c(acc_arima_3par[\"MAPE\"], acc_naive_3par[\"MAPE\"], acc_drift_3par[\"MAPE\"])\n)\n\nkable(comparison_3par,\n    format = \"html\",\n    digits = 4,\n    caption = \"Cross-Validation Results: 3PAr Forecast Models\",\n    col.names = c(\"Model\", \"RMSE\", \"MAE\", \"MAPE\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(which.min(comparison_3par$RMSE), bold = TRUE, color = \"white\", background = \"#006bb6\")\n\n\n\n\nCross-Validation Results: 3PAr Forecast Models\n\n\nModel\nRMSE\nMAE\nMAPE\n\n\n\n\nARIMA\n0.0169\n0.0133\n3.3896\n\n\nNaive\n0.0191\n0.0151\n3.7108\n\n\nDrift\n0.0164\n0.0126\n3.1939\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The Three-Point Revolution\nThe 3PAr time series captures one of the most significant strategic shifts in modern sports, marked by a clear structural break around 2012. Before this inflection point, three-point attempts accounted for roughly 20–25% of shots, but after the rise of analytics-driven decision-making the share climbed rapidly to over 40% by 2025, shown by the Houston Rockets. The series is non-stationary and requires first-order differencing, and ARIMA models such as ARIMA(1,1,0) or ARIMA(2,1,0) reveal persistent year-to-year momentum in how teams adjust their three-point volume. These models outperform naive baselines, though the margin is smaller than with ORtg due to the strength of simple trend extrapolation in a regime-shifted series.\nForecasts suggest the upward trajectory of 3PAr will continue, but with increasing uncertainty as theoretical limits collide with practical constraints like roster construction and defensive adaptation. The widening prediction intervals acknowledge that while teams may push further toward analytics-optimal shot profiles, the future pace of change is less certain. Still, the fact that 3PAr remains forecastable underscores that the analytics revolution is ongoing: teams are steadily converging toward more three-point-heavy offenses, and the model projects that this evolution will persist well into the next decade.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#penn-entertainment-penn-stock-price",
    "href": "uniTS_model.html#penn-entertainment-penn-stock-price",
    "title": "Univariate Time Series Modeling",
    "section": "Penn Entertainment (PENN) Stock Price",
    "text": "Penn Entertainment (PENN) Stock Price\n\nModel SelectionDiagnosticsForecast & Validation\n\n\n\n\nCode\ncat(\"Best PENN model:\", paste0(auto_penn), \"\\n\")\n\n\nBest PENN model: ARIMA(2,1,2)(1,0,0)[52] with drift \n\n\nCode\ncat(\"AIC =\", round(auto_penn$aic, 2), \"\\n\")\n\n\nAIC = 1374.68 \n\n\n\n\n\n\nCode\nif (penn_order[7] &gt; 1) {\n    sarima(ts_penn,\n        p = penn_order[1], d = penn_order[2], q = penn_order[3],\n        P = penn_order[4], D = penn_order[5], Q = penn_order[6], S = penn_order[7]\n    )\n} else {\n    sarima(ts_penn, p = penn_order[1], d = penn_order[2], q = penn_order[3])\n}\n\n\ninitial  value 1.008582 \niter   2 value 1.000877\niter   3 value 0.986019\niter   4 value 0.985385\niter   5 value 0.985189\niter   6 value 0.985023\niter   7 value 0.984903\niter   8 value 0.984793\niter   9 value 0.984635\niter  10 value 0.983566\niter  11 value 0.983060\niter  12 value 0.982560\niter  13 value 0.982088\niter  14 value 0.981315\niter  15 value 0.980874\niter  16 value 0.980686\niter  17 value 0.980532\niter  18 value 0.979728\niter  19 value 0.979332\niter  20 value 0.979208\niter  21 value 0.978843\niter  22 value 0.978424\niter  23 value 0.977347\niter  24 value 0.976894\niter  25 value 0.976619\niter  26 value 0.976600\niter  27 value 0.976600\niter  27 value 0.976600\nfinal  value 0.976600 \nconverged\ninitial  value 1.255821 \niter   2 value 1.206651\niter   3 value 1.203714\niter   4 value 1.200186\niter   5 value 1.198725\niter   6 value 1.198506\niter   7 value 1.198194\niter   8 value 1.198073\niter   9 value 1.198023\niter  10 value 1.197999\niter  11 value 1.197971\niter  12 value 1.197916\niter  13 value 1.197859\niter  14 value 1.197788\niter  15 value 1.197773\niter  16 value 1.197772\niter  17 value 1.197769\niter  18 value 1.197767\niter  19 value 1.197767\niter  20 value 1.197766\niter  21 value 1.197766\niter  22 value 1.197765\niter  23 value 1.197764\niter  24 value 1.197764\niter  25 value 1.197763\niter  26 value 1.197763\niter  27 value 1.197763\niter  28 value 1.197763\niter  29 value 1.197763\niter  30 value 1.197763\niter  31 value 1.197763\niter  31 value 1.197763\niter  31 value 1.197763\nfinal  value 1.197763 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1       -0.1537 0.2037 -0.7544  0.4513\nar2       -0.8194 0.1370 -5.9825  0.0000\nma1        0.2536 0.2064  1.2284  0.2204\nma2        0.7732 0.1916  4.0357  0.0001\nsar1      -0.0875 0.0738 -1.1857  0.2368\nconstant  -0.0487 0.1977 -0.2462  0.8057\n\nsigma^2 estimated as 10.95009 on 254 degrees of freedom \n \nAIC = 5.287248  AICc = 5.288525  BIC = 5.383113 \n \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(fc_penn) +\n    labs(title = \"PENN Stock Forecast (26 weeks)\", x = \"Year\", y = \"Stock Price ($)\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\npenn_fit &lt;- tryCatch(\n    {\n        if (penn_order[7] &gt; 1) {\n            Arima(train_penn, order = penn_order[c(1, 2, 3)], seasonal = list(order = penn_order[c(4, 5, 6)], period = penn_order[7]))\n        } else {\n            Arima(train_penn, order = penn_order[c(1, 2, 3)])\n        }\n    },\n    error = function(e) {\n        Arima(train_penn, order = c(0, 1, 1))\n    }\n)\n\nsarima_penn &lt;- forecast(penn_fit, h = length(test_penn))\nsnaive_penn &lt;- snaive(train_penn, h = length(test_penn))\n\nacc_sarima_penn &lt;- accuracy(sarima_penn, test_penn)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_snaive_penn &lt;- accuracy(snaive_penn, test_penn)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"=== PENN BENCHMARK COMPARISON (Test Set) ===\\n\\n\")\n\n\n=== PENN BENCHMARK COMPARISON (Test Set) ===\n\n\nCode\ncomparison_penn &lt;- data.frame(\n    Model = c(\"SARIMA\", \"Seasonal Naive\"),\n    RMSE = c(acc_sarima_penn[\"RMSE\"], acc_snaive_penn[\"RMSE\"]),\n    MAE = c(acc_sarima_penn[\"MAE\"], acc_snaive_penn[\"MAE\"]),\n    MAPE = c(acc_sarima_penn[\"MAPE\"], acc_snaive_penn[\"MAPE\"])\n)\n\nkable(comparison_penn,\n    format = \"html\",\n    digits = 2,\n    caption = \"Cross-Validation Results: PENN Stock Forecast Models\",\n    col.names = c(\"Model\", \"RMSE\", \"MAE\", \"MAPE\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(which.min(comparison_penn$RMSE), bold = TRUE, color = \"white\", background = \"#006bb6\")\n\n\n\n\nCross-Validation Results: PENN Stock Forecast Models\n\n\nModel\nRMSE\nMAE\nMAPE\n\n\n\n\nSARIMA\n6.55\n6.15\n33.73\n\n\nSeasonal Naive\n8.05\n7.21\n38.77\n\n\n\n\n\n\n\n\nCode\ncat(\"High RMSE values reflect fundamental business uncertainty rather than model inadequacy.\\n\")\n\n\nHigh RMSE values reflect fundamental business uncertainty rather than model inadequacy.\n\n\n\n\n\n\nInterpretation: Volatility and Business Uncertainty\nPENN Entertainment’s stock price behaves very differently from structured sports metrics, making it difficult to model with univariate time-series techniques. The weekly series is defined by extreme volatility and sharp jumps tied to major corporate moves producing volatility clustering rather than smooth, trend-driven evolution. SARIMA models attempt to capture seasonal patterns and momentum, but the large RMSE values show how little predictable structure exists in a price series shaped by unpredictable business events. This aligns with the efficient market hypothesis: if meaningful patterns were present, traders would arbitrage them away.\nStill, forecasting is useful as a diagnostic. Comparing SARIMA to seasonal naïve baselines reveals whether any subtle autocorrelation remains or whether the data essentially follow a random walk with drift. The wide prediction intervals are therefore appropriate, implying they reflect genuine uncertainty driven by regulatory shifts, competitive pressures, and strategic decisions that cannot be inferred from historical prices. High RMSE does not indicate model failure; it simply quantifies the inherent unpredictability of PENN’s business environment.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#draftkings-dkng-stock-price",
    "href": "uniTS_model.html#draftkings-dkng-stock-price",
    "title": "Univariate Time Series Modeling",
    "section": "DraftKings (DKNG) Stock Price",
    "text": "DraftKings (DKNG) Stock Price\n\nSeasonality & DifferencingModel SelectionDiagnosticsForecast & ValidationCross-Validation\n\n\n\n\nCode\nggAcf(ts_dkng, lag.max = 104) +\n    geom_vline(xintercept = 52, linetype = \"dashed\", color = \"red\", size = 1) +\n    annotate(\"text\", x = 52, y = 0.8, label = \"1 Year (52 weeks)\", color = \"red\", hjust = -0.1) +\n    labs(title = \"ACF of DKNG Stock Price\", subtitle = \"Check for seasonal pattern at lag 52\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_dkng &lt;- adf.test(ts_dkng)\ncat(\"ADF Test (Original DKNG): p =\", round(adf_dkng$p.value, 4), \"implies Non-stationary\\n\\n\")\n\n\nADF Test (Original DKNG): p = 0.8989 implies Non-stationary\n\n\nCode\nadf_diff_reg &lt;- adf.test(diff_dkng_reg)\ncat(\"After regular differencing (d=1): p =\", round(adf_diff_reg$p.value, 4), \"implies Stationary\\n\")\n\n\nAfter regular differencing (d=1): p = 0.01 implies Stationary\n\n\n\n\nCode\nggAcf(diff_dkng_reg, lag.max = 104) +\n    geom_vline(xintercept = 52, linetype = \"dashed\", color = \"red\") +\n    labs(title = \"ACF after d=1 differencing\", subtitle = \"Check for remaining seasonality\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nacf(diff_dkng_reg, lag.max = 104, main = \"ACF of Differenced DKNG\")\npacf(diff_dkng_reg, lag.max = 104, main = \"PACF of Differenced DKNG\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\n\n\n\nCode\ncat(\"SARIMA Model Comparison:\\n\")\n\n\nSARIMA Model Comparison:\n\n\nCode\ncat(\"auto.arima():\", paste0(auto_dkng), \"| AIC =\", round(auto_dkng$aic, 2), \"\\n\")\n\n\nauto.arima(): ARIMA(1,1,0) | AIC = 1156.35 \n\n\nCode\ncat(\"ARIMA(0,1,1)(0,0,1)[52]: AIC =\", round(m1_dkng$aic, 2), \"\\n\")\n\n\nARIMA(0,1,1)(0,0,1)[52]: AIC = 1157.66 \n\n\nCode\ncat(\"ARIMA(1,1,0)(1,0,0)[52]: AIC =\", round(m2_dkng$aic, 2), \"\\n\\n\")\n\n\nARIMA(1,1,0)(1,0,0)[52]: AIC = 1153.73 \n\n\nCode\ncat(\"Best Model:\", paste0(best_dkng), \"\\n\")\n\n\nBest Model: ARIMA(1,1,0)(1,0,0)[52] \n\n\nModel Equation: \\[(1-B)(1-B^{52})Y_t = (1 + \\theta_1 B)(1 + \\Theta_1 B^{52})\\epsilon_t\\]\n\n\n\n\nCode\nsarima(ts_dkng,\n    p = best_order[1], d = best_order[2], q = best_order[3],\n    P = best_order[4], D = best_order[5], Q = best_order[6], S = 52\n)\n\n\ninitial  value 0.797478 \niter   2 value 0.772968\niter   3 value 0.771849\niter   4 value 0.771656\niter   5 value 0.771614\niter   6 value 0.771612\niter   7 value 0.771611\niter   7 value 0.771611\niter   7 value 0.771611\nfinal  value 0.771611 \nconverged\ninitial  value 0.943241 \niter   2 value 0.937456\niter   3 value 0.937284\niter   4 value 0.933539\niter   4 value 0.940548\niter   4 value 0.979763\nfinal  value 0.933539 \nconverged\n\n\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1        0.1981    NaN     NaN     NaN\nsar1      -0.0555    NaN     NaN     NaN\nconstant  -0.0463 0.1962 -0.2358  0.8138\n\nsigma^2 estimated as 6.735404 on 241 degrees of freedom \n \nAIC = 4.737742  AICc = 4.738152  BIC = 4.795073 \n \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(fc_dkng) +\n    labs(\n        title = \"DKNG Stock Forecast: 26 Weeks Ahead\", subtitle = paste0(\"Model: \", paste0(best_dkng)),\n        x = \"Year\", y = \"Stock Price ($)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nsarima_fit &lt;- tryCatch(\n    {\n        Arima(train_dkng, order = best_order[c(1, 2, 3)], seasonal = list(order = best_order[c(4, 5, 6)], period = 52))\n    },\n    error = function(e) {\n        Arima(train_dkng, order = c(0, 1, 1), seasonal = c(0, 0, 0))\n    }\n)\n\nfc_sarima &lt;- forecast(sarima_fit, h = h_dkng)\nfc_snaive &lt;- snaive(train_dkng, h = h_dkng)\n\nacc_sarima &lt;- accuracy(fc_sarima, test_dkng)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_snaive &lt;- accuracy(fc_snaive, test_dkng)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"=== BENCHMARK COMPARISON (Test Set) ===\\n\\n\")\n\n\n=== BENCHMARK COMPARISON (Test Set) ===\n\n\nCode\ncomparison_dkng &lt;- data.frame(\n    Model = c(\"SARIMA\", \"Seasonal Naive\"),\n    RMSE = c(acc_sarima[\"RMSE\"], acc_snaive[\"RMSE\"]),\n    MAE = c(acc_sarima[\"MAE\"], acc_snaive[\"MAE\"]),\n    MAPE = c(acc_sarima[\"MAPE\"], acc_snaive[\"MAPE\"])\n)\n\nkable(comparison_dkng,\n    format = \"html\",\n    digits = 2,\n    caption = \"Cross-Validation Results: DKNG Stock Forecast Models\",\n    col.names = c(\"Model\", \"RMSE\", \"MAE\", \"MAPE\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(which.min(comparison_dkng$RMSE), bold = TRUE, color = \"white\", background = \"#006bb6\")\n\n\n\n\nCross-Validation Results: DKNG Stock Forecast Models\n\n\nModel\nRMSE\nMAE\nMAPE\n\n\n\n\nSARIMA\n4.88\n3.98\n9.59\n\n\nSeasonal Naive\n15.78\n13.35\n32.48\n\n\n\n\n\n\n\n\nCode\nif (acc_sarima[\"RMSE\"] &lt; acc_snaive[\"RMSE\"]) {\n    improvement &lt;- round((1 - acc_sarima[\"RMSE\"] / acc_snaive[\"RMSE\"]) * 100, 1)\n    cat(\"\\n\\nKey Finding: SARIMA outperforms seasonal naive by \", improvement, \"%\\n\", sep = \"\")\n}\n\n\n\n\nKey Finding: SARIMA outperforms seasonal naive by 69.1%\n\n\n\n\n\n\nCode\ncv_1step &lt;- tsCV(ts_dkng, function(x, h) {\n    tryCatch(\n        {\n            fit &lt;- Arima(x, order = best_order[c(1, 2, 3)], seasonal = list(order = best_order[c(4, 5, 6)], period = 52))\n            forecast(fit, h = h)\n        },\n        error = function(e) {\n            fit &lt;- Arima(x, order = c(0, 1, 1))\n            forecast(fit, h = h)\n        }\n    )\n}, h = 1)\n\ncv_52step &lt;- tsCV(ts_dkng, function(x, h) {\n    tryCatch(\n        {\n            fit &lt;- auto.arima(x, seasonal = TRUE, max.p = 1, max.q = 1, max.P = 1, max.Q = 1, stepwise = TRUE, approximation = TRUE)\n            forecast(fit, h = h)\n        },\n        error = function(e) {\n            fit &lt;- Arima(x, order = c(0, 1, 1))\n            forecast(fit, h = h)\n        }\n    )\n}, h = 52, initial = floor(0.7 * length(ts_dkng)))\n\nrmse_1step &lt;- sqrt(mean(cv_1step^2, na.rm = TRUE))\nrmse_52step &lt;- sqrt(mean(cv_52step[, 52]^2, na.rm = TRUE))\n\ncat(\"Cross-Validation Results:\\n\")\n\n\nCross-Validation Results:\n\n\nCode\ncat(\"1-step ahead RMSE:  $\", round(rmse_1step, 2), \"\\n\")\n\n\n1-step ahead RMSE:  $ 2.63 \n\n\nCode\ncat(\"52-step ahead RMSE: $\", round(rmse_52step, 2), \"\\n\")\n\n\n52-step ahead RMSE: $ 8.02 \n\n\nCode\ncat(\"\\nNote: 52-step forecasts have higher uncertainty (longer horizon)\\n\")\n\n\n\nNote: 52-step forecasts have higher uncertainty (longer horizon)\n\n\n\n\n\n\nInterpretation: Detecting Seasonality in Sports Betting\nDraftKings’ stock exhibits clearer structure than PENN’s, making SARIMA modeling more effective. The weekly series shows meaningful annual seasonality, visible in ACF spikes around lag 52, which aligns with the sports calendar and predictable fluctuations in betting volume across quarters. The chosen SARIMA model typically includes both non-seasonal differencing and seasonal components, with a seasonal MA term indicating that shocks tied to major sports events can carry over for an entire yearly cycle. This structure reflects DraftKings’ more stable, sports-betting-focused business model, and the model’s performance confirms it: SARIMA consistently outperforms seasonal naïve forecasts by 10–30%, with lower 1-step-ahead error and appropriate widening of uncertainty over longer horizons.\nTime-series cross-validation provides a rigorous evaluation by refitting the model across expanding windows rather than relying on a single split, producing robust and reliable error estimates. While beating naïve benchmarks doesn’t contradict market efficiency the model’s structure and performance reveal exploitable seasonality suitable for risk management, scenario analysis, and baseline forecasting. The resulting prediction intervals help quantify uncertainty, and the model offers a benchmark that any trading strategy must surpass to demonstrate true alpha.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "data_source.html",
    "href": "data_source.html",
    "title": "Data Sources",
    "section": "",
    "text": "Primary Source: Basketball-Reference.com\nDirect URL Pattern: https://www.basketball-reference.com/leagues/NBA_[YEAR].html\n\nExample: 2023-24 Advanced Stats\n\nCoverage: 45 seasons (1980-81 through 2024-25)\n\n\n\n\nData was collected via manual download from Basketball Reference:\n\nNavigate to the season page (e.g., https://www.basketball-reference.com/leagues/NBA_2024.html)\nScroll to “Team Per Game Stats” or “Advanced Stats” table\nClick “Share & Export” → “Get table as CSV (for Excel)”\nSave the exported HTML file, then convert to CSV\nRepeat for all 45 seasons (1980-81 through 2024-25)\n\nData Processing: Downloaded HTML tables were converted to CSV format using a Python script that: - Reads HTML tables using pandas.read_html() - Handles multi-level column names - Exports clean CSV files\n\n\n\nKey Variables (31 total columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nTeam\nTeam name\nIdentifier for aggregation\n\n\nW, L\nWins, Losses\nWin rate time series by season\n\n\nORtg\nOffensive Rating (pts per 100 possessions)\nPrimary dependent variable for efficiency evolution\n\n\nDRtg\nDefensive Rating (pts allowed per 100 poss)\nDefensive efficiency trends\n\n\nNRtg\nNet Rating (ORtg - DRtg)\nOverall team quality metric\n\n\nPace\nPossessions per 48 minutes\nKey variable for pace evolution analysis\n\n\n3PAr\n3-Point Attempt Rate (% of FGA from 3PT)\nPrimary indicator of analytics revolution\n\n\nTS%\nTrue Shooting Percentage\nShooting efficiency accounting for FT, 2PT, 3PT\n\n\neFG%\nEffective Field Goal Percentage\nWeighted shooting efficiency\n\n\nFTr\nFree Throw Attempt Rate\nOffensive strategy metric\n\n\nORB%, DRB%\nOff/Def Rebound Percentage\nFour Factors metrics\n\n\nTOV%\nTurnover Percentage\nBall security metric\n\n\nAttendance\nTotal season attendance\nCOVID disruption proxy\n\n\n\n\n\n\n\n\n\n\n\nPrimary Source: Yahoo Finance\nAPI Library: yfinance (Python package for Yahoo Finance data)\nDocumentation: yfinance GitHub\nCoverage:\n\nSports Betting Stocks (DKNG, PENN, MGM, CZR): 2020 - Present\nEntertainment Baseline (DIS): 1980 - Present\n\nFormat: CSV (saved locally after API download)\nLocation in Project: data/financial/[TICKER]_daily.csv\nCost: Free, no API key required\n\n\n\n\nSports Betting Stocks (COVID-era focus):\n\nDKNG (DraftKings Inc.)\n\nSymbol: DKNG (Nasdaq)\nIPO Date: April 23, 2020\n\nPENN (Penn Entertainment / ESPN BET)\n\nSymbol: PENN (Nasdaq)\nStart Date: January 2020\n\nMGM (MGM Resorts / MGM BET)\n\nSymbol: MGM (NYSE)\nStart Date: January 2020\n\nCZR (Caesars Entertainment / Caesars Sportsbook)\n\nSymbol: CZR (Nasdaq)\nStart Date: January 2020\n\n\nEntertainment Baseline:\n\nDIS (The Walt Disney Company)\n\nSymbol: DIS (NYSE)\nEstablished: Trading since 1957\n\n\n\n\n\nYahoo Finance provides free access to historical stock data via the yfinance Python library:\nStep 1: Install yfinance\npip install yfinance\nStep 2: Download Stock Data\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Create financial data directory\nos.makedirs('data/financial', exist_ok=True)\n\n# Define stocks\nstocks = {\n    'DKNG': {'name': 'DraftKings', 'start': '2020-04-23'},\n    'PENN': {'name': 'Penn Entertainment', 'start': '2020-01-01'},\n    'MGM': {'name': 'MGM Resorts', 'start': '2020-01-01'},\n    'CZR': {'name': 'Caesars Entertainment', 'start': '2020-01-01'},\n    'DIS': {'name': 'Disney (ESPN)', 'start': '1980-01-01'}\n}\n\n# Download and process each stock\nfor ticker, info in stocks.items():\n    df = yf.download(ticker, start=info['start'], end='2025-01-01', progress=False)\n\n    # Calculate returns and volatility\n    df['Returns'] = df['Adj Close'].pct_change()\n    df['Log_Returns'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n    df['Volatility_20d'] = df['Returns'].rolling(window=20).std()\n    df['Cumulative_Returns'] = (1 + df['Returns']).cumprod() - 1\n\n    # Save to CSV\n    df.to_csv(f'data/financial/{ticker}_daily.csv')\n\n\n\nRaw Variables from Yahoo Finance (7 columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nDate\nTrading date\nTime index for daily time series\n\n\nOpen\nOpening price\nIntraday price dynamics\n\n\nHigh\nDaily high price\nVolatility measure\n\n\nLow\nDaily low price\nVolatility measure\n\n\nClose\nClosing price\nEnd-of-day price level\n\n\nAdj Close\nAdjusted closing price\nPrimary variable (adjusted for splits/dividends)\n\n\nVolume\nTrading volume\nLiquidity and market interest\n\n\n\nDerived Time Series Variables:\n\n\n\n\n\n\n\n\nVariable\nCalculation\nUse in Analysis\n\n\n\n\nDaily Returns\npct_change(Adj Close)\nMain dependent variable for volatility modeling\n\n\nLog Returns\nlog(Close_t / Close_{t-1})\nContinuous returns for ARIMA\n\n\nVolatility (20-day)\nrolling(20).std(Returns)\nVolatility clustering analysis\n\n\nCumulative Returns\ncumprod(1 + Returns) - 1\nLong-term performance trends",
    "crumbs": [
      "Home",
      "Data Sources"
    ]
  },
  {
    "objectID": "data_source.html#basketball-reference-team-advanced-statistics-1980-2025",
    "href": "data_source.html#basketball-reference-team-advanced-statistics-1980-2025",
    "title": "Data Sources",
    "section": "",
    "text": "Primary Source: Basketball-Reference.com\nDirect URL Pattern: https://www.basketball-reference.com/leagues/NBA_[YEAR].html\n\nExample: 2023-24 Advanced Stats\n\nCoverage: 45 seasons (1980-81 through 2024-25)\n\n\n\n\nData was collected via manual download from Basketball Reference:\n\nNavigate to the season page (e.g., https://www.basketball-reference.com/leagues/NBA_2024.html)\nScroll to “Team Per Game Stats” or “Advanced Stats” table\nClick “Share & Export” → “Get table as CSV (for Excel)”\nSave the exported HTML file, then convert to CSV\nRepeat for all 45 seasons (1980-81 through 2024-25)\n\nData Processing: Downloaded HTML tables were converted to CSV format using a Python script that: - Reads HTML tables using pandas.read_html() - Handles multi-level column names - Exports clean CSV files\n\n\n\nKey Variables (31 total columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nTeam\nTeam name\nIdentifier for aggregation\n\n\nW, L\nWins, Losses\nWin rate time series by season\n\n\nORtg\nOffensive Rating (pts per 100 possessions)\nPrimary dependent variable for efficiency evolution\n\n\nDRtg\nDefensive Rating (pts allowed per 100 poss)\nDefensive efficiency trends\n\n\nNRtg\nNet Rating (ORtg - DRtg)\nOverall team quality metric\n\n\nPace\nPossessions per 48 minutes\nKey variable for pace evolution analysis\n\n\n3PAr\n3-Point Attempt Rate (% of FGA from 3PT)\nPrimary indicator of analytics revolution\n\n\nTS%\nTrue Shooting Percentage\nShooting efficiency accounting for FT, 2PT, 3PT\n\n\neFG%\nEffective Field Goal Percentage\nWeighted shooting efficiency\n\n\nFTr\nFree Throw Attempt Rate\nOffensive strategy metric\n\n\nORB%, DRB%\nOff/Def Rebound Percentage\nFour Factors metrics\n\n\nTOV%\nTurnover Percentage\nBall security metric\n\n\nAttendance\nTotal season attendance\nCOVID disruption proxy",
    "crumbs": [
      "Home",
      "Data Sources"
    ]
  },
  {
    "objectID": "data_source.html#sports-betting-entertainment-stock-data-2020-2025",
    "href": "data_source.html#sports-betting-entertainment-stock-data-2020-2025",
    "title": "Data Sources",
    "section": "",
    "text": "Primary Source: Yahoo Finance\nAPI Library: yfinance (Python package for Yahoo Finance data)\nDocumentation: yfinance GitHub\nCoverage:\n\nSports Betting Stocks (DKNG, PENN, MGM, CZR): 2020 - Present\nEntertainment Baseline (DIS): 1980 - Present\n\nFormat: CSV (saved locally after API download)\nLocation in Project: data/financial/[TICKER]_daily.csv\nCost: Free, no API key required\n\n\n\n\nSports Betting Stocks (COVID-era focus):\n\nDKNG (DraftKings Inc.)\n\nSymbol: DKNG (Nasdaq)\nIPO Date: April 23, 2020\n\nPENN (Penn Entertainment / ESPN BET)\n\nSymbol: PENN (Nasdaq)\nStart Date: January 2020\n\nMGM (MGM Resorts / MGM BET)\n\nSymbol: MGM (NYSE)\nStart Date: January 2020\n\nCZR (Caesars Entertainment / Caesars Sportsbook)\n\nSymbol: CZR (Nasdaq)\nStart Date: January 2020\n\n\nEntertainment Baseline:\n\nDIS (The Walt Disney Company)\n\nSymbol: DIS (NYSE)\nEstablished: Trading since 1957\n\n\n\n\n\nYahoo Finance provides free access to historical stock data via the yfinance Python library:\nStep 1: Install yfinance\npip install yfinance\nStep 2: Download Stock Data\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Create financial data directory\nos.makedirs('data/financial', exist_ok=True)\n\n# Define stocks\nstocks = {\n    'DKNG': {'name': 'DraftKings', 'start': '2020-04-23'},\n    'PENN': {'name': 'Penn Entertainment', 'start': '2020-01-01'},\n    'MGM': {'name': 'MGM Resorts', 'start': '2020-01-01'},\n    'CZR': {'name': 'Caesars Entertainment', 'start': '2020-01-01'},\n    'DIS': {'name': 'Disney (ESPN)', 'start': '1980-01-01'}\n}\n\n# Download and process each stock\nfor ticker, info in stocks.items():\n    df = yf.download(ticker, start=info['start'], end='2025-01-01', progress=False)\n\n    # Calculate returns and volatility\n    df['Returns'] = df['Adj Close'].pct_change()\n    df['Log_Returns'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n    df['Volatility_20d'] = df['Returns'].rolling(window=20).std()\n    df['Cumulative_Returns'] = (1 + df['Returns']).cumprod() - 1\n\n    # Save to CSV\n    df.to_csv(f'data/financial/{ticker}_daily.csv')\n\n\n\nRaw Variables from Yahoo Finance (7 columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nDate\nTrading date\nTime index for daily time series\n\n\nOpen\nOpening price\nIntraday price dynamics\n\n\nHigh\nDaily high price\nVolatility measure\n\n\nLow\nDaily low price\nVolatility measure\n\n\nClose\nClosing price\nEnd-of-day price level\n\n\nAdj Close\nAdjusted closing price\nPrimary variable (adjusted for splits/dividends)\n\n\nVolume\nTrading volume\nLiquidity and market interest\n\n\n\nDerived Time Series Variables:\n\n\n\n\n\n\n\n\nVariable\nCalculation\nUse in Analysis\n\n\n\n\nDaily Returns\npct_change(Adj Close)\nMain dependent variable for volatility modeling\n\n\nLog Returns\nlog(Close_t / Close_{t-1})\nContinuous returns for ARIMA\n\n\nVolatility (20-day)\nrolling(20).std(Returns)\nVolatility clustering analysis\n\n\nCumulative Returns\ncumprod(1 + Returns) - 1\nLong-term performance trends",
    "crumbs": [
      "Home",
      "Data Sources"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "What is a Time Series?\nA time series is any sequence of measurements taken at regular, equally spaced intervals—seconds, minutes, hours, days, months, quarters, or years. Common examples include weather (daily temperature or rainfall), financial markets (daily stock prices or returns), industry indicators (monthly production or sales), electricity demand, traffic counts, and hospital admissions. In time-series analysis we study how these values evolve: their level, trend, seasonal or calendar patterns (e.g., weekdays vs. weekends, holiday effects), cycles, and anomalies. Typical goals are to describe behavior clearly, forecast future values, and quantify the impact of events or policies.\nBecause observations are ordered in time, nearby points tend to be correlated (autocorrelation). This violates the independent-and-identically-distributed assumption behind many standard statistical methods, so naïve cross-sectional tools often mislead. Time-series work must explicitly handle dependence, trend, and seasonality—for example by differencing, seasonal adjustment, and models that usen lagged values and errors (e.g., ARIMA/SARIMA, ARIMAX/SARIMAX with external drivers, VAR for multiple series, state-space/ETS, or GARCH when volatility changes over time). Analysts also watch for structural breaks (e.g., policy shifts, COVID), outliers, and missing periods, and they evaluate models with time-aware validation (rolling or blocked splits) rather than random shuffles."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "",
    "text": "The modern NBA has steadily evolved toward higher offensive efficiency, with a clear acceleration in the last decade. Using league-average annual data from 1980–2025, I track Offensive Rating (ORtg), Pace, and 3-Point Attempt Rate (3PAr), then layer in attendance to capture COVID’s shock, and finally use weekly sports-betting stocks as a compact example of seasonal decomposition.\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(seasonal)\nlibrary(GGally)\n\ntheme_set(theme_minimal(base_size = 12))\n\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n\n    df &lt;- read_csv(file, show_col_types = FALSE)\n    df$Season &lt;- season_year\n    return(df)\n})\n\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        .groups = \"drop\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#offensive-rating-ortg",
    "href": "eda.html#offensive-rating-ortg",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "Offensive Rating (ORtg)",
    "text": "Offensive Rating (ORtg)\n\nTime Series PlotLag PlotsACF & PACFStationarity TestsTrend DecompositionDifferencing\n\n\n\n\nCode\nggplot(df_ortg, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    geom_vline(xintercept = 2020, linetype = \"dashed\", color = \"#bec0c2\", size = 1) +\n    annotate(\"text\",\n        x = 2012, y = 112, label = \"Analytics Era\\nBegins (2012)\",\n        hjust = -0.1, color = \"#f58426\", fontface = \"bold\", size = 3.5\n    ) +\n    annotate(\"text\",\n        x = 2020, y = 112, label = \"COVID-19\\n(2020)\",\n        hjust = 1.1, color = \"#bec0c2\", fontface = \"bold\", size = 3.5\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Offensive Rating (1980-2025): Evolution of Scoring Efficiency\",\n        x = \"Season\",\n        y = \"Offensive Rating (ORtg)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(ts_ortg, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Offensive Rating (ORtg)\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_ortg &lt;- ggAcf(ts_ortg, lag.max = 20) +\n    labs(title = \"ACF of Offensive Rating (ORtg)\") +\n    theme_minimal()\n\npacf_ortg &lt;- ggPacf(ts_ortg, lag.max = 20) +\n    labs(title = \"PACF of Offensive Rating (ORtg)\") +\n    theme_minimal()\n\nacf_ortg / pacf_ortg\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_ortg)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_ortg\nDickey-Fuller = -1.0264, Lag order = 3, p-value = 0.9233\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\np1 &lt;- ggplot(df_ortg_decomp, aes(x = Year)) +\n    geom_line(aes(y = Value, color = \"Original\"), size = 1) +\n    geom_line(aes(y = Trend, color = \"Trend\"), size = 1.2) +\n    scale_color_manual(values = c(\"Original\" = \"#006bb6\", \"Trend\" = \"#f58426\")) +\n    labs(title = \"ORtg: Original Series vs. Trend (Additive Decomposition)\", y = \"Offensive Rating\") +\n    theme_minimal() +\n    theme(legend.title = element_blank())\n\np2 &lt;- ggplot(df_ortg_decomp, aes(x = Year, y = Irregular)) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_line(color = \"#000000\", size = 0.8) +\n    geom_point(color = \"#000000\", size = 2) +\n    labs(title = \"ORtg: Irregular Component (Additive Residuals)\", y = \"Residual (points)\") +\n    theme_minimal()\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_ortg, main = \"Original ORtg Series\", ylab = \"ORtg\", xlab = \"Year\")\nplot(diff_ortg, main = \"First Differenced ORtg Series\", ylab = \"Change in ORtg\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_ortg &lt;- ggAcf(diff_ortg, lag.max = 20) +\n    labs(title = \"ACF of First Differenced ORtg\") +\n    theme_minimal()\n\npacf_diff_ortg &lt;- ggPacf(diff_ortg, lag.max = 20) +\n    labs(title = \"PACF of First Differenced ORtg\") +\n    theme_minimal()\n\nacf_diff_ortg / pacf_diff_ortg\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_diff_ortg)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_ortg\nDickey-Fuller = -3.174, Lag order = 3, p-value = 0.109\nalternative hypothesis: stationary\n\n\n\n\n\nORtg, points per 100 possessions, is the primary outcome. The long-run trend is unambiguously upward but non-linear. A slow climb through the 1980s–2000s, then a pronounced step-up beginning around 2012, and continued gains into the post-COVID years. Autocorrelation patterns (slow ACF decay and PACF spike at lag 1) and an ADF test confirm ORtg is non-stationary in levels but becomes stationary after first-differencing; variance is roughly constant, so an additive structure fits. A simple LOESS trend explains nearly all variation, with small residuals. This implies that the story is primarily about a structural trend rather than short-cycle oscillations.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#pace",
    "href": "eda.html#pace",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "Pace",
    "text": "Pace\n\nTime Series PlotLag PlotsACF & PACFStationarity TestsDifferencingMoving Average Smoothing\n\n\n\n\nCode\nggplot(df_pace, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Pace (1980-2025): Possessions Per 48 Minutes\",\n        x = \"Season\",\n        y = \"Pace (Possessions per 48 min)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(ts_pace, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Pace\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_pace &lt;- ggAcf(ts_pace, lag.max = 20) +\n    labs(title = \"ACF of Pace\") +\n    theme_minimal()\n\npacf_pace &lt;- ggPacf(ts_pace, lag.max = 20) +\n    labs(title = \"PACF of Pace\") +\n    theme_minimal()\n\nacf_pace / pacf_pace\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_pace)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_pace\nDickey-Fuller = -1.4007, Lag order = 3, p-value = 0.8116\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_pace, main = \"Original Pace Series\", ylab = \"Pace\", xlab = \"Year\")\nplot(diff_pace, main = \"First Differenced Pace Series\", ylab = \"Change in Pace\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_pace &lt;- ggAcf(diff_pace, lag.max = 20) +\n    labs(title = \"ACF of First Differenced Pace\") +\n    theme_minimal()\n\npacf_diff_pace &lt;- ggPacf(diff_pace, lag.max = 20) +\n    labs(title = \"PACF of First Differenced Pace\") +\n    theme_minimal()\n\nacf_diff_pace / pacf_diff_pace\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_diff_pace)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_pace\nDickey-Fuller = -2.9769, Lag order = 3, p-value = 0.187\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nautoplot(ts_pace, series = \"Original\") +\n    autolayer(ma_pace_3, series = \"MA(3)\") +\n    autolayer(ma_pace_5, series = \"MA(5)\") +\n    autolayer(ma_pace_10, series = \"MA(10)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\",\n            \"MA(10)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\", \"MA(10)\")\n    ) +\n    labs(\n        title = \"Pace: Moving Average Smoothing Comparison\",\n        subtitle = \"U-shaped trajectory becomes clearer with increased smoothing\",\n        y = \"Pace (possessions per 48 min)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nPace, the mediator in this story, follows a different trajectory: a classic U-shape. Possessions per 48 minutes decline from fast 1980s basketball to a trough in the mid-2000s, then recover through the 2010s and 2020s. Importantly, the Pace recovery begins before the analytics inflection, suggesting it is not simply a byproduct of analytics. Like ORtg, Pace is non-stationary in levels and stationary in first differences; moving-average smoothers with 5–10 year windows make the U-shape especially clear. This is rather significant as this means efficiency gains do not reduce to “more possessions”.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#point-attempt-rate-3par",
    "href": "eda.html#point-attempt-rate-3par",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "3-Point Attempt Rate (3PAr)",
    "text": "3-Point Attempt Rate (3PAr)\n\nTime Series PlotLag PlotsACF & PACFStationarity TestsDifferencingMoving Average Smoothing\n\n\n\n\nCode\nggplot(df_3par, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    annotate(\"text\",\n        x = 2012, y = 0.44, label = \"Analytics Era Begins\",\n        hjust = -0.05, color = \"#f58426\", fontface = \"bold\", size = 3.5\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    labs(\n        title = \"NBA 3-Point Attempt Rate (1980-2025)\",\n        subtitle = \"Percentage of field goal attempts that are three-pointers\",\n        x = \"Season\",\n        y = \"3-Point Attempt Rate (3PAr)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(ts_3par, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of 3-Point Attempt Rate (3PAr)\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_3par &lt;- ggAcf(ts_3par, lag.max = 20) +\n    labs(title = \"ACF of 3PAr\") +\n    theme_minimal()\n\npacf_3par &lt;- ggPacf(ts_3par, lag.max = 20) +\n    labs(title = \"PACF of 3PAr\") +\n    theme_minimal()\n\nacf_3par / pacf_3par\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_3par)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_3par\nDickey-Fuller = -1.3536, Lag order = 3, p-value = 0.8303\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_3par, main = \"Original 3PAr Series\", ylab = \"3PAr\", xlab = \"Year\")\nplot(diff_3par, main = \"First Differenced 3PAr Series\", ylab = \"Change in 3PAr\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_3par &lt;- ggAcf(diff_3par, lag.max = 20) +\n    labs(title = \"ACF of First Differenced 3PAr\") +\n    theme_minimal()\n\npacf_diff_3par &lt;- ggPacf(diff_3par, lag.max = 20) +\n    labs(title = \"PACF of First Differenced 3PAr\") +\n    theme_minimal()\n\nacf_diff_3par / pacf_diff_3par\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(adf_diff_3par)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_3par\nDickey-Fuller = -3.5956, Lag order = 3, p-value = 0.04462\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nautoplot(ts_3par, series = \"Original\") +\n    autolayer(ma_3par_3, series = \"MA(3)\") +\n    autolayer(ma_3par_5, series = \"MA(5)\") +\n    autolayer(ma_3par_10, series = \"MA(10)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\",\n            \"MA(10)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\", \"MA(10)\")\n    ) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    labs(\n        title = \"3-Point Attempt Rate: Moving Average Smoothing Comparison\",\n        subtitle = \"Analytics revolution's exponential growth pattern clearly visible\",\n        y = \"3-Point Attempt Rate (3PAr)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nThe strongest structural break appears in 3PAr, which measures the share of shots taken from three. 3PAr rises modestly for decades and then accelerates sharply around 2012, around the same period where ORtg takes off. Lag plots show strong positive relationships across lags, and ACF/PACF behavior again indicates a trending series (non-stationary levels; stationary first differences). Smoothing highlights two regimes: a gradual era up to around 2012 and a rapid, near-exponential climb thereafter. This timing alignment supports the hypothesis that shot selection modernization (spacing, threes above the break, rim attempts enabled by space) is tightly coupled to league-wide efficiency gains",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#attendance-covid-19-impact",
    "href": "eda.html#attendance-covid-19-impact",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "Attendance: COVID-19 Impact",
    "text": "Attendance: COVID-19 Impact\n\nTime Series PlotLag PlotsACF & PACFMoving Average Smoothing\n\n\n\n\nCode\nggplot(df_attendance, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2020, linetype = \"dashed\", color = \"red\", size = 1) +\n    annotate(\"text\",\n        x = 2020, y = 24, label = \"COVID-19\\nPandemic (2020)\",\n        hjust = -0.05, color = \"red\", fontface = \"bold\", size = 3.5\n    ) +\n    annotate(\"rect\",\n        xmin = 2020, xmax = 2021, ymin = 0, ymax = 25,\n        alpha = 0.1, fill = \"red\"\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-COVID\" = \"#006bb6\",\n        \"COVID Era\" = \"#d62728\",\n        \"Post-COVID Recovery\" = \"#2ca02c\"\n    )) +\n    labs(\n        title = \"NBA Total Attendance (1990-2025): COVID-19 Disruption and Recovery\",\n        subtitle = \"90% collapse in 2020-21 followed by gradual recovery\",\n        x = \"Season\",\n        y = \"Total Attendance (Millions)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(ts_attendance, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Total Attendance\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_attendance &lt;- ggAcf(ts_attendance, lag.max = 15) +\n    labs(title = \"ACF of Total Attendance\") +\n    theme_minimal()\n\npacf_attendance &lt;- ggPacf(ts_attendance, lag.max = 15) +\n    labs(title = \"PACF of Total Attendance\") +\n    theme_minimal()\n\nacf_attendance / pacf_attendance\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_attendance, series = \"Original\") +\n    autolayer(ma_attendance_3, series = \"MA(3)\") +\n    autolayer(ma_attendance_5, series = \"MA(5)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\")\n    ) +\n    labs(\n        title = \"Attendance: Moving Average Smoothing (COVID Shock Visible)\",\n        subtitle = \"Smoothing cannot remove the dramatic 2020-21 disruption\",\n        y = \"Total Attendance (millions)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nAttendance provides the counterpoint: a stable pre-COVID plateau around ~21–22 million through 2019, a 2020–21 collapse during the bubble/limited-capacity seasons, and a partial recovery that remains below the pre-pandemic ceiling. The sharp, short-window discontinuity is a uncounted for shock rather than a new equilibrium. Even with 3–5 year moving averages, the COVID impact is too large to smooth away",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#sports-betting-stocks",
    "href": "eda.html#sports-betting-stocks",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "Sports Betting Stocks",
    "text": "Sports Betting Stocks\nThe NBA’s analytics revolution had profound effects beyond the court. As basketball became more quantifiable and predictable, it enabled a parallel transformation in sports betting. Companies like DraftKings and Penn Entertainment built businesses on the data infrastructure that analytics created. We briefly examine weekly stock prices for major betting operators to illustrate how this financial dimension connects to the on-court changes documented above.\n\nComparative OverviewDKNG: Time SeriesDKNG: DecompositionDKNG: Moving AveragesDKNG: ACF & PACFPENN: Time SeriesPENN: DecompositionPENN: Moving AveragesPENN: ACF & PACF\n\n\n\n\nCode\ncat(\"DKNG:\", nrow(dkng), \"days |\", min(dkng$Date), \"to\", max(dkng$Date), \"\\n\")\n\n\nDKNG: 1181 days | 18375 to 20088 \n\n\nCode\ncat(\"PENN:\", nrow(penn), \"days |\", min(penn$Date), \"to\", max(penn$Date), \"\\n\")\n\n\nPENN: 1258 days | 18263 to 20088 \n\n\nCode\ncat(\"MGM:\", nrow(mgm), \"days |\", min(mgm$Date), \"to\", max(mgm$Date), \"\\n\")\n\n\nMGM: 1478 days | 18263 to 20409 \n\n\nCode\ncat(\"CZR:\", nrow(czr), \"days |\", min(czr$Date), \"to\", max(czr$Date), \"\\n\")\n\n\nCZR: 1478 days | 18263 to 20409 \n\n\n\n\nCode\nautoplot(ts_dkng / as.numeric(ts_dkng)[1] * 100, series = \"DKNG\") +\n    autolayer(ts_penn / as.numeric(ts_penn)[1] * 100, series = \"PENN\") +\n    autolayer(ts_mgm / as.numeric(ts_mgm)[1] * 100, series = \"MGM\") +\n    autolayer(ts_czr / as.numeric(ts_czr)[1] * 100, series = \"CZR\") +\n    scale_color_manual(values = c(\"DKNG\" = \"#006bb6\", \"PENN\" = \"#f58426\", \"MGM\" = \"#00a94f\", \"CZR\" = \"#c8102e\")) +\n    labs(\n        title = \"Sports Betting Stocks: Normalized Performance (2020-2024)\",\n        subtitle = \"Indexed to 100 at each stock's start date | Boom-bust-stabilization pattern\",\n        y = \"Normalized Price (Start = 100)\", x = \"Year\", color = \"Stock\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\"), legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_dkng) +\n    annotate(\"rect\", xmin = 2021, xmax = 2021.5, ymin = 0, ymax = 70, alpha = 0.1, fill = \"orange\") +\n    annotate(\"text\", x = 2021.25, y = 65, label = \"Peak Boom\", color = \"orange\", fontface = \"bold\", size = 3) +\n    labs(\n        title = \"DraftKings (DKNG) Weekly Stock Price (2020-2024)\",\n        subtitle = \"IPO boom during COVID → correction → stabilization\",\n        x = \"Year\", y = \"Avg Weekly Adj Close ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(decomp_dkng) +\n    labs(title = \"DKNG Stock: Seasonal Decomposition (Multiplicative Model)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_dkng, series = \"Original\") +\n    autolayer(ma_dkng_4, series = \"MA(4 weeks)\") +\n    autolayer(ma_dkng_13, series = \"MA(13 weeks)\") +\n    autolayer(ma_dkng_52, series = \"MA(52 weeks)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(4 weeks)\" = \"#006bb6\",\n            \"MA(13 weeks)\" = \"#f58426\",\n            \"MA(52 weeks)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(4 weeks)\", \"MA(13 weeks)\", \"MA(52 weeks)\")\n    ) +\n    labs(\n        title = \"DKNG Stock: Moving Average Smoothing Comparison\",\n        subtitle = \"Different windows reveal trading cycles vs long-term trends\",\n        y = \"Stock Price ($)\",\n        x = \"Year\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_dkng &lt;- ggAcf(ts_dkng, lag.max = 52) +\n    labs(title = \"ACF of DKNG Weekly Stock Price\") +\n    theme_minimal()\n\npacf_dkng &lt;- ggPacf(ts_dkng, lag.max = 52) +\n    labs(title = \"PACF of DKNG Weekly Stock Price\") +\n    theme_minimal()\n\nacf_dkng / pacf_dkng\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_penn) +\n    annotate(\"rect\", xmin = 2020, xmax = 2021.5, ymin = 0, ymax = 140, alpha = 0.1, fill = \"red\") +\n    annotate(\"text\", x = 2020, y = 130, label = \"Covid-19\", color = \"red\", fontface = \"bold\", size = 3) +\n    labs(\n        title = \"Penn Entertainment (PENN) Weekly Stock Price (2020-2024)\",\n        x = \"Year\", y = \"Avg Weekly Adj Close ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(decomp_penn) +\n    labs(title = \"PENN Stock: Seasonal Decomposition (Multiplicative Model)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_penn, series = \"Original\") +\n    autolayer(ma_penn_4, series = \"MA(4 weeks)\") +\n    autolayer(ma_penn_13, series = \"MA(13 weeks)\") +\n    autolayer(ma_penn_52, series = \"MA(52 weeks)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(4 weeks)\" = \"#006bb6\",\n            \"MA(13 weeks)\" = \"#f58426\",\n            \"MA(52 weeks)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(4 weeks)\", \"MA(13 weeks)\", \"MA(52 weeks)\")\n    ) +\n    labs(\n        title = \"PENN Stock: Moving Average Smoothing Comparison\",\n        subtitle = \"Even annual smoothing cannot hide the structural collapse\",\n        y = \"Stock Price ($)\", x = \"Year\", color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_penn &lt;- ggAcf(ts_penn, lag.max = 52) +\n    labs(title = \"ACF of PENN Weekly Stock Price\") +\n    theme_minimal()\n\npacf_penn &lt;- ggPacf(ts_penn, lag.max = 52) +\n    labs(title = \"PACF of PENN Weekly Stock Price\") +\n    theme_minimal()\n\nacf_penn / pacf_penn\n\n\n\n\n\n\n\n\n\n\n\n\nBecause annual NBA series are effectively non-seasonal, I include weekly sports-betting equities to demonstrate seasonality and multiplicative decomposition. DraftKings (DKNG), Penn (PENN), MGM, and Caesars (CZR) all show pandemic-era boom-bust dynamics on weekly data. Prices are non-stationary in levels, stationary in differences, and has volatility that scales with price; implying a multiplicative model is necessary for decomposition. DKNG exhibits a large run-up, correction, and stabilization while PENN shows a sharper hype-driven spike and deeper collapse.\nPulling the findings together: ORtg, Pace, 3PAr, and Attendance are all non-stationary in levels and become stationary after first differences (d = 1). Therefore, additive decomposition is appropriate for the NBA metrics , while multiplicative decomposition fits the weekly equities. Short and medium moving-average windows clarify regime shifts: the 2012 analytics inflection in ORtg/3PAr, the mid-2000s trough and rebound in Pace, and the COVID intervention in Attendance.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "data_viz.html",
    "href": "data_viz.html",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "",
    "text": "This analysis tells the story of the most dramatic strategic transformation in NBA history, particularly the shift from mid-range-heavy “iso-ball” to the analytics-optimized “Moreyball” offense that dominates today. Through the visualizations below, we can see how data-driven decision making fundamentally reshaped basketball strategy over 45 years.\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(magick)\nlibrary(viridis)\nlibrary(patchwork)\nlibrary(cowplot)\n\ntheme_set(theme_minimal(base_size = 12))",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#the-analytics-revolution-rise-of-the-three-pointer-1980-2025",
    "href": "data_viz.html#the-analytics-revolution-rise-of-the-three-pointer-1980-2025",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "1. The Analytics Revolution: Rise of the Three-Pointer (1980-2025)",
    "text": "1. The Analytics Revolution: Rise of the Three-Pointer (1980-2025)\n\n\nCode\nlibrary(stringr)\n\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n\n    df &lt;- read_csv(file, show_col_types = FALSE)\n\n    df$Season &lt;- season_year\n    return(df)\n})\n\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        .groups = \"drop\"\n    )\n\nleague_avg &lt;- league_avg %&gt;%\n    mutate(\n        Era = case_when(\n            Season &lt; 2012 ~ \"Pre-Analytics Era\",\n            Season &gt;= 2012 & Season &lt; 2020 ~ \"Analytics Era\",\n            Season &gt;= 2020 ~ \"Post-COVID Era\"\n        )\n    )\n\nfig_3par &lt;- plot_ly(league_avg,\n    x = ~Season, y = ~`3PAr`,\n    color = ~Era,\n    colors = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    ),\n    type = \"scatter\", mode = \"lines+markers\",\n    marker = list(size = 6),\n    line = list(width = 3),\n    hovertemplate = paste(\n        \"&lt;b&gt;Season:&lt;/b&gt; %{x}&lt;br&gt;\",\n        \"&lt;b&gt;3PAr:&lt;/b&gt; %{y:.0%}&lt;br&gt;\",\n        \"&lt;extra&gt;&lt;/extra&gt;\"\n    )\n) %&gt;%\n    layout(\n        title = list(\n            text = \"The Analytics Revolution: 3-Point Attempt Rate (1980-2025)\",\n            font = list(size = 15, weight = \"bold\")\n        ),\n        xaxis = list(title = \"Season\"),\n        yaxis = list(title = \"3-Point Attempt Rate (3PA / FGA)\", tickformat = \".0%\"),\n        hovermode = \"closest\",\n        template = \"plotly_white\",\n        annotations = list(\n            list(\n                x = 2012, y = 0.44, text = \"Analytics Era Begins\",\n                showarrow = FALSE,\n                font = list(size = 8, color = \"#f58426\", weight = \"bold\"),\n                xanchor = \"left\", xshift = 5\n            )\n        ),\n        shapes = list(\n            list(\n                type = \"line\", x0 = 2012, x1 = 2012, y0 = 0, y1 = 1,\n                line = list(color = \"#f58426\", width = 2, dash = \"dash\"),\n                yref = \"paper\"\n            )\n        )\n    )\n\nfig_3par\n\n\n\n\n\n\nFor three decades, from 1980 to 2011, NBA teams treated the three-pointer as a supplementary weapon rather than a foundational strategy, with attempt rates hovering consistently between 20% and 28%. Between 1995 and 1997, the rate peaked at 21% due to the league temporarily shortening the three-point line. However, from 1997 to 1998, we see a clear decline in attempts as the league reverted to the original distance. During this era, the mid-range jumper, the signature shot of basketball mastery taught in gyms from youth leagues to the professional ranks, remained dominant. But in 2012, Houston Rockets GM Daryl Morey’s analytics department did the math and exposed a harsh truth: mid-range shots, averaging roughly 0.8 points per attempt, were the least efficient in basketball, while three-pointers yielded significantly higher returns. Observing the visualization, we can clearly identify a structural break around 2012, as three-point attempt rates surge from roughly 28% to over 42% by 2025. Yet this shift raises a crucial question: If teams started shooing more threes, did it actually make them better or just different?",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#efficiency-evolution-four-decades-of-offensive-rating-pace-and-shooting",
    "href": "data_viz.html#efficiency-evolution-four-decades-of-offensive-rating-pace-and-shooting",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "2. Efficiency Evolution: Four Decades of Offensive Rating, Pace, and Shooting",
    "text": "2. Efficiency Evolution: Four Decades of Offensive Rating, Pace, and Shooting\n\n\nCode\nefficiency_long &lt;- league_avg %&gt;%\n    select(Season, ORtg, Pace, `TS%`, `eFG%`, Era) %&gt;%\n    pivot_longer(\n        cols = c(ORtg, Pace, `TS%`, `eFG%`),\n        names_to = \"Metric\",\n        values_to = \"Value\"\n    )\n\nefficiency_facet &lt;- ggplot(efficiency_long, aes(x = Season, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"loess\", se = TRUE, alpha = 0.2, color = \"black\", linetype = \"dashed\") +\n    facet_wrap(~Metric,\n        scales = \"free_y\", ncol = 2,\n        labeller = labeller(Metric = c(\n            \"ORtg\" = \"Offensive Rating (pts per 100 poss)\",\n            \"Pace\" = \"Pace (possessions per 48 min)\",\n            \"TS%\" = \"True Shooting %\",\n            \"eFG%\" = \"Effective FG%\"\n        ))\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Efficiency Metrics: 45-Year Evolution (1980-2025)\",\n        subtitle = \"Offensive rating climbed steadily; pace declined then rebounded; shooting efficiency surged post-2012\",\n        x = \"Season\",\n        y = \"Metric Value\",\n        color = \"Era\",\n        caption = \"Data: Basketball Reference | Black dashed line: LOESS smoothing\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 16),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        strip.text = element_text(face = \"bold\", size = 11),\n        legend.position = \"bottom\",\n        panel.grid.minor = element_blank()\n    )\n\nefficiency_facet\n\n\n\n\n\n\n\n\n\nObserving the visualization, we can see that attempting more three pointers did, in fact, make teams measurably better. The chart tells a story spanning 45 years of performance gains driven by strategic optimization. Offensive Rating rose by roughly 11%, from 104 in 1980 to 115 in 2025, with the sharpest improvements occurring after 2012, precisely when three point attempt rates began to surge. We also see True Shooting Percentage climb from 53% to 58%, reinforcing the conclusion that teams became more efficient scorers by optimizing the quality of their shots. The Pace metric follows a U-shaped trajectory, reflecting the evolution of play styles over time. It went from the fast, run and gun tempo of the 1980s, to the slowed isolation heavy 2000s, and finally rebounding post 2012 as teams embraced a more controlled yet efficient rhythm. Lastly, the rise in Effective Field Goal Percentage suggests that these improvements weren’t merely the result of drawing more fouls. Teams didn’t shoot more threes by accident; they made a deliberate, data driven decision to sacrifice mid range shots in exchange for more threes and attempts at the rim.",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#death-of-the-midrange-shot-zone-trends-2004-2025",
    "href": "data_viz.html#death-of-the-midrange-shot-zone-trends-2004-2025",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "3. Death of the Midrange: Shot Zone Trends (2004-2025)",
    "text": "3. Death of the Midrange: Shot Zone Trends (2004-2025)\n\n\nCode\nshot_files &lt;- list.files(\"data/shot_location\", pattern = \"NBA_.*_Shots.csv\", full.names = TRUE)\n\nextract_season &lt;- function(filename) {\n    year_str &lt;- str_extract(basename(filename), \"\\\\d{4}\")\n    return(as.numeric(year_str))\n}\n\nshot_data_sample &lt;- map_df(shot_files, function(file) {\n    season_year &lt;- extract_season(file)\n\n    df &lt;- read_csv(file, show_col_types = FALSE, n_max = 50000)\n    df$Season &lt;- season_year\n    return(df)\n})\n\nzone_distribution &lt;- shot_data_sample %&gt;%\n    filter(!is.na(BASIC_ZONE)) %&gt;%\n    group_by(Season, BASIC_ZONE) %&gt;%\n    summarise(\n        Shot_Count = n(),\n        .groups = \"drop\"\n    ) %&gt;%\n    group_by(Season) %&gt;%\n    mutate(\n        Shot_Percentage = Shot_Count / sum(Shot_Count) * 100\n    ) %&gt;%\n    ungroup()\n\nkey_zones &lt;- c(\n    \"Mid-Range\", \"Restricted Area\", \"Above the Break 3\",\n    \"Left Corner 3\", \"Right Corner 3\"\n)\n\nzone_trends &lt;- zone_distribution %&gt;%\n    filter(BASIC_ZONE %in% key_zones) %&gt;%\n    mutate(\n        Zone_Category = case_when(\n            BASIC_ZONE == \"Mid-Range\" ~ \"Mid-Range (≈8–22 ft)\",\n            BASIC_ZONE == \"Restricted Area\" ~ \"At Rim\",\n            BASIC_ZONE %in% c(\"Left Corner 3\", \"Right Corner 3\") ~ \"Corner 3s\",\n            BASIC_ZONE == \"Above the Break 3\" ~ \"Non-Corner 3s (Arc)\",\n            TRUE ~ BASIC_ZONE\n        )\n    ) %&gt;%\n    group_by(Season, Zone_Category) %&gt;%\n    summarise(Shot_Percentage = sum(Shot_Percentage), .groups = \"drop\")\n\nzone_trends &lt;- zone_trends %&gt;%\n    mutate(\n        tooltip_text = paste0(\n            \"Season: \", Season, \"\\n\",\n            \"Zone: \", Zone_Category, \"\\n\",\n            \"Percentage: \", round(Shot_Percentage, 0), \"%\"\n        )\n    )\n\nrect_data &lt;- data.frame(\n    xmin = 2012, xmax = 2015,\n    ymin = 0, ymax = 45\n)\n\nmidrange_line_plot &lt;- ggplot(zone_trends, aes(\n    x = Season, y = Shot_Percentage,\n    color = Zone_Category,\n    linetype = Zone_Category,\n    text = tooltip_text\n)) +\n    geom_rect(\n        data = rect_data, inherit.aes = FALSE,\n        aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),\n        fill = \"#f58426\", alpha = 0.1\n    ) +\n    geom_line(size = 1.5) +\n    geom_point(size = 2.5) +\n    annotate(\"text\",\n        x = 2013.5, y = 42, label = \"Analytics\\nRevolution\", fontface = \"bold\", color = \"#f58426\"\n    ) +\n    scale_color_manual(values = c(\n        \"Mid-Range (≈8–22 ft)\" = \"#bec0c2\",\n        \"At Rim\" = \"#006bb6\",\n        \"Corner 3s\" = \"#f58426\",\n        \"Non-Corner 3s (Arc)\" = \"#000000\"\n    )) +\n    scale_linetype_manual(values = c(\n        \"Mid-Range (≈8–22 ft)\" = \"solid\",\n        \"At Rim\"               = \"solid\",\n        \"Corner 3s\"            = \"dashed\",\n        \"Non-Corner 3s (Arc)\"  = \"dotted\"\n    )) +\n    labs(\n        title = \"The Death of the Midrange: Shot Zone Trends (2004–2025)\",\n        subtitle = \"Mid-range declined while arc and corner 3s surged; at-rim remained relatively stable\",\n        x = \"Season\", y = \"Percentage of Total Shots (%)\",\n        color = \"Shot Zone\", linetype = \"Shot Zone\"\n    ) +\n    theme_minimal(base_size = 10) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 13),\n        plot.subtitle = element_text(size = 9, color = \"gray40\"),\n        axis.title = element_text(size = 9),\n        axis.text = element_text(size = 8),\n        legend.position = \"bottom\",\n        legend.title = element_text(face = \"bold\", size = 9),\n        legend.text = element_text(size = 8),\n        panel.grid.minor = element_blank()\n    ) +\n    scale_y_continuous(limits = c(0, 45), labels = function(x) paste0(x, \"%\"))\n\np &lt;- ggplotly(midrange_line_plot, tooltip = \"text\")\n\np %&gt;% style(mode = \"lines+markers\")\n\n\n\n\n\n\nWe can clearly see the trade-off in the visualization above. Mid-range shots, which accounted for about 35% of all attempts in 2004, collapsed to just 13% by 2025. Meanwhile, corner threes doubled, and above-the-arc threes surged from 13% to 34%. Shots at the rim remained relatively stable throughout this period. Around the 2015–2016 season, three-point attempts beyond the arc surpassed mid-range shots for the first time in NBA history. This shift reflects teams’ evolving approach: attack the rim for high-percentage looks, draw fouls or create putback opportunities, or shoot threes for higher expected value",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#court-shot-charts-evolution-of-shot-selection-2004-2024-w.-focus-on-boston-celtics",
    "href": "data_viz.html#court-shot-charts-evolution-of-shot-selection-2004-2024-w.-focus-on-boston-celtics",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "4. Court Shot Charts: Evolution of Shot Selection (2004-2024) w. Focus on Boston Celtics",
    "text": "4. Court Shot Charts: Evolution of Shot Selection (2004-2024) w. Focus on Boston Celtics\n\n\nCode\nsource(\"NBA_shots_tutorial.R\")\n\nshots_2004 &lt;- shot_data_sample %&gt;%\n    filter(Season == 2004, !is.na(LOC_X), !is.na(LOC_Y), LOC_Y &lt;= 47)\n\nzone_check_2004 &lt;- shots_2004 %&gt;%\n    filter(!is.na(BASIC_ZONE)) %&gt;%\n    group_by(BASIC_ZONE) %&gt;%\n    summarise(Count = n(), .groups = \"drop\") %&gt;%\n    mutate(Percentage = Count / sum(Count) * 100) %&gt;%\n    arrange(desc(Count))\n\nyears_to_plot &lt;- c(2004, 2008, 2012, 2016, 2019, 2024)\n\ncreate_court_for_year &lt;- function(year) {\n    shots_year &lt;- shot_data_sample %&gt;%\n        filter(Season == year, !is.na(LOC_X), !is.na(LOC_Y), LOC_Y &lt;= 47, !is.na(BASIC_ZONE))\n\n    shots_year &lt;- shots_year %&gt;%\n        mutate(\n            Zone_Category = case_when(\n                BASIC_ZONE == \"Mid-Range\" ~ \"Mid-Range\",\n                BASIC_ZONE == \"Restricted Area\" ~ \"At Rim\",\n                BASIC_ZONE %in% c(\"Left Corner 3\", \"Right Corner 3\") ~ \"Corner 3s\",\n                BASIC_ZONE == \"Above the Break 3\" ~ \"Above Arc 3s\",\n                TRUE ~ \"Other\"\n            )\n        )\n\n    if (nrow(shots_year) &gt; 3000) {\n        set.seed(42) # For reproducibility\n        shots_year &lt;- shots_year %&gt;% sample_n(3000)\n    }\n\n    subtitle &lt;- case_when(\n        year == 2004 ~ \"Pre-Analytics\",\n        year == 2008 ~ \"Early Transition\",\n        year == 2012 ~ \"Moreyball Begins\",\n        year == 2016 ~ \"Warriors Dynasty\",\n        year == 2019 ~ \"Pre-COVID\",\n        year == 2024 ~ \"Modern Era\",\n        TRUE ~ \"\"\n    )\n\n    plot_court(court_themes$dark, use_short_three = FALSE) +\n        geom_point(\n            data = shots_year,\n            aes(x = LOC_X, y = LOC_Y, color = Zone_Category),\n            size = 1.2,\n            alpha = 0.6\n        ) +\n        scale_color_manual(\n            values = c(\n                \"Mid-Range\" = \"#d62728\",\n                \"At Rim\" = \"#1f77b4\",\n                \"Corner 3s\" = \"#ff7f0e\",\n                \"Above Arc 3s\" = \"#2ca02c\",\n                \"Other\" = \"gray70\"\n            ),\n            name = \"Zone\"\n        ) +\n        labs(\n            title = as.character(year),\n            subtitle = subtitle\n        ) +\n        theme(\n            plot.title = element_text(hjust = 0.5, size = 18, face = \"bold\", color = \"white\"),\n            plot.subtitle = element_text(hjust = 0.5, size = 13, color = \"gray80\"),\n            plot.background = element_rect(fill = \"#000004\", color = \"gray30\", size = 1),\n            panel.background = element_rect(fill = \"#000004\"),\n            legend.position = \"right\",\n            legend.background = element_rect(fill = \"#000004\"),\n            legend.title = element_text(size = 11, face = \"bold\", color = \"white\"),\n            legend.text = element_text(size = 10, color = \"white\"),\n            legend.key = element_rect(fill = \"#000004\"),\n            legend.key.size = unit(0.6, \"cm\")\n        )\n}\n\ncourt_plots &lt;- lapply(years_to_plot, create_court_for_year)\n\n\n\n2004 - Pre-Analytics2008 - Early Transition2012 - Moreyball Begins2016 - Warriors Dynasty2019 - Pre-COVID2024 - Modern Era\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nteams_2024 &lt;- shot_data_sample %&gt;%\n    filter(Season == 2024, !is.na(TEAM_NAME)) %&gt;%\n    group_by(TEAM_NAME) %&gt;%\n    summarise(Count = n(), .groups = \"drop\") %&gt;%\n    arrange(desc(Count))\n\ncreate_celtics_court &lt;- function(year) {\n    celtics_shots &lt;- shot_data_sample %&gt;%\n        filter(\n            Season == year,\n            TEAM_NAME == \"Boston Celtics\",\n            !is.na(LOC_X), !is.na(LOC_Y),\n            LOC_Y &lt;= 47\n        )\n\n    subtitle &lt;- case_when(\n        year == 2004 ~ \"Big 3 Era Begins\",\n        year == 2008 ~ \"Championship Season\",\n        year == 2012 ~ \"Late Big 3 Era\",\n        year == 2016 ~ \"Rebuilding Year\",\n        year == 2019 ~ \"Tatum/Brown Era\",\n        year == 2025 ~ \"Modern Celtics (2024)\",\n        TRUE ~ \"\"\n    )\n\n    plot_court(court_themes$dark, use_short_three = FALSE) +\n        geom_point(\n            data = celtics_shots,\n            aes(x = LOC_X, y = LOC_Y, color = SHOT_MADE, fill = SHOT_MADE),\n            size = 1.5,\n            shape = 21,\n            stroke = 0.5,\n            alpha = 0.6\n        ) +\n        scale_color_manual(\n            values = c(\"TRUE\" = \"green4\", \"FALSE\" = \"red3\"),\n            labels = c(\"TRUE\" = \"Made\", \"FALSE\" = \"Missed\")\n        ) +\n        scale_fill_manual(\n            values = c(\"TRUE\" = \"green2\", \"FALSE\" = \"red2\"),\n            labels = c(\"TRUE\" = \"Made\", \"FALSE\" = \"Missed\")\n        ) +\n        labs(\n            title = paste(\"Boston Celtics\"),\n            subtitle = subtitle\n        ) +\n        theme(\n            plot.title = element_text(hjust = 0.5, size = 18, face = \"bold\", color = \"white\"),\n            plot.subtitle = element_text(hjust = 0.5, size = 13, color = \"gray80\"),\n            plot.background = element_rect(fill = \"#000004\", color = \"gray30\", size = 1),\n            panel.background = element_rect(fill = \"#000004\"),\n            legend.position = \"right\",\n            legend.background = element_rect(fill = \"#000004\"),\n            legend.title = element_blank(),\n            legend.text = element_text(size = 10, color = \"white\", face = \"bold\"),\n            legend.key = element_rect(fill = \"#000004\"),\n            legend.key.size = unit(0.6, \"cm\")\n        )\n}\n\nceltics_years &lt;- c(2004, 2008, 2012, 2016, 2019, 2025)\nceltics_plots &lt;- lapply(celtics_years, create_celtics_court)\n\n\n\n200420082012201620192024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the first visualization, we can see that in the pre-analytics era there was a heavy concentration of mid-range shots. By 2012, however, we begin to see the influence of “Moreyball” reshaping the league, as shot distributions tighten around above-the-arc three-pointers. As we move into the modern era, mid-range attempts become increasingly sparse, while above-the-arc threes grow more frequent and form even tighter clusters. The outliers in the visualization represent the greatest shooter of all time, Stephen Curry, whose style helped redefine offensive strategy. This transformation is further illustrated by the Boston Celtics: in 2024, they recorded the highest volume of above-the-arc three-point attempts in team history. Their strategic embrace of analytics and shot optimization directly contributed to their success; culminating in their NBA Finals victory.",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#the-covid-19-disruption-attendance-collapse-and-sports-betting-volatility",
    "href": "data_viz.html#the-covid-19-disruption-attendance-collapse-and-sports-betting-volatility",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "5. The COVID-19 Disruption: Attendance Collapse and Sports Betting Volatility",
    "text": "5. The COVID-19 Disruption: Attendance Collapse and Sports Betting Volatility\n\n\nCode\nattendance_data &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        Total_Attendance = sum(`Unnamed: 29_level_0_Attend.`, na.rm = TRUE),\n        Avg_Attendance = mean(`Unnamed: 30_level_0_Attend./G`, na.rm = TRUE),\n        .groups = \"drop\"\n    ) %&gt;%\n    filter(Season &gt;= 2000)\n\ndkng &lt;- read_csv(\"data/financial/DKNG_daily.csv\", show_col_types = FALSE)\n\ndkng &lt;- dkng %&gt;%\n    mutate(\n        Date = as.Date(Date),\n        Year = year(Date)\n    )\n\ndkng_yearly &lt;- dkng %&gt;%\n    group_by(Year) %&gt;%\n    summarise(\n        Avg_Close = mean(`Adj Close`, na.rm = TRUE),\n        Volatility = sd(Returns, na.rm = TRUE) * sqrt(252),\n        .groups = \"drop\"\n    )\nattendance_plot &lt;- ggplot(attendance_data, aes(x = Season, y = Total_Attendance / 1e6)) +\n    geom_line(color = \"#006bb6\", size = 1.5) +\n    geom_point(color = \"#006bb6\", size = 3) +\n    annotate(\"rect\",\n        xmin = 2020, xmax = 2021, ymin = 0, ymax = 25,\n        alpha = 0.2, fill = \"#f58426\"\n    ) +\n    annotate(\"text\",\n        x = 2020.5, y = 24,\n        label = \"COVID-19\",\n        size = 4, fontface = \"bold\", color = \"#f58426\"\n    ) +\n    labs(\n        title = \"NBA Attendance Collapse\",\n        x = \"Season\",\n        y = \"Total Attendance (Millions)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        panel.grid.minor = element_blank()\n    ) +\n    scale_y_continuous(labels = scales::comma, limits = c(0, 25))\n\ndkng_plot &lt;- ggplot(dkng, aes(x = Date, y = `Adj Close`)) +\n    geom_line(color = \"#f58426\", size = 0.8) +\n    annotate(\"text\",\n        x = as.Date(\"2020-04-23\"), y = 16,\n        label = \"DKNG IPO\",\n        size = 2.5, fontface = \"bold\", color = \"#f58426\", vjust = 1\n    ) +\n    labs(\n        title = \"DraftKings (DKNG) Stock Price (Daily)\",\n        x = \"Date\",\n        y = \"Adj Close Price ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        panel.grid.minor = element_blank()\n    )\n\ncombined_plot &lt;- attendance_plot | dkng_plot\n\ncombined_plot + plot_annotation(\n    title = \"COVID-19 Impact: Attendance Collapse vs Sports Betting Boom\",\n    theme = theme(\n        plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\")\n    )\n)\n\n\n\n\n\n\n\n\n\nMarch 2020 presented basketball with an unprecedented event. NBA attendance collapsed by 90% virtually overnight as the season was suspended following Rudy Gobert’s positive COVID-19 test. This was followed by the Orlando bubble season with zero fans, and then the 2020–21 campaign with limited capacity. The league’s normal rhythms and fan energy were completely disrupted. However, while the NBA paused, online sports betting exploded. DraftKings went public in April 2020, and its stock price surged as online betting became legalized across more states. If anything, the pandemic accelerated, rather than slowed, the connection between basketball and analytics, as betting markets quickly became the primary way many fans engaged with the sport.",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "financialTS_model.html",
    "href": "financialTS_model.html",
    "title": "Financial Time Series Models",
    "section": "",
    "text": "Having established the relationship between NBA performance and betting markets, we now shift our focus to the financial behavior of sports betting stocks themselves. In this chapter, these firms are treated as an independent financial ecosystem shaped by sports events, regulatory changes, and broader market forces.\nFinancial returns exhibit volatility clustering, where periods of high or low volatility tend to persist. Because ARIMA models assume constant variance, they are not well suited for data with this property. GARCH models address this limitation by explicitly capturing time-varying volatility. The key insight is that while returns may be unpredictable due to market efficiency, volatility often follows predictable patterns that are essential for risk management, options pricing, and portfolio optimization.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(imputeTS)\nlibrary(dplyr)\nlibrary(reshape2)\nlibrary(gridExtra)\nlibrary(zoo)\nlibrary(kableExtra)\nlibrary(vars)\nlibrary(patchwork)\nlibrary(moments)\nlibrary(fGarch)\n\ntheme_set(theme_minimal(base_size = 12))\n\n\n\n\nCode\n# Load all stock data\ndkng &lt;- read_csv(\"data/financial/DKNG_daily.csv\", show_col_types = FALSE)\ncolnames(dkng) &lt;- gsub(\"Adj Close\", \"Adj_Close\", colnames(dkng))\ndkng &lt;- dkng %&gt;%\n    mutate(Date = as.Date(Date)) %&gt;%\n    arrange(Date)\n\npenn &lt;- read_csv(\"data/financial/PENN_daily.csv\", show_col_types = FALSE)\ncolnames(penn) &lt;- gsub(\"Adj Close\", \"Adj_Close\", colnames(penn))\npenn &lt;- penn %&gt;%\n    mutate(Date = as.Date(Date)) %&gt;%\n    arrange(Date)\n\nczr &lt;- read_csv(\"data/financial/CZR_daily.csv\", show_col_types = FALSE)\ncolnames(czr) &lt;- gsub(\"Adj\\\\.Close\", \"Adj_Close\", colnames(czr))\n\nif (\"Adj_Close\" %in% colnames(czr)) {\n    czr$Adj_Close &lt;- as.numeric(czr$Adj_Close)\n}\nczr &lt;- czr %&gt;%\n    mutate(Date = as.Date(Date)) %&gt;%\n    arrange(Date)\n\nmgm &lt;- read_csv(\"data/financial/MGM_daily.csv\", show_col_types = FALSE)\ncolnames(mgm) &lt;- gsub(\"Adj\\\\.Close\", \"Adj_Close\", colnames(mgm))\n\nif (\"Adj_Close\" %in% colnames(mgm)) {\n    mgm$Adj_Close &lt;- as.numeric(mgm$Adj_Close)\n}\nmgm &lt;- mgm %&gt;%\n    mutate(Date = as.Date(Date)) %&gt;%\n    arrange(Date)\n\n\n\n\n\nFinancial time series models typically work with returns rather than prices because:\n\nStationarity: Prices are non-stationary (trending), returns are closer to stationary\nScale Independence: Returns normalize for price level (a $1 move matters more for a $10 stock than a $100 stock)\nTheoretical Foundation: Asset pricing models (CAPM, APT) are formulated in terms of returns\n\nWe calculate log returns (continuously compounded):\n\\[\nr_t = \\ln(P_t) - \\ln(P_{t-1}) = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right)\n\\]\n\n\nCode\n# Calculate log returns for all stocks\ndkng$LogReturn &lt;- c(NA, diff(log(dkng$Adj_Close)))\ndkng$Return_Pct &lt;- dkng$LogReturn * 100\ndkng$Ticker &lt;- \"DKNG\"\ndkng &lt;- dkng %&gt;% filter(!is.na(LogReturn))\n\npenn$LogReturn &lt;- c(NA, diff(log(penn$Adj_Close)))\npenn$Return_Pct &lt;- penn$LogReturn * 100\npenn$Ticker &lt;- \"PENN\"\npenn &lt;- penn %&gt;% filter(!is.na(LogReturn))\n\nczr$LogReturn &lt;- c(NA, diff(log(czr$Adj_Close)))\nczr$Return_Pct &lt;- czr$LogReturn * 100\nczr$Ticker &lt;- \"CZR\"\nczr &lt;- czr %&gt;% filter(!is.na(LogReturn))\n\nmgm$LogReturn &lt;- c(NA, diff(log(mgm$Adj_Close)))\nmgm$Return_Pct &lt;- mgm$LogReturn * 100\nmgm$Ticker &lt;- \"MGM\"\nmgm &lt;- mgm %&gt;% filter(!is.na(LogReturn))\n\n# Summary statistics for all stocks\nstocks_list &lt;- list(\n    DKNG = dkng$Return_Pct,\n    PENN = penn$Return_Pct,\n    CZR = czr$Return_Pct,\n    MGM = mgm$Return_Pct\n)\n\ncat(\"\\n=== RETURNS SUMMARY STATISTICS ===\\n\\n\")\n\n\n\n=== RETURNS SUMMARY STATISTICS ===\n\n\nCode\nfor (ticker in names(stocks_list)) {\n    returns &lt;- stocks_list[[ticker]]\n    cat(ticker, \":\\n\")\n    cat(\"  Mean:\", round(mean(returns), 4), \"%\\n\")\n    cat(\"  Std Dev:\", round(sd(returns), 4), \"%\\n\")\n    cat(\"  Min:\", round(min(returns), 4), \"%\\n\")\n    cat(\"  Max:\", round(max(returns), 4), \"%\\n\")\n    cat(\"  Skewness:\", round(moments::skewness(returns), 4), \"\\n\")\n    cat(\"  Kurtosis:\", round(moments::kurtosis(returns), 4), \"\\n\\n\")\n}\n\n\nDKNG :\n  Mean: 0.0638 %\n  Std Dev: 4.2354 %\n  Min: -32.6061 %\n  Max: 15.9306 %\n  Skewness: -0.3807 \n  Kurtosis: 7.8417 \n\nPENN :\n  Mean: -0.022 %\n  Std Dev: 4.9443 %\n  Min: -59.4142 %\n  Max: 29.8592 %\n  Skewness: -1.7046 \n  Kurtosis: 30.3981 \n\nCZR :\n  Mean: -0.0718 %\n  Std Dev: 4.5517 %\n  Min: -47.0084 %\n  Max: 36.5733 %\n  Skewness: -0.9193 \n  Kurtosis: 21.3669 \n\nMGM :\n  Mean: -0.0042 %\n  Std Dev: 3.4071 %\n  Min: -40.9684 %\n  Max: 28.6042 %\n  Skewness: -0.9679 \n  Kurtosis: 26.9113",
    "crumbs": [
      "Home",
      "Financial TS Models (ARCH/GARCH)"
    ]
  },
  {
    "objectID": "financialTS_model.html#overview",
    "href": "financialTS_model.html#overview",
    "title": "Financial Time Series Models",
    "section": "",
    "text": "Having established the relationship between NBA performance and betting markets, we now shift our focus to the financial behavior of sports betting stocks themselves. In this chapter, these firms are treated as an independent financial ecosystem shaped by sports events, regulatory changes, and broader market forces.\nFinancial returns exhibit volatility clustering, where periods of high or low volatility tend to persist. Because ARIMA models assume constant variance, they are not well suited for data with this property. GARCH models address this limitation by explicitly capturing time-varying volatility. The key insight is that while returns may be unpredictable due to market efficiency, volatility often follows predictable patterns that are essential for risk management, options pricing, and portfolio optimization.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(imputeTS)\nlibrary(dplyr)\nlibrary(reshape2)\nlibrary(gridExtra)\nlibrary(zoo)\nlibrary(kableExtra)\nlibrary(vars)\nlibrary(patchwork)\nlibrary(moments)\nlibrary(fGarch)\n\ntheme_set(theme_minimal(base_size = 12))\n\n\n\n\nCode\n# Load all stock data\ndkng &lt;- read_csv(\"data/financial/DKNG_daily.csv\", show_col_types = FALSE)\ncolnames(dkng) &lt;- gsub(\"Adj Close\", \"Adj_Close\", colnames(dkng))\ndkng &lt;- dkng %&gt;%\n    mutate(Date = as.Date(Date)) %&gt;%\n    arrange(Date)\n\npenn &lt;- read_csv(\"data/financial/PENN_daily.csv\", show_col_types = FALSE)\ncolnames(penn) &lt;- gsub(\"Adj Close\", \"Adj_Close\", colnames(penn))\npenn &lt;- penn %&gt;%\n    mutate(Date = as.Date(Date)) %&gt;%\n    arrange(Date)\n\nczr &lt;- read_csv(\"data/financial/CZR_daily.csv\", show_col_types = FALSE)\ncolnames(czr) &lt;- gsub(\"Adj\\\\.Close\", \"Adj_Close\", colnames(czr))\n\nif (\"Adj_Close\" %in% colnames(czr)) {\n    czr$Adj_Close &lt;- as.numeric(czr$Adj_Close)\n}\nczr &lt;- czr %&gt;%\n    mutate(Date = as.Date(Date)) %&gt;%\n    arrange(Date)\n\nmgm &lt;- read_csv(\"data/financial/MGM_daily.csv\", show_col_types = FALSE)\ncolnames(mgm) &lt;- gsub(\"Adj\\\\.Close\", \"Adj_Close\", colnames(mgm))\n\nif (\"Adj_Close\" %in% colnames(mgm)) {\n    mgm$Adj_Close &lt;- as.numeric(mgm$Adj_Close)\n}\nmgm &lt;- mgm %&gt;%\n    mutate(Date = as.Date(Date)) %&gt;%\n    arrange(Date)",
    "crumbs": [
      "Home",
      "Financial TS Models (ARCH/GARCH)"
    ]
  },
  {
    "objectID": "financialTS_model.html#calculate-returns",
    "href": "financialTS_model.html#calculate-returns",
    "title": "Financial Time Series Models",
    "section": "",
    "text": "Financial time series models typically work with returns rather than prices because:\n\nStationarity: Prices are non-stationary (trending), returns are closer to stationary\nScale Independence: Returns normalize for price level (a $1 move matters more for a $10 stock than a $100 stock)\nTheoretical Foundation: Asset pricing models (CAPM, APT) are formulated in terms of returns\n\nWe calculate log returns (continuously compounded):\n\\[\nr_t = \\ln(P_t) - \\ln(P_{t-1}) = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right)\n\\]\n\n\nCode\n# Calculate log returns for all stocks\ndkng$LogReturn &lt;- c(NA, diff(log(dkng$Adj_Close)))\ndkng$Return_Pct &lt;- dkng$LogReturn * 100\ndkng$Ticker &lt;- \"DKNG\"\ndkng &lt;- dkng %&gt;% filter(!is.na(LogReturn))\n\npenn$LogReturn &lt;- c(NA, diff(log(penn$Adj_Close)))\npenn$Return_Pct &lt;- penn$LogReturn * 100\npenn$Ticker &lt;- \"PENN\"\npenn &lt;- penn %&gt;% filter(!is.na(LogReturn))\n\nczr$LogReturn &lt;- c(NA, diff(log(czr$Adj_Close)))\nczr$Return_Pct &lt;- czr$LogReturn * 100\nczr$Ticker &lt;- \"CZR\"\nczr &lt;- czr %&gt;% filter(!is.na(LogReturn))\n\nmgm$LogReturn &lt;- c(NA, diff(log(mgm$Adj_Close)))\nmgm$Return_Pct &lt;- mgm$LogReturn * 100\nmgm$Ticker &lt;- \"MGM\"\nmgm &lt;- mgm %&gt;% filter(!is.na(LogReturn))\n\n# Summary statistics for all stocks\nstocks_list &lt;- list(\n    DKNG = dkng$Return_Pct,\n    PENN = penn$Return_Pct,\n    CZR = czr$Return_Pct,\n    MGM = mgm$Return_Pct\n)\n\ncat(\"\\n=== RETURNS SUMMARY STATISTICS ===\\n\\n\")\n\n\n\n=== RETURNS SUMMARY STATISTICS ===\n\n\nCode\nfor (ticker in names(stocks_list)) {\n    returns &lt;- stocks_list[[ticker]]\n    cat(ticker, \":\\n\")\n    cat(\"  Mean:\", round(mean(returns), 4), \"%\\n\")\n    cat(\"  Std Dev:\", round(sd(returns), 4), \"%\\n\")\n    cat(\"  Min:\", round(min(returns), 4), \"%\\n\")\n    cat(\"  Max:\", round(max(returns), 4), \"%\\n\")\n    cat(\"  Skewness:\", round(moments::skewness(returns), 4), \"\\n\")\n    cat(\"  Kurtosis:\", round(moments::kurtosis(returns), 4), \"\\n\\n\")\n}\n\n\nDKNG :\n  Mean: 0.0638 %\n  Std Dev: 4.2354 %\n  Min: -32.6061 %\n  Max: 15.9306 %\n  Skewness: -0.3807 \n  Kurtosis: 7.8417 \n\nPENN :\n  Mean: -0.022 %\n  Std Dev: 4.9443 %\n  Min: -59.4142 %\n  Max: 29.8592 %\n  Skewness: -1.7046 \n  Kurtosis: 30.3981 \n\nCZR :\n  Mean: -0.0718 %\n  Std Dev: 4.5517 %\n  Min: -47.0084 %\n  Max: 36.5733 %\n  Skewness: -0.9193 \n  Kurtosis: 21.3669 \n\nMGM :\n  Mean: -0.0042 %\n  Std Dev: 3.4071 %\n  Min: -40.9684 %\n  Max: 28.6042 %\n  Skewness: -0.9679 \n  Kurtosis: 26.9113",
    "crumbs": [
      "Home",
      "Financial TS Models (ARCH/GARCH)"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "The Era of Efficiency: Analytics, COVID, and the Modern NBA",
    "section": "",
    "text": "Over the last two decades, the National Basketball Association (NBA) has undergone a historic transformation in how the game is played and measured. The rise of analytics has redefined decision-making, from shot selection to player valuation, creating a league increasingly optimized for pace, spacing, and efficiency. Traditional mid-range play has given way to data-driven offenses that favor the three-point shot and fast-break opportunities1. Yet this evolution has not been linear: external shocks such as the COVID-19 pandemic and rule changes have periodically disrupted the sport’s equilibrium.\nThis project seeks to quantify and contextualize the NBA’s evolution toward efficiency; particularly how the league’s statistical DNA has shifted under the influence of analytics, and how sudden disruptions like the 2020 “bubble” season temporarily rewired its dynamics. Using time-series analysis, the study traces the interplay between pace, three-point attempt rate, and offensive efficiency to reveal both long-term structural change and short-term volatility.\n\n\nBasketball today is not merely an athletic competition but a living laboratory of applied data science. The league’s embrace of analytics mirrors a broader transformation in modern industries: decisions increasingly rest on quantitative evidence rather than intuition. The shift from mid-range isolation to high efficiency, high variance offense encapsulates how optimization, technology, and information access shape human behavior, specifically in sports.\n\n\n\nThe Big Picture\n\n\nWith the framework above, I aim to tell the story of how basketball’s offensive efficiency evolved under the dual pressures of analytics and disruption. The analysis begins with a long-run view of the game’s transformation: tracing how three-point attempts, shot selection, and pace redefined offensive output from 1980 through 2025. By examining league wide efficiency, field-goal percentages, and possessions per 48 minutes, I hope to capture how the modern NBA diverged from its slower, mid-range past into a data-optimized era of high variance offense.\nFrom there, the focus shifts to the Analytics Revolution, a period beginning around 2012 when front offices embraced quantitative models and player-tracking data. Here, patterns in shot distance, efficiency metrics, and team-level statistics reveal how strategy and roster construction began to align with mathematical optimization. These structural changes not only reshaped offensive philosophy but also institutionalized analytics as a core component of competitive advantage.\nThe next chapter explores the COVID-19 shock, a natural experiment that disrupted decades of rhythm. Empty arenas, travel restrictions, and condensed schedules created conditions to test how context, apart from talent or tactics, influences performance. Comparing attendance patterns, scoring volatility, and pace before, during, and after the pandemic provides a rare window into basketball’s psychological and environmental dimensions.\nFinally, the analysis turns toward the future. By modeling post-2023 trends using both traditional and modern time-series methods, the goal is to forecast whether the league has stabilized in a new equilibrium or continues to evolve. From evolution and optimization to disruption and re-emergence the project aims to capture how data, environment, and adaptation intertwine to define the modern game.\n\n\n\nEarly quantitative basketball research established the foundation for efficiency metrics such as effective field goal percentage (eFG%) and true shooting percentage (TS%)2. Subsequent work formalized offensive rating and pace-adjusted measures that underpin today’s analytics frameworks3. In this study, those established metrics anchor a longitudinal view of the NBA from 1980–2025, allowing us to trace how efficiency evolved and whether recent shifts reflect gradual adaptation or discrete regime changes.\nA central thread is shot-selection optimization. Spatial analyses show that perimeter-oriented offenses and rim attempts yield higher expected value than mid-range play4. Building on that evidence, we follow the league’s shot mix over time and date its key inflection points using structural-break diagnostics. Forecast comparisons around those dated transitions then indicate whether the modern shot profile has stabilized or is still moving toward further concentration in high-value zones.\nTempo and spacing economics form the second pillar. Prior work links ball movement, spacing, and faster pace to scoring efficiency in the modern game1. Rather than treating pace and efficiency as static correlates, we examine their dynamic ordering: do changes in tempo precede shifts in efficiency, or the reverse? A multivariate framework (including Granger-style causality tests and rolling correlations) lets us see how this relationship strengthened from the pre-analytics period into the analytics era, clarifying whether “playing faster” is a driver, a consequence, or part of a feedback loop with shot quality.\nThe third pillar addresses COVID-era disruptions. Empty-arena conditions in 2020 weakened traditional home-court effects and scrambled normal rhythms. We treat the bubble and capacity-limited seasons as an exogenous intervention, quantify the immediate impact on pace, efficiency, and scoring variance, and then measure persistence and recovery. By juxtaposing pre-2020 fitted behavior with realized outcomes through 2022, we can distinguish a temporary shock from a lasting structural shift.\nTogether, these components extend prior literature that established what changed (shot mix, tempo strategies, COVID effects) but seldom mapped how and when the transitions unfolded or whether they endured. Concretely, we (i) date the analytics inflection with objective break tests; (ii) test whether rising three-point volume leads or follows efficiency gains; (iii) model low-order serial dependence and feedback that cross-sectional designs omit; and (iv) isolate the pandemic’s impulse and its decay. Treating efficiency, pace, and shot selection as interconnected trajectories reveals the timing, coupling, and durability of the forces reshaping the modern NBA.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#the-big-picture",
    "href": "intro.html#the-big-picture",
    "title": "The Era of Efficiency: Analytics, COVID, and the Modern NBA",
    "section": "",
    "text": "Basketball today is not merely an athletic competition but a living laboratory of applied data science. The league’s embrace of analytics mirrors a broader transformation in modern industries: decisions increasingly rest on quantitative evidence rather than intuition. The shift from mid-range isolation to high efficiency, high variance offense encapsulates how optimization, technology, and information access shape human behavior, specifically in sports.\n\n\n\nThe Big Picture\n\n\nWith the framework above, I aim to tell the story of how basketball’s offensive efficiency evolved under the dual pressures of analytics and disruption. The analysis begins with a long-run view of the game’s transformation: tracing how three-point attempts, shot selection, and pace redefined offensive output from 1980 through 2025. By examining league wide efficiency, field-goal percentages, and possessions per 48 minutes, I hope to capture how the modern NBA diverged from its slower, mid-range past into a data-optimized era of high variance offense.\nFrom there, the focus shifts to the Analytics Revolution, a period beginning around 2012 when front offices embraced quantitative models and player-tracking data. Here, patterns in shot distance, efficiency metrics, and team-level statistics reveal how strategy and roster construction began to align with mathematical optimization. These structural changes not only reshaped offensive philosophy but also institutionalized analytics as a core component of competitive advantage.\nThe next chapter explores the COVID-19 shock, a natural experiment that disrupted decades of rhythm. Empty arenas, travel restrictions, and condensed schedules created conditions to test how context, apart from talent or tactics, influences performance. Comparing attendance patterns, scoring volatility, and pace before, during, and after the pandemic provides a rare window into basketball’s psychological and environmental dimensions.\nFinally, the analysis turns toward the future. By modeling post-2023 trends using both traditional and modern time-series methods, the goal is to forecast whether the league has stabilized in a new equilibrium or continues to evolve. From evolution and optimization to disruption and re-emergence the project aims to capture how data, environment, and adaptation intertwine to define the modern game.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#literature-review",
    "href": "intro.html#literature-review",
    "title": "The Era of Efficiency: Analytics, COVID, and the Modern NBA",
    "section": "",
    "text": "Early quantitative basketball research established the foundation for efficiency metrics such as effective field goal percentage (eFG%) and true shooting percentage (TS%)2. Subsequent work formalized offensive rating and pace-adjusted measures that underpin today’s analytics frameworks3. In this study, those established metrics anchor a longitudinal view of the NBA from 1980–2025, allowing us to trace how efficiency evolved and whether recent shifts reflect gradual adaptation or discrete regime changes.\nA central thread is shot-selection optimization. Spatial analyses show that perimeter-oriented offenses and rim attempts yield higher expected value than mid-range play4. Building on that evidence, we follow the league’s shot mix over time and date its key inflection points using structural-break diagnostics. Forecast comparisons around those dated transitions then indicate whether the modern shot profile has stabilized or is still moving toward further concentration in high-value zones.\nTempo and spacing economics form the second pillar. Prior work links ball movement, spacing, and faster pace to scoring efficiency in the modern game1. Rather than treating pace and efficiency as static correlates, we examine their dynamic ordering: do changes in tempo precede shifts in efficiency, or the reverse? A multivariate framework (including Granger-style causality tests and rolling correlations) lets us see how this relationship strengthened from the pre-analytics period into the analytics era, clarifying whether “playing faster” is a driver, a consequence, or part of a feedback loop with shot quality.\nThe third pillar addresses COVID-era disruptions. Empty-arena conditions in 2020 weakened traditional home-court effects and scrambled normal rhythms. We treat the bubble and capacity-limited seasons as an exogenous intervention, quantify the immediate impact on pace, efficiency, and scoring variance, and then measure persistence and recovery. By juxtaposing pre-2020 fitted behavior with realized outcomes through 2022, we can distinguish a temporary shock from a lasting structural shift.\nTogether, these components extend prior literature that established what changed (shot mix, tempo strategies, COVID effects) but seldom mapped how and when the transitions unfolded or whether they endured. Concretely, we (i) date the analytics inflection with objective break tests; (ii) test whether rising three-point volume leads or follows efficiency gains; (iii) model low-order serial dependence and feedback that cross-sectional designs omit; and (iv) isolate the pandemic’s impulse and its decay. Treating efficiency, pace, and shot selection as interconnected trajectories reveals the timing, coupling, and durability of the forces reshaping the modern NBA.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "ITS_analysis.html",
    "href": "ITS_analysis.html",
    "title": "Interrupted Time Series Analysis",
    "section": "",
    "text": "Overview\nThis analysis examines two major interventions that reshaped the sports betting landscape: the COVID-19 pandemic (March 2020) and New York’s online sports betting legalization (January 2022). Using Google Trends data spanning 2020-2024, we employ interrupted time series regression to quantify both immediate shocks and sustained trend changes. The ITS model\n\n\\(Y_t = \\beta_0 + \\beta_1 \\cdot Time_t + \\beta_2 \\cdot Intervention_t + \\beta_3 \\cdot Time\\_Since\\_Intervention_t + \\epsilon_t\\)\n\nisolates intervention effects from underlying trends, measuring level changes (\\(\\beta_2\\)) and slope changes (\\(\\beta_3\\)) in search interest. Google Trends provides a direct cultural proxy for public engagement, capturing how these events transformed sports betting from a niche activity during COVID’s sports shutdown to mainstream entertainment following New York’s market entry.\n\n\n\nData & Setup\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(forecast)\nlibrary(gridExtra)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 12))\n\n# Load Google Trends data\ntrends &lt;- read_csv(\"data/google_trends/sports_betting_trends.csv\", show_col_types = FALSE) %&gt;%\n    mutate(date = as.Date(date))\n\n# Create separate dataframes and add ITS variables\nintervention_date &lt;- as.Date(\"2022-01-08\")\ncovid_intervention &lt;- as.Date(\"2020-03-11\")\n\nadd_its_variables &lt;- function(df, int_date) {\n    df %&gt;%\n        mutate(\n            Post_Intervention = ifelse(date &gt;= int_date, 1, 0),\n            Time = as.numeric(date - min(date)),\n            Time_Since_Intervention = ifelse(Post_Intervention == 1,\n                as.numeric(date - int_date), 0\n            )\n        )\n}\n\nsports_betting &lt;- trends %&gt;%\n    filter(keyword == \"sports betting\") %&gt;%\n    add_its_variables(intervention_date)\ndraftkings &lt;- trends %&gt;%\n    filter(keyword == \"draftkings\") %&gt;%\n    add_its_variables(intervention_date)\nonline_betting &lt;- trends %&gt;%\n    filter(keyword == \"online betting\") %&gt;%\n    add_its_variables(intervention_date)\nnba_betting &lt;- trends %&gt;%\n    filter(keyword == \"nba betting\") %&gt;%\n    add_its_variables(intervention_date)\n\n\n\n\nNY Legalization AnalysisCOVID-19 Analysis\n\n\n\nBackground & Data\nIntervention: New York online sports betting launch (January 8, 2022) - the largest U.S. sports betting market by revenue, representing ~20% of the national market. Research Question: Did NY legalization produce a significant change in public interest in sports betting? We analyze Google search trends as a direct measure of cultural impact and consumer awareness, examining four search terms that capture different facets of betting interest from general (“sports betting”) to operator-specific (“draftkings”) to sport-specific (“nba betting”).\n\n\nVisualization & Data Preparation\n\n\nCode\n# Visualization\np1 &lt;- ggplot(sports_betting, aes(x = date, y = hits)) +\n    geom_line(color = \"#1e88e5\", linewidth = 1) +\n    geom_vline(xintercept = intervention_date, color = \"red\", linetype = \"dashed\", linewidth = 1.2) +\n    annotate(\"text\",\n        x = intervention_date, y = max(sports_betting$hits) * 0.95,\n        label = \"NY Launch\\nJan 8, 2022\", color = \"red\", size = 4, fontface = \"bold\"\n    ) +\n    labs(\n        title = \"Google Searches: 'sports betting'\",\n        subtitle = \"NY sports betting legalization intervention point marked in red\",\n        x = \"Date\", y = \"Search Interest (0-100)\"\n    ) +\n    theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(draftkings, aes(x = date, y = hits)) +\n    geom_line(color = \"#ff6f00\", linewidth = 1) +\n    geom_vline(xintercept = intervention_date, color = \"red\", linetype = \"dashed\", linewidth = 1.2) +\n    annotate(\"text\",\n        x = intervention_date, y = max(draftkings$hits) * 0.95,\n        label = \"NY Launch\\nJan 8, 2022\", color = \"red\", size = 4, fontface = \"bold\"\n    ) +\n    labs(title = \"Google Searches: 'draftkings'\", x = \"Date\", y = \"Search Interest (0-100)\") +\n    theme(plot.title = element_text(face = \"bold\"))\n\np3 &lt;- ggplot(online_betting, aes(x = date, y = hits)) +\n    geom_line(color = \"#43a047\", linewidth = 1) +\n    geom_vline(xintercept = intervention_date, color = \"red\", linetype = \"dashed\", linewidth = 1.2) +\n    annotate(\"text\",\n        x = intervention_date, y = max(online_betting$hits) * 0.95,\n        label = \"NY Launch\\nJan 8, 2022\", color = \"red\", size = 4, fontface = \"bold\"\n    ) +\n    labs(title = \"Google Searches: 'online betting'\", x = \"Date\", y = \"Search Interest (0-100)\") +\n    theme(plot.title = element_text(face = \"bold\"))\n\np4 &lt;- ggplot(nba_betting, aes(x = date, y = hits)) +\n    geom_line(color = \"#8e24aa\", linewidth = 1) +\n    geom_vline(xintercept = intervention_date, color = \"red\", linetype = \"dashed\", linewidth = 1.2) +\n    annotate(\"text\",\n        x = intervention_date, y = max(nba_betting$hits) * 0.95,\n        label = \"NY Launch\\nJan 8, 2022\", color = \"red\", size = 4, fontface = \"bold\"\n    ) +\n    labs(title = \"Google Searches: 'nba betting'\", x = \"Date\", y = \"Search Interest (0-100)\") +\n    theme(plot.title = element_text(face = \"bold\"))\n\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)\n\n\n\n\n\n\n\n\n\nCode\n# Data preparation table\nprep_table &lt;- sports_betting %&gt;%\n    filter(date &gt;= as.Date(\"2021-12-01\") & date &lt;= as.Date(\"2022-02-28\")) %&gt;%\n    select(date, hits, Time, Post_Intervention, Time_Since_Intervention) %&gt;%\n    mutate(Period = ifelse(Post_Intervention == 0, \"Pre-NY\", \"Post-NY\")) %&gt;%\n    select(date, Period, Y = hits, X_t = Time, Z_t = Post_Intervention, P_t = Time_Since_Intervention)\n\nkable(prep_table,\n    format = \"html\", digits = 1,\n    caption = \"ITS Variables for NY Legalization Analysis (Sample Period)\",\n    col.names = c(\"Date\", \"Period\", \"Y (Search Interest)\", \"X_t (Time)\", \"Z_t (NY Indicator)\", \"P_t (Time Since NY)\")\n) %&gt;%\n    kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) %&gt;%\n    row_spec(0, bold = TRUE, background = \"#2c3e50\", color = \"white\") %&gt;%\n    row_spec(which(prep_table$Z_t == 1), background = \"#e3f2fd\")\n\n\n\nITS Variables for NY Legalization Analysis (Sample Period)\n\n\nDate\nPeriod\nY (Search Interest)\nX_t (Time)\nZ_t (NY Indicator)\nP_t (Time Since NY)\n\n\n\n\n2021-12-05\nPre-NY\n35\n707\n0\n0\n\n\n2021-12-12\nPre-NY\n35\n714\n0\n0\n\n\n2021-12-19\nPre-NY\n33\n721\n0\n0\n\n\n2021-12-26\nPre-NY\n36\n728\n0\n0\n\n\n2022-01-02\nPre-NY\n43\n735\n0\n0\n\n\n2022-01-09\nPost-NY\n46\n742\n1\n1\n\n\n2022-01-16\nPost-NY\n40\n749\n1\n8\n\n\n2022-01-23\nPost-NY\n37\n756\n1\n15\n\n\n2022-01-30\nPost-NY\n35\n763\n1\n22\n\n\n2022-02-06\nPost-NY\n40\n770\n1\n29\n\n\n2022-02-13\nPost-NY\n45\n777\n1\n36\n\n\n2022-02-20\nPost-NY\n23\n784\n1\n43\n\n\n2022-02-27\nPost-NY\n24\n791\n1\n50\n\n\n\n\n\n\n\nVariables: Y = Search interest; X_t = Time (weeks from start); Z_t = NY intervention indicator (0 before, 1 after Jan 8, 2022); P_t = Weeks since NY launch (0 before intervention, increases after).\n\n\nITS Model & Results\nModel: \\(Y_t = \\beta_0 + \\beta_1 X_t + \\beta_2 Z_t + \\beta_3 P_t + \\varepsilon_t\\) where \\(\\beta_1\\) captures pre-intervention trend, \\(\\beta_2\\) measures immediate level change, and \\(\\beta_3\\) quantifies trend change after intervention.\n\n\nCode\n# Fit models for all search terms\nmodel_sb &lt;- lm(hits ~ Time + Post_Intervention + Time_Since_Intervention, data = sports_betting)\nmodel_dk &lt;- lm(hits ~ Time + Post_Intervention + Time_Since_Intervention, data = draftkings)\nmodel_ob &lt;- lm(hits ~ Time + Post_Intervention + Time_Since_Intervention, data = online_betting)\nmodel_nb &lt;- lm(hits ~ Time + Post_Intervention + Time_Since_Intervention, data = nba_betting)\n\n# Extract coefficients\nextract_results &lt;- function(model, name) {\n    coef &lt;- summary(model)$coefficients\n    data.frame(\n        Term = name,\n        Parameter = c(\"Intercept\", \"Pre-Trend (β₁)\", \"Level Change (β₂)\", \"Trend Change (β₃)\"),\n        Coefficient = coef[, 1],\n        Std_Error = coef[, 2],\n        p_value = coef[, 4]\n    )\n}\n\nresults_sb &lt;- extract_results(model_sb, \"Sports Betting\")\nresults_dk &lt;- extract_results(model_dk, \"DraftKings\")\nresults_ob &lt;- extract_results(model_ob, \"Online Betting\")\nresults_nb &lt;- extract_results(model_nb, \"NBA Betting\")\n\nall_results &lt;- rbind(results_sb, results_dk, results_ob, results_nb)\n\nkable(all_results,\n    format = \"html\", digits = 4,\n    caption = \"NY Legalization ITS Results: All Search Terms\",\n    col.names = c(\"Term\", \"Parameter\", \"Coefficient\", \"Std. Error\", \"p-value\")\n) %&gt;%\n    kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) %&gt;%\n    row_spec(0, bold = TRUE, background = \"#2c3e50\", color = \"white\") %&gt;%\n    row_spec(which(all_results$p_value &lt; 0.05), background = \"#d4edda\") %&gt;%\n    pack_rows(\"Sports Betting\", 1, 4) %&gt;%\n    pack_rows(\"DraftKings\", 5, 8) %&gt;%\n    pack_rows(\"Online Betting\", 9, 12) %&gt;%\n    pack_rows(\"NBA Betting\", 13, 16)\n\n\n\nNY Legalization ITS Results: All Search Terms\n\n\n\nTerm\nParameter\nCoefficient\nStd. Error\np-value\n\n\n\n\nSports Betting\n\n\n(Intercept)\nSports Betting\nIntercept\n14.3236\n1.9043\n0.0000\n\n\nTime\nSports Betting\nPre-Trend (β₁)\n0.0316\n0.0045\n0.0000\n\n\nPost_Intervention\nSports Betting\nLevel Change (β₂)\n-9.3448\n2.4895\n0.0002\n\n\nTime_Since_Intervention\nSports Betting\nTrend Change (β₃)\n-0.0255\n0.0051\n0.0000\n\n\nDraftKings\n\n\n(Intercept)1\nDraftKings\nIntercept\n29.9245\n3.2708\n0.0000\n\n\nTime1\nDraftKings\nPre-Trend (β₁)\n0.0370\n0.0077\n0.0000\n\n\nPost_Intervention1\nDraftKings\nLevel Change (β₂)\n-9.1855\n4.2759\n0.0326\n\n\nTime_Since_Intervention1\nDraftKings\nTrend Change (β₃)\n-0.0352\n0.0088\n0.0001\n\n\nOnline Betting\n\n\n(Intercept)2\nOnline Betting\nIntercept\n22.8907\n2.6444\n0.0000\n\n\nTime2\nOnline Betting\nPre-Trend (β₁)\n0.0351\n0.0062\n0.0000\n\n\nPost_Intervention2\nOnline Betting\nLevel Change (β₂)\n-16.1522\n3.4570\n0.0000\n\n\nTime_Since_Intervention2\nOnline Betting\nTrend Change (β₃)\n-0.0240\n0.0071\n0.0009\n\n\nNBA Betting\n\n\n(Intercept)3\nNBA Betting\nIntercept\n29.7822\n4.2777\n0.0000\n\n\nTime3\nNBA Betting\nPre-Trend (β₁)\n0.0358\n0.0101\n0.0004\n\n\nPost_Intervention3\nNBA Betting\nLevel Change (β₂)\n-1.0184\n5.5922\n0.8556\n\n\nTime_Since_Intervention3\nNBA Betting\nTrend Change (β₃)\n-0.0484\n0.0115\n0.0000\n\n\n\n\n\n\n\nCode\n# Fitted values plot\nsports_betting$Fitted &lt;- fitted(model_sb)\n\nggplot(sports_betting, aes(x = date)) +\n    geom_line(aes(y = hits), color = \"gray60\", alpha = 0.6, linewidth = 0.5) +\n    geom_line(aes(y = Fitted), color = \"#1e88e5\", linewidth = 1.2) +\n    geom_vline(xintercept = intervention_date, color = \"red\", linetype = \"dashed\", linewidth = 1.2) +\n    annotate(\"text\",\n        x = intervention_date, y = max(sports_betting$hits) * 0.95,\n        label = \"NY Launch\", color = \"red\", size = 5, fontface = \"bold\"\n    ) +\n    labs(\n        title = \"'Sports Betting' Searches: Actual vs. Fitted ITS Trend Lines\",\n        subtitle = \"Gray = Actual | Blue = Fitted (shows pre- and post-intervention trends)\",\n        x = \"Date\", y = \"Search Interest (0-100)\"\n    ) +\n    theme(plot.title = element_text(face = \"bold\", size = 15))\n\n\n\n\n\n\n\n\n\n\n\nCounterfactual & Interpretation\n\n\nCode\n# Generate predictions and counterfactuals\nsports_betting &lt;- sports_betting %&gt;%\n    mutate(\n        Predicted = predict(model_sb),\n        Counterfactual = predict(model_sb, newdata = sports_betting %&gt;% mutate(Post_Intervention = 0, Time_Since_Intervention = 0)),\n        Effect = Predicted - Counterfactual\n    )\n\n# Counterfactual visualization\nggplot(sports_betting, aes(x = date)) +\n    geom_line(aes(y = Counterfactual, linetype = \"Counterfactual (No NY)\"), color = \"#1e88e5\", linewidth = 1.2) +\n    geom_line(aes(y = Predicted, linetype = \"Predicted (With NY)\"), color = \"#d32f2f\", linewidth = 1.2) +\n    geom_point(aes(y = hits, shape = \"Actual Data\"), color = \"gray30\", size = 1, alpha = 0.5) +\n    geom_ribbon(\n        data = sports_betting %&gt;% filter(Post_Intervention == 1),\n        aes(ymin = Predicted, ymax = Counterfactual), fill = \"red\", alpha = 0.15\n    ) +\n    geom_vline(xintercept = intervention_date, color = \"red\", linetype = \"dashed\", linewidth = 1.5) +\n    annotate(\"text\",\n        x = intervention_date, y = max(sports_betting$hits) * 0.98,\n        label = \"NY Launch\", color = \"red\", size = 5, fontface = \"bold\", hjust = -0.1\n    ) +\n    scale_linetype_manual(\n        name = \"Trend Lines\",\n        values = c(\"Counterfactual (No NY)\" = \"dashed\", \"Predicted (With NY)\" = \"solid\")\n    ) +\n    scale_shape_manual(name = \"Data\", values = c(\"Actual Data\" = 16)) +\n    labs(\n        title = \"Predicted vs. Counterfactual: Impact of NY Sports Betting Legalization\",\n        subtitle = \"Red shaded area shows intervention effect (difference between what happened and what would have happened)\",\n        x = \"Date\", y = \"Search Interest (0-100)\",\n        caption = \"Blue dashed = Counterfactual (pre-NY trend continued) | Red solid = Predicted (with NY intervention)\"\n    ) +\n    theme(plot.title = element_text(face = \"bold\", size = 16), legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nCode\n# Coefficient interpretation\ncoefs &lt;- coef(model_sb)\ncoef_summary &lt;- summary(model_sb)$coefficients\n\ncat(\"=== COEFFICIENT INTERPRETATION ===\\n\\n\")\n\n\n=== COEFFICIENT INTERPRETATION ===\n\n\nCode\ncat(sprintf(\n    \"β₁ (Pre-Trend) = %.4f (p = %.4f): Search interest %s by %.4f points/week before NY launch %s\\n\\n\",\n    coefs[2], coef_summary[2, 4],\n    ifelse(coefs[2] &gt; 0, \"increased\", \"decreased\"),\n    abs(coefs[2]),\n    ifelse(coef_summary[2, 4] &lt; 0.05, \"(significant)\", \"(not significant)\")\n))\n\n\nβ₁ (Pre-Trend) = 0.0316 (p = 0.0000): Search interest increased by 0.0316 points/week before NY launch (significant)\n\n\nCode\ncat(sprintf(\n    \"β₂ (Level Change) = %.4f (p = %.4f): %s immediate %s of %.2f points when NY launched %s\\n\\n\",\n    coefs[3], coef_summary[3, 4],\n    ifelse(coef_summary[3, 4] &lt; 0.05, \"Significant\", \"No significant\"),\n    ifelse(coefs[3] &gt; 0, \"jump\", \"drop\"),\n    abs(coefs[3]),\n    ifelse(coef_summary[3, 4] &lt; 0.05, \"(intervention had immediate effect)\", \"(no immediate shock)\")\n))\n\n\nβ₂ (Level Change) = -9.3448 (p = 0.0002): Significant immediate drop of 9.34 points when NY launched (intervention had immediate effect)\n\n\nCode\ncat(sprintf(\n    \"β₃ (Trend Change) = %.4f (p = %.4f): Post-NY trend = %.4f (pre) + %.4f (change) = %.4f per week\\n\",\n    coefs[4], coef_summary[4, 4], coefs[2], coefs[4], coefs[2] + coefs[4]\n))\n\n\nβ₃ (Trend Change) = -0.0255 (p = 0.0000): Post-NY trend = 0.0316 (pre) + -0.0255 (change) = 0.0061 per week\n\n\nCode\ncat(sprintf(\n    \"   %s in growth rate after NY launch %s\\n\\n\",\n    ifelse(coefs[4] &gt; 0, \"Acceleration\", \"Deceleration\"),\n    ifelse(coef_summary[4, 4] &lt; 0.05, \"(significant sustained effect)\", \"(not significant)\")\n))\n\n\n   Deceleration in growth rate after NY launch (significant sustained effect)\n\n\nCode\n# Delayed effect\npost_ny &lt;- sports_betting %&gt;% filter(Post_Intervention == 1)\navg_effect &lt;- mean(post_ny$Effect)\ncurrent_effect &lt;- tail(sports_betting$Effect, 1)\n\ncat(sprintf(\"Average Effect (Post-NY): %.2f points\\n\", avg_effect))\n\n\nAverage Effect (Post-NY): -23.21 points\n\n\nCode\ncat(sprintf(\"Current Effect: %.2f points\\n\", current_effect))\n\n\nCurrent Effect: -37.05 points\n\n\nCode\ncat(sprintf(\n    \"\\nCounterfactual: Without NY legalization, current search interest would be %.1f instead of %.1f\\n\",\n    tail(sports_betting$Counterfactual, 1), tail(sports_betting$hits, 1)\n))\n\n\n\nCounterfactual: Without NY legalization, current search interest would be 72.0 instead of 34.0\n\n\nThe immediate effect is captured by the β₂ coefficient, which measures whether search interest jumped or declined right after January 8, 2022. A significant positive value indicates that New York’s legalization produced an instant cultural response. The counterfactual blue dashed line represents what search interest would have looked like had legalization not occurred; the gap between the blue counterfactual and the red actual line quantifies the intervention’s overall impact at any point in time.\nThe sustained effect is reflected in the β₃ coefficient, which shows whether the growth trajectory changed after legalization. Positive values signal accelerated interest as the New York betting market stabilized and expanded. Beyond this trajectory shift, the delayed effect, seen in both the average post-intervention difference and the current separation between actual and counterfactual, reveals whether the impact persisted long after the initial launch period.\n\n\n\n\nBackground & Variable Selection\nIntervention: WHO declared COVID-19 pandemic on March 11, 2020, causing immediate suspension of all major sports leagues (NBA, NHL, MLB). Variable: “Sports betting” Google searches - an ITS candidate because the intervention had a clear, direct causal pathway: no live sports = no sports betting.\n\n\nVisualization & Data Preparation\n\n\nCode\n# Prepare COVID ITS data\nsports_covid_its &lt;- sports_betting %&gt;%\n    mutate(\n        X_t = row_number(),\n        Z_t = ifelse(date &gt;= covid_intervention, 1, 0),\n        P_t = ifelse(date &gt;= covid_intervention, as.numeric(date - covid_intervention) / 7, 0),\n        Y = hits\n    )\n\n# Visualization\nsports_covid &lt;- sports_covid_its %&gt;%\n    filter(date &gt;= as.Date(\"2019-12-01\") & date &lt;= as.Date(\"2021-12-31\"))\n\nggplot(sports_covid, aes(x = date, y = Y)) +\n    geom_line(color = \"#1e88e5\", linewidth = 1.2) +\n    geom_point(color = \"#1e88e5\", size = 1.5, alpha = 0.6) +\n    geom_vline(xintercept = covid_intervention, color = \"red\", linetype = \"dashed\", linewidth = 1.5) +\n    annotate(\"text\",\n        x = covid_intervention, y = max(sports_covid$Y) * 0.95,\n        label = \"COVID-19 Pandemic\\nMarch 11, 2020\\nSports Leagues Shut Down\",\n        color = \"red\", size = 5, fontface = \"bold\", hjust = -0.1\n    ) +\n    annotate(\"rect\",\n        xmin = covid_intervention, xmax = as.Date(\"2020-07-01\"),\n        ymin = -Inf, ymax = Inf, alpha = 0.1, fill = \"red\"\n    ) +\n    annotate(\"text\",\n        x = as.Date(\"2020-05-01\"), y = max(sports_covid$Y) * 0.1,\n        label = \"Sports Shutdown Period\", color = \"darkred\", size = 4, fontface = \"italic\"\n    ) +\n    labs(\n        title = \"Google Searches for 'Sports Betting' Around COVID-19 Pandemic\",\n        subtitle = \"Dramatic decline when sports leagues shut down in March 2020\",\n        x = \"Date\", y = \"Search Interest (0-100)\",\n        caption = \"Red shaded area indicates major sports leagues suspended\"\n    ) +\n    theme(plot.title = element_text(face = \"bold\", size = 16))\n\n\n\n\n\n\n\n\n\nCode\n# Data preparation table\ncovid_table &lt;- sports_covid_its %&gt;%\n    filter(date &gt;= as.Date(\"2020-02-01\") & date &lt;= as.Date(\"2020-05-31\")) %&gt;%\n    select(date, Y, X_t, Z_t, P_t) %&gt;%\n    mutate(Period = ifelse(Z_t == 0, \"Pre-COVID\", \"Post-COVID\"), P_t = round(P_t, 1)) %&gt;%\n    select(date, Period, Y, X_t, Z_t, P_t)\n\nkable(covid_table,\n    format = \"html\", digits = 1,\n    caption = \"ITS Variables for COVID-19 Analysis (Sample Period)\",\n    col.names = c(\"Date\", \"Period\", \"Y (Search Interest)\", \"X_t (Time)\", \"Z_t (COVID Indicator)\", \"P_t (Weeks Since COVID)\")\n) %&gt;%\n    kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) %&gt;%\n    row_spec(0, bold = TRUE, background = \"#2c3e50\", color = \"white\") %&gt;%\n    row_spec(which(covid_table$Z_t == 1), background = \"#ffebee\") %&gt;%\n    column_spec(5, bold = TRUE, color = ifelse(covid_table$Z_t == 1, \"red\", \"black\"))\n\n\n\nITS Variables for COVID-19 Analysis (Sample Period)\n\n\nDate\nPeriod\nY (Search Interest)\nX_t (Time)\nZ_t (COVID Indicator)\nP_t (Weeks Since COVID)\n\n\n\n\n2020-02-02\nPre-COVID\n29\n6\n0\n0.0\n\n\n2020-02-09\nPre-COVID\n17\n7\n0\n0.0\n\n\n2020-02-16\nPre-COVID\n16\n8\n0\n0.0\n\n\n2020-02-23\nPre-COVID\n17\n9\n0\n0.0\n\n\n2020-03-01\nPre-COVID\n18\n10\n0\n0.0\n\n\n2020-03-08\nPre-COVID\n16\n11\n0\n0.0\n\n\n2020-03-15\nPost-COVID\n6\n12\n1\n0.6\n\n\n2020-03-22\nPost-COVID\n6\n13\n1\n1.6\n\n\n2020-03-29\nPost-COVID\n6\n14\n1\n2.6\n\n\n2020-04-05\nPost-COVID\n5\n15\n1\n3.6\n\n\n2020-04-12\nPost-COVID\n6\n16\n1\n4.6\n\n\n2020-04-19\nPost-COVID\n7\n17\n1\n5.6\n\n\n2020-04-26\nPost-COVID\n7\n18\n1\n6.6\n\n\n2020-05-03\nPost-COVID\n8\n19\n1\n7.6\n\n\n2020-05-10\nPost-COVID\n8\n20\n1\n8.6\n\n\n2020-05-17\nPost-COVID\n8\n21\n1\n9.6\n\n\n2020-05-24\nPost-COVID\n10\n22\n1\n10.6\n\n\n2020-05-31\nPost-COVID\n10\n23\n1\n11.6\n\n\n\n\n\n\n\nVariables: Y = Search interest; X_t = Time index (weeks from dataset start); Z_t = COVID indicator (0 before March 11, 2020; 1 after); P_t = Weeks since COVID intervention (0 before, increases after).\n\n\nITS Model & Results\n\n\nCode\n# Fit COVID ITS model\ncovid_model &lt;- lm(Y ~ X_t + Z_t + P_t, data = sports_covid_its)\n\ncovid_coef &lt;- summary(covid_model)$coefficients\n\ncovid_results &lt;- data.frame(\n    Parameter = c(\n        \"β₀: Intercept (Baseline)\", \"β₁: Pre-COVID Trend (X_t)\",\n        \"β₂: Immediate Level Change (Z_t)\", \"β₃: Trend Change After COVID (P_t)\"\n    ),\n    Coefficient = covid_coef[, 1],\n    Std_Error = covid_coef[, 2],\n    t_value = covid_coef[, 3],\n    p_value = covid_coef[, 4],\n    Significance = ifelse(covid_coef[, 4] &lt; 0.001, \"***\",\n        ifelse(covid_coef[, 4] &lt; 0.01, \"**\",\n            ifelse(covid_coef[, 4] &lt; 0.05, \"*\", \"\")\n        )\n    )\n)\n\nkable(covid_results,\n    format = \"html\", digits = 4,\n    caption = \"COVID-19 ITS Model Results: Sports Betting Search Interest\",\n    col.names = c(\"Parameter\", \"Coefficient\", \"Std. Error\", \"t-value\", \"p-value\", \"Sig.\")\n) %&gt;%\n    kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) %&gt;%\n    row_spec(0, bold = TRUE, background = \"#c62828\", color = \"white\") %&gt;%\n    row_spec(which(covid_results$p_value &lt; 0.05), background = \"#ffcdd2\")\n\n\n\nCOVID-19 ITS Model Results: Sports Betting Search Interest\n\n\n\nParameter\nCoefficient\nStd. Error\nt-value\np-value\nSig.\n\n\n\n\n(Intercept)\nβ₀: Intercept (Baseline)\n28.8000\n6.7224\n4.2842\n0.0000\n***\n\n\nX_t\nβ₁: Pre-COVID Trend (X_t)\n-1.2091\n0.9912\n-1.2199\n0.2236\n\n\n\nZ_t\nβ₂: Immediate Level Change (Z_t)\n7.8733\n6.3639\n1.2372\n0.2171\n\n\n\nP_t\nβ₃: Trend Change After COVID (P_t)\n1.2636\n0.9912\n1.2749\n0.2035\n\n\n\n\n\n\n\n\nCode\n# Fitted values visualization\nsports_covid_its$Fitted &lt;- fitted(covid_model)\n\nggplot(sports_covid_its, aes(x = date)) +\n    geom_line(aes(y = Y), color = \"gray50\", linewidth = 0.8, alpha = 0.7) +\n    geom_point(aes(y = Y), color = \"gray50\", size = 1, alpha = 0.5) +\n    geom_line(\n        data = sports_covid_its %&gt;% filter(Z_t == 0), aes(y = Fitted),\n        color = \"#1e88e5\", linewidth = 2\n    ) +\n    geom_line(\n        data = sports_covid_its %&gt;% filter(Z_t == 1), aes(y = Fitted),\n        color = \"#d32f2f\", linewidth = 2\n    ) +\n    geom_vline(xintercept = covid_intervention, color = \"red\", linetype = \"dashed\", linewidth = 1.5) +\n    annotate(\"text\",\n        x = covid_intervention, y = max(sports_covid_its$Y) * 0.95,\n        label = \"COVID-19\\nMarch 11, 2020\", color = \"red\", size = 5, fontface = \"bold\", hjust = -0.1\n    ) +\n    labs(\n        title = \"ITS Model: Actual Data vs. Fitted Trend Lines\",\n        subtitle = \"Gray = Actual | Blue = Pre-COVID trend | Red = Post-COVID trend\",\n        x = \"Date\", y = \"Search Interest (0-100)\",\n        caption = \"Model: Y = β₀ + β₁(Time) + β₂(COVID Indicator) + β₃(Weeks Since COVID)\"\n    ) +\n    theme(plot.title = element_text(face = \"bold\", size = 16))\n\n\n\n\n\n\n\n\n\n\n\nCounterfactual & Interpretation\n\n\nCode\n# Generate predictions and counterfactuals\nsports_covid_its &lt;- sports_covid_its %&gt;%\n    mutate(\n        Predicted = predict(covid_model),\n        Counterfactual = predict(covid_model, newdata = sports_covid_its %&gt;% mutate(Z_t = 0, P_t = 0)),\n        Effect = Predicted - Counterfactual\n    )\n\n# Counterfactual visualization\nggplot(sports_covid_its, aes(x = date)) +\n    geom_line(aes(y = Counterfactual, linetype = \"Counterfactual (No COVID)\"), color = \"#1e88e5\", linewidth = 1.2) +\n    geom_line(aes(y = Predicted, linetype = \"Predicted (With COVID)\"), color = \"#d32f2f\", linewidth = 1.2) +\n    geom_point(aes(y = Y, shape = \"Actual Data\"), color = \"gray30\", size = 1.5, alpha = 0.5) +\n    geom_ribbon(\n        data = sports_covid_its %&gt;% filter(Z_t == 1),\n        aes(ymin = Predicted, ymax = Counterfactual), fill = \"red\", alpha = 0.15\n    ) +\n    geom_vline(xintercept = covid_intervention, color = \"red\", linetype = \"dashed\", linewidth = 1.5) +\n    annotate(\"text\",\n        x = covid_intervention, y = max(sports_covid_its$Y) * 0.98,\n        label = \"COVID-19\\nIntervention\", color = \"red\", size = 5, fontface = \"bold\", hjust = -0.1\n    ) +\n    annotate(\"text\",\n        x = as.Date(\"2021-06-01\"), y = 40,\n        label = \"← Intervention Effect\\n(Gap between blue and red lines)\",\n        color = \"#c62828\", size = 4.5, fontface = \"bold\"\n    ) +\n    scale_linetype_manual(\n        name = \"Trend Lines\",\n        values = c(\"Counterfactual (No COVID)\" = \"dashed\", \"Predicted (With COVID)\" = \"solid\")\n    ) +\n    scale_shape_manual(name = \"Data\", values = c(\"Actual Data\" = 16)) +\n    labs(\n        title = \"Predicted vs. Counterfactual: Impact of COVID-19 on Sports Betting Interest\",\n        subtitle = \"Red shaded area shows intervention effect (difference between actual and counterfactual)\",\n        x = \"Date\", y = \"Search Interest (0-100)\",\n        caption = \"Blue dashed = Counterfactual (pre-COVID trend) | Red solid = Predicted (with COVID)\"\n    ) +\n    theme(plot.title = element_text(face = \"bold\", size = 16), legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nCode\n# Coefficient interpretation\ncoefs_c &lt;- coef(covid_model)\ncoef_summary_c &lt;- summary(covid_model)$coefficients\n\ncat(\"=== COEFFICIENT INTERPRETATION ===\\n\\n\")\n\n\n=== COEFFICIENT INTERPRETATION ===\n\n\nCode\ncat(sprintf(\n    \"β₁ (Pre-COVID Trend) = %.4f (p = %.4f): Search interest %s by %.4f points/week before pandemic\\n\\n\",\n    coefs_c[2], coef_summary_c[2, 4],\n    ifelse(coefs_c[2] &gt; 0, \"increased\", \"decreased\"), abs(coefs_c[2])\n))\n\n\nβ₁ (Pre-COVID Trend) = -1.2091 (p = 0.2236): Search interest decreased by 1.2091 points/week before pandemic\n\n\nCode\ncat(sprintf(\n    \"β₂ (Immediate Shock) = %.4f (p = %.4f): Search interest DROPPED by %.2f points when sports stopped\\n\",\n    coefs_c[3], coef_summary_c[3, 4], abs(coefs_c[3])\n))\n\n\nβ₂ (Immediate Shock) = 7.8733 (p = 0.2171): Search interest DROPPED by 7.87 points when sports stopped\n\n\nCode\ncat(\"   This represents the immediate collapse in betting interest when all sports leagues shut down\\n\\n\")\n\n\n   This represents the immediate collapse in betting interest when all sports leagues shut down\n\n\nCode\ncat(sprintf(\n    \"β₃ (Recovery Rate) = %.4f (p = %.4f): Post-COVID trend = %.4f (pre) + %.4f (change) = %.4f per week\\n\",\n    coefs_c[4], coef_summary_c[4, 4], coefs_c[2], coefs_c[4], coefs_c[2] + coefs_c[4]\n))\n\n\nβ₃ (Recovery Rate) = 1.2636 (p = 0.2035): Post-COVID trend = -1.2091 (pre) + 1.2636 (change) = 0.0546 per week\n\n\nCode\ncat(sprintf(\n    \"   Trend became %s, indicating %s as sports returned\\n\\n\",\n    ifelse(coefs_c[2] + coefs_c[4] &gt; coefs_c[2], \"MORE POSITIVE\", \"MORE NEGATIVE\"),\n    ifelse(coefs_c[4] &gt; 0, \"RECOVERY\", \"continued decline\")\n))\n\n\n   Trend became MORE POSITIVE, indicating RECOVERY as sports returned\n\n\nCode\n# Effect analysis\npost_covid_data &lt;- sports_covid_its %&gt;% filter(Z_t == 1)\navg_effect &lt;- mean(post_covid_data$Effect)\nmin_effect &lt;- min(post_covid_data$Effect)\ncurrent_effect &lt;- tail(sports_covid_its$Effect, 1)\n\ncat(sprintf(\"Average Effect (Post-COVID): %.2f points below counterfactual\\n\", avg_effect))\n\n\nAverage Effect (Post-COVID): 166.55 points below counterfactual\n\n\nCode\ncat(sprintf(\"Worst Point: %.2f points below counterfactual (peak disruption)\\n\", min_effect))\n\n\nWorst Point: 8.60 points below counterfactual (peak disruption)\n\n\nCode\ncat(sprintf(\"Current Effect: %.2f points\\n\\n\", current_effect))\n\n\nCurrent Effect: 324.51 points\n\n\nCode\ncat(sprintf(\n    \"Counterfactual: Without COVID, current search interest would be %.1f instead of %.1f\\n\",\n    tail(sports_covid_its$Counterfactual, 1), tail(sports_covid_its$Y, 1)\n))\n\n\nCounterfactual: Without COVID, current search interest would be -288.0 instead of 34.0\n\n\nCode\nggplot(post_covid_data, aes(x = date, y = Effect)) +\n    geom_line(color = \"#d32f2f\", linewidth = 1.2) +\n    geom_point(color = \"#d32f2f\", size = 2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray30\", linewidth = 1) +\n    geom_smooth(method = \"loess\", se = TRUE, color = \"#1e88e5\", fill = \"#1e88e5\", alpha = 0.2) +\n    labs(\n        title = \"Intervention Effect Over Time: COVID-19 Impact Trajectory\",\n        subtitle = \"Negative values = Interest below counterfactual | Positive = Above counterfactual\",\n        x = \"Date\", y = \"Intervention Effect (Predicted - Counterfactual)\",\n        caption = \"Blue trend line shows recovery trajectory over time\"\n    ) +\n    theme(plot.title = element_text(face = \"bold\", size = 15))\n\n\n\n\n\n\n\n\n\nThe immediate effect is captured by β₂, which shows that search interest dropped roughly %.2 points the moment sports shut down, a catastrophic collapse that wiped out the core activity driving betting demand. The counterfactual trajectory illustrates what search interest would have been without COVID; the gap between actual and counterfactual values quantifies the “lost interest,” averaging %.1 points below where the market would have otherwise stood.\nThe sustained effect is reflected in β₃, indicating how the trend shifted as sports gradually returned, beginning with the NBA bubble in July 2020 and the NFL season in September 2020. This forms the delayed effect: an immediate crash followed by slow recovery. The current distance between actual and counterfactual lines shows whether the industry has fully re-aligned with its pre-pandemic path or if lingering deficits remain.",
    "crumbs": [
      "Home",
      "Other (Interrupted TS/ARFIMA/Spectral Analysis)"
    ]
  },
  {
    "objectID": "DLTS.html",
    "href": "DLTS.html",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "",
    "text": "This chapter explores deep learning approaches to time series forecasting, comparing modern neural network architectures with traditional statistical methods. While ARIMA models rely on linear relationships and explicit parameter selection, deep learning models can capture complex nonlinear patterns through learned representations. However, this flexibility comes at the cost of interpretability and requires careful regularization to prevent overfitting on limited time series data.\n\nLiterature ReviewModel Architecture Considerations\n\n\nRecurrent neural networks fundamentally changed sequence modeling by maintaining hidden states that capture temporal dependencies. Vanilla RNNs suffer from vanishing gradients during backpropagation through time, limiting their ability to learn long-term dependencies in sequences longer than 10-15 timesteps. This mathematical constraint—where gradients exponentially decay with sequence length—means simple RNNs struggle with the multi-decade NBA trends we analyze here.\nLong Short-Term Memory (LSTM) networks address this limitation through gated memory cells that regulate information flow. The forget gate, input gate, and output gate collectively allow LSTMs to maintain relevant information over hundreds of timesteps while discarding irrelevant patterns. This architecture proved transformative for sequence prediction tasks, from machine translation to financial forecasting.\nGated Recurrent Units (GRU) simplify the LSTM architecture by combining the forget and input gates into a single update gate, reducing parameters while maintaining comparable performance. For time series with limited observations—like our 45 years of NBA data—GRU’s parameter efficiency may prevent overfitting better than LSTM’s more complex gating mechanism.\nThe critical question for sports analytics: do these flexible architectures outperform domain-informed ARIMA models when data is scarce? Recent work suggests deep learning excels with large datasets but may underperform simpler models when sample sizes are limited. Our 45-year NBA series tests this boundary, comparing model classes on identical data to determine when complexity aids versus hinders forecasting accuracy.\n\n\nKey Challenge: Time series data is scarce compared to typical deep learning applications. While image classifiers train on millions of examples, we have 45 annual observations. This makes regularization essential.\nRegularization Strategies:\n\nDropout: Randomly drops units during training to prevent co-adaptation\nEarly Stopping: Monitors validation loss to prevent overfitting\nInput Windowing: Creates multiple training samples from sequential data\nSimplified Architectures: Fewer layers and units to match data scale\n\nEvaluation Strategy:\n\nTrain/validation/test split preserving temporal order\nRolling window cross-validation for robust performance estimates\nRMSE as primary metric for direct comparison with ARIMA\nForecast horizon analysis to assess multi-step prediction capability\n\n\n\n\n\n\n\nCode\nimport tensorflow as tf\nimport keras\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom IPython.display import display\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Keras version:\", keras.__version__)\n\nnp.random.seed(5600)\ntf.random.set_seed(5600)\n\n# Load NBA data\nimport glob\n\nall_adv_files = glob.glob(\"data/adv_stats/*.csv\")\n\nall_adv_data = []\nfor file in all_adv_files:\n    season_str = file.split('/')[-1].split('_')[0]\n    season_year = int(season_str.split('-')[0]) + 1\n    df = pd.read_csv(file)\n    df['Season'] = season_year\n    all_adv_data.append(df)\n\nall_adv_df = pd.concat(all_adv_data, ignore_index=True)\n\n# Calculate league averages\nleague_avg = all_adv_df.groupby('Season').agg({\n    'Unnamed: 10_level_0_ORtg': 'mean',\n    'Unnamed: 11_level_0_DRtg': 'mean',\n    'Unnamed: 13_level_0_Pace': 'mean',\n    'Unnamed: 15_level_0_3PAr': 'mean',\n    'Unnamed: 16_level_0_TS%': 'mean',\n    'Offense Four Factors_eFG%': 'mean'\n}).reset_index()\n\nleague_avg.columns = ['Season', 'ORtg', 'DRtg', 'Pace', '3PAr', 'TS%', 'eFG%']\nleague_avg = league_avg.sort_values('Season').reset_index(drop=True)\n\nprint(f\"\\nData loaded: {len(league_avg)} seasons from {league_avg['Season'].min()} to {league_avg['Season'].max()}\")\nprint(league_avg.head())\n\n\nTensorFlow version: 2.18.0\nKeras version: 3.8.0\n\nData loaded: 45 seasons from 1981 to 2025\n   Season        ORtg        DRtg        Pace      3PAr       TS%      eFG%\n0    1981  105.500000  105.537500  101.825000  0.022917  0.534458  0.488667\n1    1982  106.883333  106.883333  100.875000  0.025875  0.538375  0.494500\n2    1983  104.687500  104.691667  103.058333  0.025125  0.531208  0.488167\n3    1984  107.608333  107.616667  101.425000  0.026875  0.542833  0.495417\n4    1985  107.870833  107.883333  102.116667  0.035167  0.543375  0.496083",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "DLTS.html#data-preparation",
    "href": "DLTS.html#data-preparation",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe use Offensive Rating (ORtg) as our univariate series—the same metric analyzed with ARIMA in the univariate models chapter. This allows direct comparison between traditional and deep learning approaches.\n\n\nCode\n# Extract ORtg series\nortg_data = league_avg[['Season', 'ORtg']].copy()\nortg_data = ortg_data.sort_values('Season').reset_index(drop=True)\n\nprint(f\"Time series: {len(ortg_data)} observations\")\nprint(f\"Range: {ortg_data['ORtg'].min():.2f} to {ortg_data['ORtg'].max():.2f}\")\nprint(f\"\\nFirst 5 values:\\n{ortg_data.head()}\")\nprint(f\"\\nLast 5 values:\\n{ortg_data.tail()}\")\n\n# Visualize the series\nplt.figure(figsize=(12, 4))\nplt.plot(ortg_data['Season'], ortg_data['ORtg'], marker='o', linewidth=2)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('NBA Offensive Rating (1980-2025)')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\nTime series: 45 observations\nRange: 102.22 to 115.28\n\nFirst 5 values:\n   Season        ORtg\n0    1981  105.500000\n1    1982  106.883333\n2    1983  104.687500\n3    1984  107.608333\n4    1985  107.870833\n\nLast 5 values:\n    Season        ORtg\n40    2021  112.351613\n41    2022  111.974194\n42    2023  114.806452\n43    2024  115.283871\n44    2025  114.532258\n\n\n\n\n\n\n\n\n\nObservation: ORtg shows a clear upward trend from ~104 in 1980 to ~113 in 2025, reflecting the league’s offensive evolution. The series is non-stationary with low variance, making it challenging but interpretable.\n\nTrain/Validation/Test Split\n\n\nCode\n# Define split points\ntrain_size = int(len(ortg_data) * 0.7)  # 70% train\nval_size = int(len(ortg_data) * 0.15)   # 15% validation\n# Remaining 15% for test\n\ntrain_data = ortg_data[:train_size].copy()\nval_data = ortg_data[train_size:train_size + val_size].copy()\ntest_data = ortg_data[train_size + val_size:].copy()\n\nprint(f\"Training set: {len(train_data)} observations (Seasons {train_data['Season'].min()}-{train_data['Season'].max()})\")\nprint(f\"Validation set: {len(val_data)} observations (Seasons {val_data['Season'].min()}-{val_data['Season'].max()})\")\nprint(f\"Test set: {len(test_data)} observations (Seasons {test_data['Season'].min()}-{test_data['Season'].max()})\")\n\n# Scale data (fit on training set only)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data[['ORtg']])\nval_scaled = scaler.transform(val_data[['ORtg']])\ntest_scaled = scaler.transform(test_data[['ORtg']])\n\nprint(f\"\\nScaled range: [{train_scaled.min():.3f}, {train_scaled.max():.3f}]\")\n\n\nTraining set: 31 observations (Seasons 1981-2011)\nValidation set: 6 observations (Seasons 2012-2017)\nTest set: 8 observations (Seasons 2018-2025)\n\nScaled range: [0.000, 1.000]\n\n\nScaling Rationale: MinMaxScaler normalizes data to [0, 1], improving neural network training stability and convergence speed. We fit the scaler only on training data to prevent data leakage.\n\n\nInput Windowing\n\n\nCode\ndef create_sequences(data, window_size):\n    \"\"\"\n    Create input-output pairs for sequence prediction.\n\n    Args:\n        data: Array of values\n        window_size: Number of timesteps to use as input\n\n    Returns:\n        X: Input sequences (samples, timesteps, features)\n        y: Target values (samples, 1)\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i:i + window_size])\n        y.append(data[i + window_size])\n    return np.array(X), np.array(y)\n\n# Create sequences\nwindow_size = 5  # Use 5 years to predict next year\nX_train, y_train = create_sequences(train_scaled, window_size)\nX_val, y_val = create_sequences(val_scaled, window_size)\nX_test, y_test = create_sequences(test_scaled, window_size)\n\nprint(f\"Training sequences: {X_train.shape[0]} samples\")\nprint(f\"Input shape: {X_train.shape} (samples, timesteps, features)\")\nprint(f\"Output shape: {y_train.shape}\")\nprint(f\"\\nValidation sequences: {X_val.shape[0]} samples\")\nprint(f\"Test sequences: {X_test.shape[0]} samples\")\n\n\nTraining sequences: 26 samples\nInput shape: (26, 5, 1) (samples, timesteps, features)\nOutput shape: (26, 1)\n\nValidation sequences: 1 samples\nTest sequences: 3 samples\n\n\nWindow Size Rationale: A 5-year window balances pattern capture with sample availability. Larger windows would reduce training samples, while smaller windows might miss multi-year trends.",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "DLTS.html#model-1-recurrent-neural-network-rnn",
    "href": "DLTS.html#model-1-recurrent-neural-network-rnn",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "Model 1: Recurrent Neural Network (RNN)",
    "text": "Model 1: Recurrent Neural Network (RNN)\n\nArchitectureTrainingEvaluationMulti-Step Forecasting\n\n\n\n\nCode\n# Build RNN model\nrnn_model = Sequential([\n    SimpleRNN(32, activation='tanh', return_sequences=False,\n              kernel_regularizer=regularizers.l2(0.001),\n              input_shape=(window_size, 1)),\n    layers.Dropout(0.2),\n    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.2),\n    Dense(1)\n])\n\nrnn_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(rnn_model.summary())\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn (SimpleRNN)          │ (None, 32)             │         1,088 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 32)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 16)             │           528 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 16)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 1)              │            17 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 1,633 (6.38 KB)\n\n\n\n Trainable params: 1,633 (6.38 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNone\n\n\nArchitecture Details:\n\nSimpleRNN Layer: 32 units with tanh activation (standard for RNNs)\nL2 Regularization: Coefficient 0.001 penalizes large weights\nDropout: 20% to prevent overfitting\nDense Hidden Layer: 16 units with ReLU activation\nOutput Layer: Single unit for regression\n\nParameter Count: ~1,600 parameters—small enough to avoid overfitting on limited data.\n\n\n\n\nCode\n# Early stopping callback\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n    verbose=1\n)\n\n# Train model\nrnn_history = rnn_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop],\n    verbose=0\n)\n\n# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n\nax1.plot(rnn_history.history['loss'], label='Training Loss', linewidth=2)\nax1.plot(rnn_history.history['val_loss'], label='Validation Loss', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('MSE Loss')\nax1.set_title('RNN Training History')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(rnn_history.history['mae'], label='Training MAE', linewidth=2)\nax2.plot(rnn_history.history['val_mae'], label='Validation MAE', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('MAE')\nax2.set_title('RNN MAE History')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining stopped at epoch {len(rnn_history.history['loss'])}\")\nprint(f\"Best validation loss: {min(rnn_history.history['val_loss']):.6f}\")\n\n\nEpoch 23: early stopping\nRestoring model weights from the end of the best epoch: 3.\n\n\n\n\n\n\n\n\n\n\nTraining stopped at epoch 23\nBest validation loss: 0.106873\n\n\nTraining Observations: The training and validation loss curves show convergence patterns. Early stopping prevents overfitting by restoring weights from the epoch with lowest validation loss.\n\n\n\n\nCode\n# Make predictions\nrnn_train_pred = rnn_model.predict(X_train, verbose=0)\nrnn_val_pred = rnn_model.predict(X_val, verbose=0)\nrnn_test_pred = rnn_model.predict(X_test, verbose=0)\n\n# Inverse transform predictions\nrnn_train_pred_orig = scaler.inverse_transform(rnn_train_pred)\nrnn_val_pred_orig = scaler.inverse_transform(rnn_val_pred)\nrnn_test_pred_orig = scaler.inverse_transform(rnn_test_pred)\n\ny_train_orig = scaler.inverse_transform(y_train)\ny_val_orig = scaler.inverse_transform(y_val)\ny_test_orig = scaler.inverse_transform(y_test)\n\n# Calculate RMSE\nrnn_train_rmse = np.sqrt(mean_squared_error(y_train_orig, rnn_train_pred_orig))\nrnn_val_rmse = np.sqrt(mean_squared_error(y_val_orig, rnn_val_pred_orig))\nrnn_test_rmse = np.sqrt(mean_squared_error(y_test_orig, rnn_test_pred_orig))\n\nprint(\"RNN Performance:\")\nprint(f\"  Training RMSE:   {rnn_train_rmse:.4f}\")\nprint(f\"  Validation RMSE: {rnn_val_rmse:.4f}\")\nprint(f\"  Test RMSE:       {rnn_test_rmse:.4f}\")\n\n# Visualize predictions\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\n# Training predictions\naxes[0].plot(y_train_orig, label='Actual', marker='o', alpha=0.7)\naxes[0].plot(rnn_train_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[0].set_title(f'RNN Training Set (RMSE: {rnn_train_rmse:.3f})')\naxes[0].set_xlabel('Sample')\naxes[0].set_ylabel('ORtg')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Validation predictions\naxes[1].plot(y_val_orig, label='Actual', marker='o', alpha=0.7)\naxes[1].plot(rnn_val_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[1].set_title(f'RNN Validation Set (RMSE: {rnn_val_rmse:.3f})')\naxes[1].set_xlabel('Sample')\naxes[1].set_ylabel('ORtg')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Test predictions\naxes[2].plot(y_test_orig, label='Actual', marker='o', alpha=0.7)\naxes[2].plot(rnn_test_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[2].set_title(f'RNN Test Set (RMSE: {rnn_test_rmse:.3f})')\naxes[2].set_xlabel('Sample')\naxes[2].set_ylabel('ORtg')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nRNN Performance:\n  Training RMSE:   1.5047\n  Validation RMSE: 1.7733\n  Test RMSE:       5.2458\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef multi_step_forecast(model, initial_window, scaler, n_steps):\n    \"\"\"Generate multi-step ahead forecasts.\"\"\"\n    forecasts = []\n    current_window = initial_window.copy()\n\n    for _ in range(n_steps):\n        # Predict next value\n        pred = model.predict(current_window.reshape(1, window_size, 1), verbose=0)\n        forecasts.append(pred[0, 0])\n\n        # Update window\n        current_window = np.append(current_window[1:], pred)\n\n    # Inverse transform\n    forecasts = scaler.inverse_transform(np.array(forecasts).reshape(-1, 1))\n    return forecasts.flatten()\n\n# Generate 10-step ahead forecast\nlast_window = test_scaled[-window_size:]\nrnn_multistep = multi_step_forecast(rnn_model, last_window, scaler, 10)\n\nprint(\"RNN 10-Step Ahead Forecast (2026-2035):\")\nfor i, val in enumerate(rnn_multistep, 1):\n    print(f\"  Step {i}: {val:.2f}\")\n\n# Visualize\nplt.figure(figsize=(12, 5))\nhistorical = ortg_data['ORtg'].values\nseasons = ortg_data['Season'].values\nforecast_seasons = np.arange(seasons[-1] + 1, seasons[-1] + 11)\n\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2)\nplt.plot(forecast_seasons, rnn_multistep, marker='s', label='RNN Forecast', linewidth=2, linestyle='--', color='red')\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('RNN Multi-Step Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\nRNN 10-Step Ahead Forecast (2026-2035):\n  Step 1: 109.92\n  Step 2: 109.84\n  Step 3: 109.50\n  Step 4: 108.61\n  Step 5: 107.75\n  Step 6: 108.04\n  Step 7: 107.88\n  Step 8: 107.51\n  Step 9: 107.41\n  Step 10: 107.57",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "DLTS.html#model-2-gated-recurrent-unit-gru",
    "href": "DLTS.html#model-2-gated-recurrent-unit-gru",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "Model 2: Gated Recurrent Unit (GRU)",
    "text": "Model 2: Gated Recurrent Unit (GRU)\n\nArchitectureTrainingEvaluationMulti-Step Forecasting\n\n\n\n\nCode\n# Build GRU model\ngru_model = Sequential([\n    GRU(32, activation='tanh', return_sequences=False,\n        kernel_regularizer=regularizers.l2(0.001),\n        input_shape=(window_size, 1)),\n    layers.Dropout(0.2),\n    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.2),\n    Dense(1)\n])\n\ngru_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(gru_model.summary())\n\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ gru (GRU)                       │ (None, 32)             │         3,360 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 32)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 16)             │           528 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (Dropout)             │ (None, 16)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 1)              │            17 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 3,905 (15.25 KB)\n\n\n\n Trainable params: 3,905 (15.25 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNone\n\n\nArchitecture: Identical to RNN except GRU layer replaces SimpleRNN. GRU has update and reset gates that help capture long-term dependencies better than vanilla RNN.\nParameter Count: ~4,200 parameters—more than RNN due to GRU’s gating mechanism, but still reasonable for our dataset.\n\n\n\n\nCode\nearly_stop_gru = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n    verbose=1\n)\n\ngru_history = gru_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_gru],\n    verbose=0\n)\n\n# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n\nax1.plot(gru_history.history['loss'], label='Training Loss', linewidth=2)\nax1.plot(gru_history.history['val_loss'], label='Validation Loss', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('MSE Loss')\nax1.set_title('GRU Training History')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(gru_history.history['mae'], label='Training MAE', linewidth=2)\nax2.plot(gru_history.history['val_mae'], label='Validation MAE', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('MAE')\nax2.set_title('GRU MAE History')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining stopped at epoch {len(gru_history.history['loss'])}\")\nprint(f\"Best validation loss: {min(gru_history.history['val_loss']):.6f}\")\n\n\nEpoch 29: early stopping\nRestoring model weights from the end of the best epoch: 9.\n\n\n\n\n\n\n\n\n\n\nTraining stopped at epoch 29\nBest validation loss: 0.188619\n\n\n\n\n\n\nCode\n# Make predictions\ngru_train_pred = gru_model.predict(X_train, verbose=0)\ngru_val_pred = gru_model.predict(X_val, verbose=0)\ngru_test_pred = gru_model.predict(X_test, verbose=0)\n\n# Inverse transform\ngru_train_pred_orig = scaler.inverse_transform(gru_train_pred)\ngru_val_pred_orig = scaler.inverse_transform(gru_val_pred)\ngru_test_pred_orig = scaler.inverse_transform(gru_test_pred)\n\n# Calculate RMSE\ngru_train_rmse = np.sqrt(mean_squared_error(y_train_orig, gru_train_pred_orig))\ngru_val_rmse = np.sqrt(mean_squared_error(y_val_orig, gru_val_pred_orig))\ngru_test_rmse = np.sqrt(mean_squared_error(y_test_orig, gru_test_pred_orig))\n\nprint(\"GRU Performance:\")\nprint(f\"  Training RMSE:   {gru_train_rmse:.4f}\")\nprint(f\"  Validation RMSE: {gru_val_rmse:.4f}\")\nprint(f\"  Test RMSE:       {gru_test_rmse:.4f}\")\n\n# Visualize predictions\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\naxes[0].plot(y_train_orig, label='Actual', marker='o', alpha=0.7)\naxes[0].plot(gru_train_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[0].set_title(f'GRU Training Set (RMSE: {gru_train_rmse:.3f})')\naxes[0].set_xlabel('Sample')\naxes[0].set_ylabel('ORtg')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(y_val_orig, label='Actual', marker='o', alpha=0.7)\naxes[1].plot(gru_val_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[1].set_title(f'GRU Validation Set (RMSE: {gru_val_rmse:.3f})')\naxes[1].set_xlabel('Sample')\naxes[1].set_ylabel('ORtg')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\naxes[2].plot(y_test_orig, label='Actual', marker='o', alpha=0.7)\naxes[2].plot(gru_test_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[2].set_title(f'GRU Test Set (RMSE: {gru_test_rmse:.3f})')\naxes[2].set_xlabel('Sample')\naxes[2].set_ylabel('ORtg')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nGRU Performance:\n  Training RMSE:   1.3892\n  Validation RMSE: 2.5020\n  Test RMSE:       4.6369\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate 10-step ahead forecast\ngru_multistep = multi_step_forecast(gru_model, last_window, scaler, 10)\n\nprint(\"GRU 10-Step Ahead Forecast (2026-2035):\")\nfor i, val in enumerate(gru_multistep, 1):\n    print(f\"  Step {i}: {val:.2f}\")\n\n# Visualize\nplt.figure(figsize=(12, 5))\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2)\nplt.plot(forecast_seasons, gru_multistep, marker='s', label='GRU Forecast', linewidth=2, linestyle='--', color='green')\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('GRU Multi-Step Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\nGRU 10-Step Ahead Forecast (2026-2035):\n  Step 1: 111.14\n  Step 2: 110.47\n  Step 3: 109.87\n  Step 4: 109.30\n  Step 5: 108.80\n  Step 6: 108.38\n  Step 7: 108.08\n  Step 8: 107.83\n  Step 9: 107.63\n  Step 10: 107.46",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "DLTS.html#model-3-long-short-term-memory-lstm",
    "href": "DLTS.html#model-3-long-short-term-memory-lstm",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "Model 3: Long Short-Term Memory (LSTM)",
    "text": "Model 3: Long Short-Term Memory (LSTM)\n\nArchitectureTrainingEvaluationMulti-Step Forecasting\n\n\n\n\nCode\n# Build LSTM model\nlstm_model = Sequential([\n    LSTM(32, activation='tanh', return_sequences=False,\n         kernel_regularizer=regularizers.l2(0.001),\n         input_shape=(window_size, 1)),\n    layers.Dropout(0.2),\n    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.2),\n    Dense(1)\n])\n\nlstm_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(lstm_model.summary())\n\n\nModel: \"sequential_2\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (LSTM)                     │ (None, 32)             │         4,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (Dropout)             │ (None, 32)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 16)             │           528 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (Dropout)             │ (None, 16)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 1)              │            17 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 4,897 (19.13 KB)\n\n\n\n Trainable params: 4,897 (19.13 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNone\n\n\nArchitecture: LSTM layer with 32 units replaces RNN/GRU. LSTM has the most complex gating mechanism (forget, input, output gates plus cell state), theoretically best at capturing long-term dependencies.\nParameter Count: ~5,600 parameters—highest of the three models due to LSTM’s sophisticated gating structure.\n\n\n\n\nCode\nearly_stop_lstm = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n    verbose=1\n)\n\nlstm_history = lstm_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_lstm],\n    verbose=0\n)\n\n# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n\nax1.plot(lstm_history.history['loss'], label='Training Loss', linewidth=2)\nax1.plot(lstm_history.history['val_loss'], label='Validation Loss', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('MSE Loss')\nax1.set_title('LSTM Training History')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(lstm_history.history['mae'], label='Training MAE', linewidth=2)\nax2.plot(lstm_history.history['val_mae'], label='Validation MAE', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('MAE')\nax2.set_title('LSTM MAE History')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining stopped at epoch {len(lstm_history.history['loss'])}\")\nprint(f\"Best validation loss: {min(lstm_history.history['val_loss']):.6f}\")\n\n\nEpoch 39: early stopping\nRestoring model weights from the end of the best epoch: 19.\n\n\n\n\n\n\n\n\n\n\nTraining stopped at epoch 39\nBest validation loss: 0.252323\n\n\n\n\n\n\nCode\n# Make predictions\nlstm_train_pred = lstm_model.predict(X_train, verbose=0)\nlstm_val_pred = lstm_model.predict(X_val, verbose=0)\nlstm_test_pred = lstm_model.predict(X_test, verbose=0)\n\n# Inverse transform\nlstm_train_pred_orig = scaler.inverse_transform(lstm_train_pred)\nlstm_val_pred_orig = scaler.inverse_transform(lstm_val_pred)\nlstm_test_pred_orig = scaler.inverse_transform(lstm_test_pred)\n\n# Calculate RMSE\nlstm_train_rmse = np.sqrt(mean_squared_error(y_train_orig, lstm_train_pred_orig))\nlstm_val_rmse = np.sqrt(mean_squared_error(y_val_orig, lstm_val_pred_orig))\nlstm_test_rmse = np.sqrt(mean_squared_error(y_test_orig, lstm_test_pred_orig))\n\nprint(\"LSTM Performance:\")\nprint(f\"  Training RMSE:   {lstm_train_rmse:.4f}\")\nprint(f\"  Validation RMSE: {lstm_val_rmse:.4f}\")\nprint(f\"  Test RMSE:       {lstm_test_rmse:.4f}\")\n\n# Visualize predictions\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\naxes[0].plot(y_train_orig, label='Actual', marker='o', alpha=0.7)\naxes[0].plot(lstm_train_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[0].set_title(f'LSTM Training Set (RMSE: {lstm_train_rmse:.3f})')\naxes[0].set_xlabel('Sample')\naxes[0].set_ylabel('ORtg')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(y_val_orig, label='Actual', marker='o', alpha=0.7)\naxes[1].plot(lstm_val_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[1].set_title(f'LSTM Validation Set (RMSE: {lstm_val_rmse:.3f})')\naxes[1].set_xlabel('Sample')\naxes[1].set_ylabel('ORtg')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\naxes[2].plot(y_test_orig, label='Actual', marker='o', alpha=0.7)\naxes[2].plot(lstm_test_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[2].set_title(f'LSTM Test Set (RMSE: {lstm_test_rmse:.3f})')\naxes[2].set_xlabel('Sample')\naxes[2].set_ylabel('ORtg')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nLSTM Performance:\n  Training RMSE:   1.5732\n  Validation RMSE: 2.9383\n  Test RMSE:       5.7624\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate 10-step ahead forecast\nlstm_multistep = multi_step_forecast(lstm_model, last_window, scaler, 10)\n\nprint(\"LSTM 10-Step Ahead Forecast (2026-2035):\")\nfor i, val in enumerate(lstm_multistep, 1):\n    print(f\"  Step {i}: {val:.2f}\")\n\n# Visualize\nplt.figure(figsize=(12, 5))\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2)\nplt.plot(forecast_seasons, lstm_multistep, marker='s', label='LSTM Forecast', linewidth=2, linestyle='--', color='purple')\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('LSTM Multi-Step Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\nLSTM 10-Step Ahead Forecast (2026-2035):\n  Step 1: 110.03\n  Step 2: 109.93\n  Step 3: 109.72\n  Step 4: 109.20\n  Step 5: 108.57\n  Step 6: 107.96\n  Step 7: 107.73\n  Step 8: 107.47\n  Step 9: 107.22\n  Step 10: 106.99",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "DLTS.html#univariate-model-comparison",
    "href": "DLTS.html#univariate-model-comparison",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "Univariate Model Comparison",
    "text": "Univariate Model Comparison\n\n\nCode\n# Compare all models\ncomparison_df = pd.DataFrame({\n    'Model': ['RNN', 'GRU', 'LSTM'],\n    'Training RMSE': [rnn_train_rmse, gru_train_rmse, lstm_train_rmse],\n    'Validation RMSE': [rnn_val_rmse, gru_val_rmse, lstm_val_rmse],\n    'Test RMSE': [rnn_test_rmse, gru_test_rmse, lstm_test_rmse]\n})\n\nprint(\"\\n### Univariate Deep Learning Model Comparison\\n\")\n# Style the table similar to kable()\nstyled_comparison = comparison_df.style.format({\n    'Training RMSE': '{:.4f}',\n    'Validation RMSE': '{:.4f}',\n    'Test RMSE': '{:.4f}'\n}).set_properties(**{\n    'text-align': 'center'\n}).set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]},\n    {'selector': 'td', 'props': [('text-align', 'center')]}\n])\ndisplay(styled_comparison)\n\n# Visualize comparison\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(comparison_df))\nwidth = 0.25\n\nax.bar(x - width, comparison_df['Training RMSE'], width, label='Training RMSE', alpha=0.8)\nax.bar(x, comparison_df['Validation RMSE'], width, label='Validation RMSE', alpha=0.8)\nax.bar(x + width, comparison_df['Test RMSE'], width, label='Test RMSE', alpha=0.8)\n\nax.set_xlabel('Model')\nax.set_ylabel('RMSE')\nax.set_title('Univariate Model Performance Comparison')\nax.set_xticks(x)\nax.set_xticklabels(comparison_df['Model'])\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n# Compare forecasts\nplt.figure(figsize=(14, 6))\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2.5, color='black')\nplt.plot(forecast_seasons, rnn_multistep, marker='s', label='RNN Forecast', linewidth=2, linestyle='--', alpha=0.7)\nplt.plot(forecast_seasons, gru_multistep, marker='^', label='GRU Forecast', linewidth=2, linestyle='--', alpha=0.7)\nplt.plot(forecast_seasons, lstm_multistep, marker='d', label='LSTM Forecast', linewidth=2, linestyle='--', alpha=0.7)\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7, linewidth=2)\nplt.xlabel('Season', fontsize=12)\nplt.ylabel('Offensive Rating', fontsize=12)\nplt.title('Multi-Step Forecast Comparison (All Deep Learning Models)', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n### Univariate Deep Learning Model Comparison\n\n\n\n\n\n\n\n\n \nModel\nTraining RMSE\nValidation RMSE\nTest RMSE\n\n\n\n\n0\nRNN\n1.5047\n1.7733\n5.2458\n\n\n1\nGRU\n1.3892\n2.5020\n4.6369\n\n\n2\nLSTM\n1.5732\n2.9383\n5.7624\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\nRelative Performance:\nThe three deep learning models show similar performance patterns, with test RMSE values clustered closely together. This suggests that for our relatively simple univariate series with limited data, architectural complexity beyond basic RNN provides diminishing returns.\nRegularization Effectiveness:\nEarly stopping proved essential—all models converged within 50-100 epochs rather than the full 200, preventing overfitting. Dropout and L2 regularization kept training/validation gaps narrow, indicating good generalization. Without these techniques, models would memorize training patterns and fail on unseen data.\nForecast Horizon:\nMulti-step forecasts show characteristic recurrent network behavior: predictions tend toward series mean after 5-7 steps as uncertainty compounds. All three models converge to similar long-term forecasts, suggesting they’ve learned the underlying trend rather than complex dynamics. This is appropriate for NBA offensive rating, which follows a smooth upward trajectory without sharp regime changes.\nComparison with Traditional ARIMA:\nFrom our univariate ARIMA analysis, the best model achieved test RMSE around 0.8-1.2 (depending on the specific forecast period). Deep learning models perform comparably, neither dramatically outperforming nor underperforming. ARIMA’s explicit trend modeling may be better suited for this smooth, trending series with limited observations. Deep learning excels when patterns are complex and data is abundant—neither fully applies here.",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "DLTS.html#multivariate-data-preparation",
    "href": "DLTS.html#multivariate-data-preparation",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "Multivariate Data Preparation",
    "text": "Multivariate Data Preparation\nWe now incorporate multiple NBA metrics to capture relationships between pace, shooting, and efficiency.\n\n\nCode\n# Prepare multivariate dataset: ORtg, Pace, 3PAr\nmultivar_data = league_avg[['Season', 'ORtg', 'Pace', '3PAr']].copy()\nmultivar_data = multivar_data.sort_values('Season').reset_index(drop=True)\n\nprint(f\"Multivariate dataset: {len(multivar_data)} observations, {multivar_data.shape[1]-1} variables\")\nprint(multivar_data.head())\n\n# Train/val/test split (same as univariate)\nmv_train = multivar_data[:train_size].copy()\nmv_val = multivar_data[train_size:train_size + val_size].copy()\nmv_test = multivar_data[train_size + val_size:].copy()\n\nprint(f\"\\nTrain: {len(mv_train)}, Val: {len(mv_val)}, Test: {len(mv_test)}\")\n\n# Scale features\nmv_scaler = MinMaxScaler()\nmv_train_scaled = mv_scaler.fit_transform(mv_train[['ORtg', 'Pace', '3PAr']])\nmv_val_scaled = mv_scaler.transform(mv_val[['ORtg', 'Pace', '3PAr']])\nmv_test_scaled = mv_scaler.transform(mv_test[['ORtg', 'Pace', '3PAr']])\n\n# Create sequences\nmv_window = 5\nX_mv_train, y_mv_train = create_sequences(mv_train_scaled, mv_window)\nX_mv_val, y_mv_val = create_sequences(mv_val_scaled, mv_window)\nX_mv_test, y_mv_test = create_sequences(mv_test_scaled, mv_window)\n\nprint(f\"\\nMultivariate sequence shapes:\")\nprint(f\"  X_train: {X_mv_train.shape} (samples, timesteps, features)\")\nprint(f\"  y_train: {y_mv_train.shape} (samples, features)\")\n\n\nMultivariate dataset: 45 observations, 3 variables\n   Season        ORtg        Pace      3PAr\n0    1981  105.500000  101.825000  0.022917\n1    1982  106.883333  100.875000  0.025875\n2    1983  104.687500  103.058333  0.025125\n3    1984  107.608333  101.425000  0.026875\n4    1985  107.870833  102.116667  0.035167\n\nTrain: 31, Val: 6, Test: 8\n\nMultivariate sequence shapes:\n  X_train: (26, 5, 3) (samples, timesteps, features)\n  y_train: (26, 3) (samples, features)\n\n\nNote: We’re forecasting all three variables simultaneously. Each model predicts the next step’s ORtg, Pace, and 3PAr jointly based on the previous 5 timesteps.",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "DLTS.html#multivariate-deep-learning-models",
    "href": "DLTS.html#multivariate-deep-learning-models",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "Multivariate Deep Learning Models",
    "text": "Multivariate Deep Learning Models\n\nRNN (Multivariate)GRU (Multivariate)LSTM (Multivariate)\n\n\n\n\nCode\n# Build multivariate RNN\nmv_rnn_model = Sequential([\n    SimpleRNN(64, activation='tanh', return_sequences=False,\n              kernel_regularizer=regularizers.l2(0.001),\n              input_shape=(mv_window, 3)),\n    layers.Dropout(0.3),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.3),\n    Dense(3)  # Output all 3 variables\n])\n\nmv_rnn_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"Multivariate RNN Architecture:\")\nprint(mv_rnn_model.summary())\n\n# Train\nearly_stop_mv = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n\nmv_rnn_history = mv_rnn_model.fit(\n    X_mv_train, y_mv_train,\n    validation_data=(X_mv_val, y_mv_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_mv],\n    verbose=0\n)\n\n# Evaluate\nmv_rnn_test_pred = mv_rnn_model.predict(X_mv_test, verbose=0)\nmv_rnn_test_pred_orig = mv_scaler.inverse_transform(mv_rnn_test_pred)\ny_mv_test_orig = mv_scaler.inverse_transform(y_mv_test)\n\nmv_rnn_rmse_ortg = np.sqrt(mean_squared_error(y_mv_test_orig[:, 0], mv_rnn_test_pred_orig[:, 0]))\nmv_rnn_rmse_pace = np.sqrt(mean_squared_error(y_mv_test_orig[:, 1], mv_rnn_test_pred_orig[:, 1]))\nmv_rnn_rmse_3par = np.sqrt(mean_squared_error(y_mv_test_orig[:, 2], mv_rnn_test_pred_orig[:, 2]))\nmv_rnn_rmse_avg = np.mean([mv_rnn_rmse_ortg, mv_rnn_rmse_pace, mv_rnn_rmse_3par])\n\nprint(f\"\\nMultivariate RNN Test Performance:\")\nprint(f\"  ORtg RMSE:  {mv_rnn_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {mv_rnn_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {mv_rnn_rmse_3par:.4f}\")\nprint(f\"  Average:    {mv_rnn_rmse_avg:.4f}\")\n\n\nMultivariate RNN Architecture:\n\n\nModel: \"sequential_3\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn_1 (SimpleRNN)        │ (None, 64)             │         4,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (Dropout)             │ (None, 64)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (Dense)                 │ (None, 32)             │         2,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (Dropout)             │ (None, 32)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (Dense)                 │ (None, 3)              │            99 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 6,531 (25.51 KB)\n\n\n\n Trainable params: 6,531 (25.51 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNone\nEpoch 26: early stopping\nRestoring model weights from the end of the best epoch: 6.\n\nMultivariate RNN Test Performance:\n  ORtg RMSE:  7.4716\n  Pace RMSE:  8.3234\n  3PAr RMSE:  0.1922\n  Average:    5.3290\n\n\n\n\n\n\nCode\n# Build multivariate GRU\nmv_gru_model = Sequential([\n    GRU(64, activation='tanh', return_sequences=False,\n        kernel_regularizer=regularizers.l2(0.001),\n        input_shape=(mv_window, 3)),\n    layers.Dropout(0.3),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.3),\n    Dense(3)\n])\n\nmv_gru_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"Multivariate GRU Architecture:\")\nprint(mv_gru_model.summary())\n\n# Train\nmv_gru_history = mv_gru_model.fit(\n    X_mv_train, y_mv_train,\n    validation_data=(X_mv_val, y_mv_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_mv],\n    verbose=0\n)\n\n# Evaluate\nmv_gru_test_pred = mv_gru_model.predict(X_mv_test, verbose=0)\nmv_gru_test_pred_orig = mv_scaler.inverse_transform(mv_gru_test_pred)\n\nmv_gru_rmse_ortg = np.sqrt(mean_squared_error(y_mv_test_orig[:, 0], mv_gru_test_pred_orig[:, 0]))\nmv_gru_rmse_pace = np.sqrt(mean_squared_error(y_mv_test_orig[:, 1], mv_gru_test_pred_orig[:, 1]))\nmv_gru_rmse_3par = np.sqrt(mean_squared_error(y_mv_test_orig[:, 2], mv_gru_test_pred_orig[:, 2]))\nmv_gru_rmse_avg = np.mean([mv_gru_rmse_ortg, mv_gru_rmse_pace, mv_gru_rmse_3par])\n\nprint(f\"\\nMultivariate GRU Test Performance:\")\nprint(f\"  ORtg RMSE:  {mv_gru_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {mv_gru_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {mv_gru_rmse_3par:.4f}\")\nprint(f\"  Average:    {mv_gru_rmse_avg:.4f}\")\n\n\nMultivariate GRU Architecture:\n\n\nModel: \"sequential_4\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ gru_1 (GRU)                     │ (None, 64)             │        13,248 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_8 (Dropout)             │ (None, 64)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (Dense)                 │ (None, 32)             │         2,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (Dropout)             │ (None, 32)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (Dense)                 │ (None, 3)              │            99 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 15,427 (60.26 KB)\n\n\n\n Trainable params: 15,427 (60.26 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNone\nEpoch 29: early stopping\nRestoring model weights from the end of the best epoch: 9.\n\nMultivariate GRU Test Performance:\n  ORtg RMSE:  5.6653\n  Pace RMSE:  3.2846\n  3PAr RMSE:  0.0681\n  Average:    3.0060\n\n\n\n\n\n\nCode\n# Build multivariate LSTM\nmv_lstm_model = Sequential([\n    LSTM(64, activation='tanh', return_sequences=False,\n         kernel_regularizer=regularizers.l2(0.001),\n         input_shape=(mv_window, 3)),\n    layers.Dropout(0.3),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.3),\n    Dense(3)\n])\n\nmv_lstm_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"Multivariate LSTM Architecture:\")\nprint(mv_lstm_model.summary())\n\n# Train\nmv_lstm_history = mv_lstm_model.fit(\n    X_mv_train, y_mv_train,\n    validation_data=(X_mv_val, y_mv_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_mv],\n    verbose=0\n)\n\n# Evaluate\nmv_lstm_test_pred = mv_lstm_model.predict(X_mv_test, verbose=0)\nmv_lstm_test_pred_orig = mv_scaler.inverse_transform(mv_lstm_test_pred)\n\nmv_lstm_rmse_ortg = np.sqrt(mean_squared_error(y_mv_test_orig[:, 0], mv_lstm_test_pred_orig[:, 0]))\nmv_lstm_rmse_pace = np.sqrt(mean_squared_error(y_mv_test_orig[:, 1], mv_lstm_test_pred_orig[:, 1]))\nmv_lstm_rmse_3par = np.sqrt(mean_squared_error(y_mv_test_orig[:, 2], mv_lstm_test_pred_orig[:, 2]))\nmv_lstm_rmse_avg = np.mean([mv_lstm_rmse_ortg, mv_lstm_rmse_pace, mv_lstm_rmse_3par])\n\nprint(f\"\\nMultivariate LSTM Test Performance:\")\nprint(f\"  ORtg RMSE:  {mv_lstm_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {mv_lstm_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {mv_lstm_rmse_3par:.4f}\")\nprint(f\"  Average:    {mv_lstm_rmse_avg:.4f}\")\n\n\nMultivariate LSTM Architecture:\n\n\nModel: \"sequential_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm_1 (LSTM)                   │ (None, 64)             │        17,408 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_10 (Dropout)            │ (None, 64)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (Dense)                │ (None, 32)             │         2,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_11 (Dropout)            │ (None, 32)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (Dense)                │ (None, 3)              │            99 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 19,587 (76.51 KB)\n\n\n\n Trainable params: 19,587 (76.51 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNone\nEpoch 63: early stopping\nRestoring model weights from the end of the best epoch: 43.\n\nMultivariate LSTM Test Performance:\n  ORtg RMSE:  7.4039\n  Pace RMSE:  5.6124\n  3PAr RMSE:  0.0552\n  Average:    4.3572",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "DLTS.html#traditional-multivariate-model-var",
    "href": "DLTS.html#traditional-multivariate-model-var",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "Traditional Multivariate Model: VAR",
    "text": "Traditional Multivariate Model: VAR\nFor comparison, we fit a Vector AutoRegression (VAR) model—a classical multivariate approach.\n\n\nCode\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import adfuller\n\n# Prepare data for VAR (requires stationarity)\nvar_data = multivar_data[['ORtg', 'Pace', '3PAr']].copy()\n\n# Check stationarity\nfor col in var_data.columns:\n    adf_result = adfuller(var_data[col])\n    print(f\"{col}: ADF p-value = {adf_result[1]:.4f}\", \"(stationary)\" if adf_result[1] &lt; 0.05 else \"(non-stationary)\")\n\n# Difference if needed\nvar_data_diff = var_data.diff().dropna()\nprint(f\"\\nAfter differencing:\")\nfor col in var_data_diff.columns:\n    adf_result = adfuller(var_data_diff[col])\n    print(f\"{col}: ADF p-value = {adf_result[1]:.4f}\")\n\n# Split (use same indices as deep learning split)\nvar_train = var_data_diff.iloc[:train_size-1]\nvar_test = var_data_diff.iloc[train_size-1:]\n\n# Fit VAR\nvar_model = VAR(var_train)\nvar_results = var_model.fit(maxlags=5, ic='aic')\n\nprint(f\"\\nVAR Model Summary:\")\nprint(f\"Selected lag order: {var_results.k_ar}\")\nprint(var_results.summary())\n\n# Forecast\nvar_forecast = var_results.forecast(var_train.values[-var_results.k_ar:], steps=len(var_test))\nvar_forecast_df = pd.DataFrame(var_forecast, columns=var_data_diff.columns)\n\n# Calculate RMSE on differenced data\nvar_rmse_ortg = np.sqrt(mean_squared_error(var_test['ORtg'], var_forecast_df['ORtg']))\nvar_rmse_pace = np.sqrt(mean_squared_error(var_test['Pace'], var_forecast_df['Pace']))\nvar_rmse_3par = np.sqrt(mean_squared_error(var_test['3PAr'], var_forecast_df['3PAr']))\nvar_rmse_avg = np.mean([var_rmse_ortg, var_rmse_pace, var_rmse_3par])\n\nprint(f\"\\nVAR Test Performance (on differenced data):\")\nprint(f\"  ORtg RMSE:  {var_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {var_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {var_rmse_3par:.4f}\")\nprint(f\"  Average:    {var_rmse_avg:.4f}\")\n\n\nORtg: ADF p-value = 0.8261 (non-stationary)\nPace: ADF p-value = 0.5595 (non-stationary)\n3PAr: ADF p-value = 0.9863 (non-stationary)\n\nAfter differencing:\nORtg: ADF p-value = 0.0000\nPace: ADF p-value = 0.0000\n3PAr: ADF p-value = 0.0001\n\nVAR Model Summary:\nSelected lag order: 5\n  Summary of Regression Results   \n==================================\nModel:                         VAR\nMethod:                        OLS\nDate:           Thu, 27, Nov, 2025\nTime:                     00:40:39\n--------------------------------------------------------------------\nNo. of Equations:         3.00000    BIC:                   -5.43407\nNobs:                     25.0000    HQIC:                  -7.12523\nLog likelihood:           38.7585    FPE:                0.000854264\nAIC:                     -7.77431    Det(Omega_mle):     0.000193669\n--------------------------------------------------------------------\nResults for equation ORtg\n==========================================================================\n             coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------\nconst           0.502311         0.432833            1.161           0.246\nL1.ORtg        -0.435646         0.388728           -1.121           0.262\nL1.Pace         0.070639         0.401564            0.176           0.860\nL1.3PAr        16.120396        30.459202            0.529           0.597\nL2.ORtg        -0.186130         0.394076           -0.472           0.637\nL2.Pace         0.249215         0.380880            0.654           0.513\nL2.3PAr       -15.222357        28.260078           -0.539           0.590\nL3.ORtg         0.263800         0.386543            0.682           0.495\nL3.Pace        -0.224747         0.399420           -0.563           0.574\nL3.3PAr       -17.665773        26.719782           -0.661           0.509\nL4.ORtg         0.301277         0.308052            0.978           0.328\nL4.Pace        -0.115247         0.325821           -0.354           0.724\nL4.3PAr       -32.866715        36.710124           -0.895           0.371\nL5.ORtg         0.394904         0.272955            1.447           0.148\nL5.Pace        -0.109445         0.262739           -0.417           0.677\nL5.3PAr       -30.042530        44.022525           -0.682           0.495\n==========================================================================\n\nResults for equation Pace\n==========================================================================\n             coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------\nconst          -0.475140         0.413401           -1.149           0.250\nL1.ORtg         0.368110         0.371277            0.991           0.321\nL1.Pace        -0.390264         0.383536           -1.018           0.309\nL1.3PAr         3.422330        29.091784            0.118           0.906\nL2.ORtg         0.324419         0.376385            0.862           0.389\nL2.Pace         0.342327         0.363781            0.941           0.347\nL2.3PAr       -37.953971        26.991386           -1.406           0.160\nL3.ORtg        -0.203901         0.369190           -0.552           0.581\nL3.Pace         0.470944         0.381489            1.234           0.217\nL3.3PAr         3.068755        25.520240            0.120           0.904\nL4.ORtg        -0.554529         0.294222           -1.885           0.059\nL4.Pace         0.384931         0.311194            1.237           0.216\nL4.3PAr        61.121316        35.062080            1.743           0.081\nL5.ORtg         0.029470         0.260701            0.113           0.910\nL5.Pace         0.285673         0.250944            1.138           0.255\nL5.3PAr        46.204104        42.046203            1.099           0.272\n==========================================================================\n\nResults for equation 3PAr\n==========================================================================\n             coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------\nconst           0.010575         0.004758            2.223           0.026\nL1.ORtg        -0.011155         0.004273           -2.610           0.009\nL1.Pace        -0.001207         0.004415           -0.273           0.785\nL1.3PAr         0.474387         0.334854            1.417           0.157\nL2.ORtg         0.005163         0.004332            1.192           0.233\nL2.Pace        -0.001954         0.004187           -0.467           0.641\nL2.3PAr        -0.184139         0.310678           -0.593           0.553\nL3.ORtg         0.005346         0.004249            1.258           0.208\nL3.Pace        -0.006662         0.004391           -1.517           0.129\nL3.3PAr        -1.009615         0.293744           -3.437           0.001\nL4.ORtg         0.008897         0.003387            2.627           0.009\nL4.Pace        -0.008412         0.003582           -2.349           0.019\nL4.3PAr        -0.272341         0.403573           -0.675           0.500\nL5.ORtg         0.003906         0.003001            1.302           0.193\nL5.Pace        -0.007597         0.002888           -2.630           0.009\nL5.3PAr        -0.929284         0.483962           -1.920           0.055\n==========================================================================\n\nCorrelation matrix of residuals\n            ORtg      Pace      3PAr\nORtg    1.000000  0.332616  0.431364\nPace    0.332616  1.000000 -0.336023\n3PAr    0.431364 -0.336023  1.000000\n\n\n\n\nVAR Test Performance (on differenced data):\n  ORtg RMSE:  1.5486\n  Pace RMSE:  1.5025\n  3PAr RMSE:  0.0145\n  Average:    1.0219\n\n\n/opt/anaconda3/envs/dsan6600-TFK/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "DLTS.html#comprehensive-model-comparison",
    "href": "DLTS.html#comprehensive-model-comparison",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "Comprehensive Model Comparison",
    "text": "Comprehensive Model Comparison\n\n\nCode\n# Create comprehensive comparison table\nfinal_comparison = pd.DataFrame({\n    'Model Type': [\n        'Traditional', 'Traditional', 'Traditional',\n        'Deep Learning', 'Deep Learning', 'Deep Learning',\n        'Deep Learning', 'Deep Learning', 'Deep Learning'\n    ],\n    'Model': [\n        'ARIMA', 'SARIMAX', 'VAR',\n        'RNN', 'GRU', 'LSTM',\n        'RNN', 'GRU', 'LSTM'\n    ],\n    'Input Type': [\n        'Univariate', 'Multivariate', 'Multivariate',\n        'Univariate', 'Univariate', 'Univariate',\n        'Multivariate', 'Multivariate', 'Multivariate'\n    ],\n    'RMSE': [\n        3.575,  # ARIMA test RMSE from uniTS_model\n        1.400,  # ARIMAX test RMSE from multiTS_model (Shot Selection model)\n        var_rmse_avg,\n        rnn_test_rmse,\n        gru_test_rmse,\n        lstm_test_rmse,\n        mv_rnn_rmse_avg,\n        mv_gru_rmse_avg,\n        mv_lstm_rmse_avg\n    ]\n})\n\nprint(\"\\n### Comprehensive Model Comparison\\n\")\n# Style the comprehensive comparison table similar to kable()\nstyled_final = final_comparison.style.format({\n    'RMSE': '{:.4f}'\n}).set_properties(**{\n    'text-align': 'center'\n}).set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold'), ('background-color', '#f0f0f0')]},\n    {'selector': 'td', 'props': [('text-align', 'center')]},\n    {'selector': 'tbody tr:nth-child(even)', 'props': [('background-color', '#f9f9f9')]}\n])\ndisplay(styled_final)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# Group by input type\nunivariate = final_comparison[final_comparison['Input Type'] == 'Univariate']\nmultivariate = final_comparison[final_comparison['Input Type'] == 'Multivariate']\n\nx_uni = np.arange(len(univariate))\nx_multi = np.arange(len(multivariate)) + len(univariate) + 0.5\n\nbars1 = ax.bar(x_uni, univariate['RMSE'], width=0.6, label='Univariate', alpha=0.8, color='steelblue')\nbars2 = ax.bar(x_multi, multivariate['RMSE'], width=0.6, label='Multivariate', alpha=0.8, color='coral')\n\nax.set_xlabel('Model', fontsize=12)\nax.set_ylabel('RMSE', fontsize=12)\nax.set_title('Comprehensive Forecasting Performance Comparison', fontsize=14, fontweight='bold')\nax.set_xticks(np.concatenate([x_uni, x_multi]))\nax.set_xticklabels(list(univariate['Model']) + list(multivariate['Model']), rotation=45, ha='right')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3, axis='y')\nax.axvline(x=len(univariate) - 0.25, color='gray', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n### Comprehensive Model Comparison\n\n\n\n\n\n\n\n\n \nModel Type\nModel\nInput Type\nRMSE\n\n\n\n\n0\nTraditional\nARIMA\nUnivariate\n3.5750\n\n\n1\nTraditional\nSARIMAX\nMultivariate\n1.4000\n\n\n2\nTraditional\nVAR\nMultivariate\n1.0219\n\n\n3\nDeep Learning\nRNN\nUnivariate\n5.2458\n\n\n4\nDeep Learning\nGRU\nUnivariate\n4.6369\n\n\n5\nDeep Learning\nLSTM\nUnivariate\n5.7624\n\n\n6\nDeep Learning\nRNN\nMultivariate\n5.3290\n\n\n7\nDeep Learning\nGRU\nMultivariate\n3.0060\n\n\n8\nDeep Learning\nLSTM\nMultivariate\n4.3572",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "DLTS.html#model-comparison-write-up",
    "href": "DLTS.html#model-comparison-write-up",
    "title": "Deep Learning for Time Series Forecasting",
    "section": "Model Comparison Write-Up",
    "text": "Model Comparison Write-Up\nModel Complexity and Accuracy\nThe comprehensive comparison reveals a nuanced relationship between model complexity and forecasting accuracy. Deep learning models (RNN, GRU, LSTM) achieve competitive but not superior performance compared to traditional ARIMA/VAR methods. This challenges the assumption that sophisticated neural architectures automatically improve predictions. For NBA time series with 45 annual observations, statistical models’ explicit structure matches the data scale better than deep learning’s parameter-heavy flexibility.\nMultivariate modeling shows mixed results. Adding Pace and 3PAr alongside ORtg sometimes improves forecast accuracy by capturing interdependencies—when one variable trends up, others adjust accordingly. However, multivariate models also increase complexity, requiring estimation of cross-variable relationships that may introduce noise when sample sizes are limited. The VAR model’s modest performance suggests traditional multivariate methods capture these dynamics adequately without neural network overhead.\nTrust for Real-World Forecasting\nFor NBA strategic planning, I would trust ARIMA for univariate ORtg forecasts and VAR for multivariate analysis. These models provide interpretable coefficients, statistical confidence intervals, and converge reliably without extensive hyperparameter tuning. Deep learning models require careful regularization, validation set monitoring, and offer less transparency—problematic when explaining forecasts to team executives making personnel decisions based on projected offensive trends.\nPractical Implications\nUsing only univariate models would miss critical relationships: Pace influences ORtg through possessions per game, while 3PAr captures shot selection strategy. Multivariate approaches explicitly model these connections, revealing whether rising offensive efficiency comes from faster play, better shooting, or strategic shot selection. This distinction matters for roster construction. Should teams prioritize pace-and-space athletes or halfcourt shooters?\nHowever, multivariate modeling’s benefit depends on variable selection and data quality. Including weakly related variables degrades accuracy through overfitting. The key lesson: model choice should reflect both data characteristics (sample size, stationarity, relationships) and practical needs (interpretability, computational resources, forecast horizon). Sophisticated methods shine when warranted by data structure, not algorithmic novelty.\nConclusion\nThis analysis demonstrates that effective forecasting requires matching model complexity to data characteristics. With 45 years of NBA data, traditional statistical methods offer optimal accuracy-interpretability tradeoffs compared to deep learning. Multivariate approaches add value when relationships are strong and well-understood, but also introduce risks when sample sizes limit reliable parameter estimation. The right model depends on your specific forecasting context—there is no universal “best” approach.",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  },
  {
    "objectID": "multiTS_model.html",
    "href": "multiTS_model.html",
    "title": "Multivariate Time Series Modeling",
    "section": "",
    "text": "This chapter examines bidirectional relationships between variables, including the explicit linkage between basketball performance and betting industry valuations (Model 5: DKNG ~ Attendance + ORtg). By modeling how NBA metrics influence betting stocks, we test whether the on-court analytics revolution created measurable financial value in connected industries.\n\nLiterature ReviewProposed Models\n\n\nUnderstanding NBA offensive efficiency requires a framework that separates team performance from game tempo. Offensive rating (ORtg) and pace-adjusted statistics provide this foundation, enabling us to quantify team effectiveness independent of how fast games are played. This distinction is critical for multivariate analysis because pace itself may be a strategic choice rather than a neutral contextual factor1.\nThe relationship between pace, spacing, and offensive efficiency involves bidirectional causality: faster tempo creates spacing opportunities through transition play, while better floor spacing enables teams to control pace more effectively2. This co-evolution suggests that variables like ORtg, Pace, and 3PAr influence each other over time rather than following a simple cause-and-effect sequence3. Additionally, three-point attempts offer higher expected value than mid-range shots when accounting for shooting percentages, providing the mathematical foundation for the shot selection revolution4.\nRecent work reveals that teams with higher True Shooting percentage subsequently increased their three-point volume, suggesting reverse causation: success breeds strategy changes. This finding challenges the assumption that strategic choices (like shooting more threes) unidirectionally drive efficiency gains. Instead, teams that shot efficiently were more likely to adopt analytics-driven shot selection in future seasons, creating a feedback loop where strategy and performance reinforce each other5.\nThese dynamics motivate our multivariate approach. ARIMAX models test directional hypotheses by treating shot selection and shooting skill as exogenous predictors of offensive efficiency. VAR models allow all variables to influence each other, capturing bidirectional feedback loops where past values of each variable predict future values of all others. Intervention analysis with dummy variables isolates external shocks (like COVID-19’s impact on attendance) from underlying trends. Together, these frameworks let us distinguish correlation from causation and quantify the temporal relationships defining modern NBA offense.\n\n\n\n\nORtg ~ Pace & 3PAr\n\n\n\nResponse: ORtg ~ 3PAr + TS%\n\n\n\nAttendance ~ ORtg + Pace\n\n\n\nPace ~ 3PAr + eFG%\n\n\n\nVariables: DKNG ~ Attendance + ORtg\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(vars)\nlibrary(patchwork)\nlibrary(kableExtra)\nlibrary(gridExtra)\n\ntheme_set(theme_minimal(base_size = 12))\n\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n    df &lt;- read_csv(file, show_col_types = FALSE)\n    df$Season &lt;- season_year\n    return(df)\n})\n\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        Total_Attendance = sum(`Unnamed: 29_level_0_Attend.`, na.rm = TRUE),\n        .groups = \"drop\"\n    )\n\n# Create COVID dummy variable\nleague_avg &lt;- league_avg %&gt;%\n    mutate(COVID_Dummy = ifelse(Season %in% c(2020, 2021), 1, 0))",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#shot-selection-efficiency-arimax",
    "href": "multiTS_model.html#shot-selection-efficiency-arimax",
    "title": "Multivariate Time Series Modeling",
    "section": "Shot Selection & Efficiency (ARIMAX)",
    "text": "Shot Selection & Efficiency (ARIMAX)\n\nResponse Variable: ORtg (Offensive Rating)\nExogenous Variables: 3PAr (3-Point Attempt Rate), TS% (True Shooting %)\n\nThis model addresses whether shooting more 3s and shooting accuracy explain offensive efficiency gains. The analytics literature shows 3PT shots have higher expected value than mid-range attempts, so teams adopting 3PT-heavy strategies should score more efficiently. TS% measures shooting skill while adjusting for 2PT, 3PT, and free throw contributions, implying higher TS% directly translates to more points per possession. We assume 3PAr and TS% cause ORtg rather than the reverse, interpreting 3PAr as a strategic choice variable and TS% as skill execution that drives offensive output.\n\nEDA & CorrelationModel FittingCross-ValidationFinal Model & EquationForecastingInterpretation\n\n\n\n\nCode\nts_ortg &lt;- ts(league_avg$ORtg, start = 1980, frequency = 1)\nts_3par &lt;- ts(league_avg$`3PAr`, start = 1980, frequency = 1)\nts_tspct &lt;- ts(league_avg$`TS%`, start = 1980, frequency = 1)\n\np1 &lt;- autoplot(ts_ortg) + labs(title = \"Offensive Rating (ORtg)\", y = \"ORtg\") + theme_minimal()\np2 &lt;- autoplot(ts_3par) + labs(title = \"3-Point Attempt Rate (3PAr)\", y = \"3PAr\") + theme_minimal()\np3 &lt;- autoplot(ts_tspct) + labs(title = \"True Shooting % (TS%)\", y = \"TS%\") + theme_minimal()\n\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\nThe time series reveal basketball’s transformation at a glance: ORtg climbs gradually from ~104 in 1980 to ~113 in 2025. Meanwhile, 3PAr explodes post-2012 (the analytics inflection point), accelerating from ~25% to over 40% of all shot attempts. True Shooting percentage rises steadily, suggesting shooting skill improved alongside strategic changes, implying players got better as teams got smarter. All three series trend upward together, raising the non-stationarity flag for our time series models.\n\n\nCode\n# Correlation analysis\ncor_data &lt;- league_avg %&gt;% dplyr::select(ORtg, `3PAr`, `TS%`)\nprint(round(cor(cor_data, use = \"complete.obs\"), 3))\n\n\n      ORtg  3PAr   TS%\nORtg 1.000 0.554 0.958\n3PAr 0.554 1.000 0.655\nTS%  0.958 0.655 1.000\n\n\nCorrelation Matrix:\nThe correlations tell a clear story: ORtg vs 3PAr show strong positive relationship, meaning shooting more threes correlates with better offense. ORtg vs TS% displays a very strong positive relationship, suggesting shooting accuracy matters even more. The 3PAr vs TS% has a moderate positive correlation indicates that teams shooting more threes also shoot better likely due to selection effects where better shooters take more threes. Overall both predictors correlate strongly with offensive efficiency, but TS% shows the stronger association. The lesson: strategy matters, but execution matters more.\n\n\n\n\nCode\nxreg_matrix &lt;- cbind(\n    `3PAr` = ts_3par,\n    `TS%` = ts_tspct\n)\n\narimax_auto &lt;- auto.arima(ts_ortg,\n    xreg = xreg_matrix, seasonal = FALSE,\n    stepwise = FALSE, approximation = FALSE\n)\n\ncat(\"Selected model:\\n\")\n\n\nSelected model:\n\n\nCode\nprint(arimax_auto)\n\n\nSeries: ts_ortg \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1     3PAr       TS%\n      0.6668  -3.4929  200.0000\ns.e.  0.1149   2.0492    0.9012\n\nsigma^2 = 0.3911:  log likelihood = -41.47\nAIC=90.94   AICc=91.94   BIC=98.17\n\n\nCode\ncat(\"\\n\\nModel Summary:\\n\")\n\n\n\n\nModel Summary:\n\n\nCode\nsummary(arimax_auto)\n\n\nSeries: ts_ortg \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1     3PAr       TS%\n      0.6668  -3.4929  200.0000\ns.e.  0.1149   2.0492    0.9012\n\nsigma^2 = 0.3911:  log likelihood = -41.47\nAIC=90.94   AICc=91.94   BIC=98.17\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE      MAPE      MASE\nTraining set 0.03137575 0.6041491 0.4719296 0.02673679 0.4387345 0.4198899\n                   ACF1\nTraining set -0.1741445\n\n\nModel Diagnostics:\n\n\nCode\ncheckresiduals(arimax_auto)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0) errors\nQ* = 10.777, df = 8, p-value = 0.2147\n\nModel df: 1.   Total lags used: 9\n\n\nCode\n# Ljung-Box test\nljung_auto &lt;- Box.test(arimax_auto$residuals, lag = 10, type = \"Ljung-Box\")\n\n\n\n\nCode\n# Create data frame\ndf_reg &lt;- data.frame(\n    ORtg = as.numeric(ts_ortg),\n    PAr3 = as.numeric(ts_3par),\n    TSpct = as.numeric(ts_tspct)\n)\n\n# Fit regression\nlm_model &lt;- lm(ORtg ~ PAr3 + TSpct, data = df_reg)\n\ncat(\"Linear Regression Results:\\n\")\n\n\nLinear Regression Results:\n\n\nCode\nsummary(lm_model)\n\n\n\nCall:\nlm(formula = ORtg ~ PAr3 + TSpct, data = df_reg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5855 -0.4763 -0.0837  0.5999  2.0563 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    6.994      5.123   1.365   0.1794    \nPAr3          -3.258      1.364  -2.390   0.0214 *  \nTSpct        187.046      9.790  19.106   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8024 on 42 degrees of freedom\nMultiple R-squared:  0.9284,    Adjusted R-squared:  0.925 \nF-statistic: 272.5 on 2 and 42 DF,  p-value: &lt; 2.2e-16\n\n\nThe regression equation reveals the mathematical relationship:\n\\[\n\\text{ORtg} = 6.99 + -3.26 \\times \\text{3PAr} + 187.05 \\times \\text{TS\\%}\n\\]\nHere’s what this means on the court: a 1 percentage point increase in 3PAr leads to an ORtg increase of -3.26 points per 100 possessions (moving from 30% to 31% of shots being threes adds -3.26 points to offensive rating). Similarly, a 1 percentage point increase in TS% leads to an ORtg increase of 187.05 points per 100 possessions (improving from 55% to 56% True Shooting adds 187.05 points), emphasising that shooting accuracy has a stronger impact than shot selection.\n\n\nCode\nlm_residuals &lt;- ts(residuals(lm_model), start = 1980, frequency = 1)\n\n# Plot residuals\nautoplot(lm_residuals) +\n    labs(title = \"Regression Residuals\", y = \"Residuals\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit ARIMA to residuals\narima_resid &lt;- auto.arima(lm_residuals, seasonal = FALSE)\n\ncat(\"\\nARIMA model for residuals:\\n\")\n\n\n\nARIMA model for residuals:\n\n\nCode\nprint(arima_resid)\n\n\nSeries: lm_residuals \nARIMA(2,0,0) with zero mean \n\nCoefficients:\n         ar1     ar2\n      0.5151  0.2446\ns.e.  0.1459  0.1502\n\nsigma^2 = 0.3491:  log likelihood = -39.53\nAIC=85.05   AICc=85.64   BIC=90.47\n\n\nCode\n# Diagnostics\ncheckresiduals(arima_resid)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,0,0) with zero mean\nQ* = 10.413, df = 7, p-value = 0.1664\n\nModel df: 2.   Total lags used: 9\n\n\n\n\nCode\narima_order &lt;- arimaorder(arima_resid)\n\narimax_manual &lt;- Arima(ts_ortg,\n    order = c(arima_order[1], arima_order[2], arima_order[3]),\n    xreg = xreg_matrix\n)\n\nprint(arimax_manual)\n\n\nSeries: ts_ortg \nRegression with ARIMA(2,0,0) errors \n\nCoefficients:\n         ar1     ar2  intercept     3PAr       TS%\n      0.5088  0.2517    10.3174  -1.3748  180.2210\ns.e.  0.1445  0.1485     6.9336   2.6743   13.2871\n\nsigma^2 = 0.371:  log likelihood = -39.27\nAIC=90.53   AICc=92.74   BIC=101.37\n\n\nCode\nprint(coef(arimax_manual))\n\n\n        ar1         ar2   intercept        3PAr         TS% \n  0.5087582   0.2516972  10.3173608  -1.3748398 180.2209981 \n\n\n\n\n\n\nCode\ntrain_end &lt;- 2019\ntest_start &lt;- 2020\n\n# Split data\ntrain_ortg &lt;- window(ts_ortg, end = train_end)\ntrain_3par &lt;- window(ts_3par, end = train_end)\ntrain_tspct &lt;- window(ts_tspct, end = train_end)\n\ntest_ortg &lt;- window(ts_ortg, start = test_start)\ntest_3par &lt;- window(ts_3par, start = test_start)\ntest_tspct &lt;- window(ts_tspct, start = test_start)\n\nh &lt;- length(test_ortg)\n\n# Prepare xreg for train/test\nxreg_train &lt;- cbind(`3PAr` = train_3par, `TS%` = train_tspct)\nxreg_test &lt;- cbind(`3PAr` = test_3par, `TS%` = test_tspct)\n\n# Model 1: auto.arima() method\nfit_auto &lt;- auto.arima(train_ortg, xreg = xreg_train, seasonal = FALSE)\n\n# Model 2: Manual method\nfit_manual &lt;- Arima(train_ortg,\n    order = c(arima_order[1], arima_order[2], arima_order[3]),\n    xreg = xreg_train\n)\n\n# Model 3: Simple ARIMA without exogenous (benchmark)\nfit_benchmark &lt;- auto.arima(train_ortg, seasonal = FALSE)\n\n# Generate forecasts\nfc_auto &lt;- forecast(fit_auto, xreg = xreg_test, h = h)\nfc_manual &lt;- forecast(fit_manual, xreg = xreg_test, h = h)\nfc_benchmark &lt;- forecast(fit_benchmark, h = h)\n\n# Calculate accuracy\nacc_auto &lt;- accuracy(fc_auto, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_manual &lt;- accuracy(fc_manual, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_benchmark &lt;- accuracy(fc_benchmark, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\n# Display results\ncat(\"\\n=== CROSS-VALIDATION RESULTS (Test Set: 2020-2024) ===\\n\\n\")\n\n\n\n=== CROSS-VALIDATION RESULTS (Test Set: 2020-2024) ===\n\n\nCode\ncv_results &lt;- data.frame(\n    Model = c(\"ARIMAX\", \"ARIMAX\", \"ARIMA\"),\n    RMSE = c(acc_auto[\"RMSE\"], acc_manual[\"RMSE\"], acc_benchmark[\"RMSE\"]),\n    MAE = c(acc_auto[\"MAE\"], acc_manual[\"MAE\"], acc_benchmark[\"MAE\"]),\n    MAPE = c(acc_auto[\"MAPE\"], acc_manual[\"MAPE\"], acc_benchmark[\"MAPE\"])\n)\n\n# Display formatted table\nkable(cv_results,\n    format = \"html\",\n    digits = 3,\n    caption = \"Cross-Validation Results: ORtg Models\",\n    col.names = c(\"Model\", \"RMSE\", \"MAE\", \"MAPE\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(which.min(cv_results$RMSE), bold = TRUE, color = \"white\", background = \"#006bb6\")\n\n\n\n\nCross-Validation Results: ORtg Models\n\n\nModel\nRMSE\nMAE\nMAPE\n\n\n\n\nARIMAX\n1.400\n1.267\n1.109\n\n\nARIMAX\n1.400\n1.267\n1.109\n\n\nARIMA\n5.665\n5.319\n4.655\n\n\n\n\n\n\n\n\nCode\n# Determine best model\nbest_idx &lt;- which.min(cv_results$RMSE)\ncat(\"\\n\\n*** BEST MODEL: \", cv_results$Model[best_idx], \" ***\\n\")\n\n\n\n\n*** BEST MODEL:  ARIMAX  ***\n\n\nCode\ncat(\"RMSE =\", round(cv_results$RMSE[best_idx], 3), \"\\n\")\n\n\nRMSE = 1.4 \n\n\nCode\n# Select best model based on CV\nif (cv_results$Model[best_idx] == \"ARIMAX (auto.arima)\") {\n    final_arimax &lt;- arimax_auto\n} else if (cv_results$Model[best_idx] == \"ARIMAX (manual)\") {\n    final_arimax &lt;- arimax_manual\n} else {\n    final_arimax &lt;- auto.arima(ts_ortg, seasonal = FALSE)\n}\n\n\nWhat Cross-Validation Reveals\nThe winner is ARIMAX with RMSE = 1.4. This confirms that exogenous variables add real predictive power.\nThe analytics revolution is quantifiable: shot selection and shooting skill aren’t just correlated with efficiency, they predict it.\n\n\n\n\nCode\n# Refit winning model on full data\nif (\"xreg\" %in% names(final_arimax$call)) {\n    final_fit &lt;- Arima(ts_ortg,\n        order = arimaorder(final_arimax)[1:3],\n        xreg = xreg_matrix\n    )\n} else {\n    final_fit &lt;- Arima(ts_ortg, order = arimaorder(final_arimax)[1:3])\n}\n\nprint(summary(final_fit))\n\n\nSeries: ts_ortg \nARIMA(0,1,0) \n\nsigma^2 = 2.025:  log likelihood = -77.95\nAIC=157.9   AICc=157.99   BIC=159.68\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.2030613 1.407018 1.101305 0.1760845 1.028496 0.9798637\n                   ACF1\nTraining set -0.2573027\n\n\nCode\n# Also fit ARIMAX model for demonstration\ncat(\"\\n\\n=== For Comparison: ARIMAX Model with Exogenous Variables ===\\n\")\n\n\n\n\n=== For Comparison: ARIMAX Model with Exogenous Variables ===\n\n\nCode\nfinal_fit_arimax &lt;- Arima(ts_ortg,\n    order = arimaorder(arimax_auto)[1:3],\n    xreg = xreg_matrix\n)\nprint(summary(final_fit_arimax))\n\n\nSeries: ts_ortg \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept     3PAr       TS%\n      0.6680     8.7816  -1.9290  183.2426\ns.e.  0.1157     6.7908   2.3704   12.9989\n\nsigma^2 = 0.3861:  log likelihood = -40.64\nAIC=91.28   AICc=92.82   BIC=100.31\n\nTraining set error measures:\n                    ME      RMSE      MAE       MPE      MAPE      MASE\nTraining set 0.0240792 0.5931024 0.472989 0.0181601 0.4400635 0.4208324\n                   ACF1\nTraining set -0.1807397\n\n\nModel Equations:\n\nWinning Model (Selected by Cross-Validation)\nARIMA(0,1,0) model:\n\\[\n(1 - B) \\text{ORtg}_t = \\epsilon_t\n\\]\n\n\n\n\n\nCode\nif (\"xreg\" %in% names(final_fit$call)) {\n    # Forecast 3PAr and TS%\n    fc_3par &lt;- forecast(auto.arima(ts_3par), h = 5)\n    fc_tspct &lt;- forecast(auto.arima(ts_tspct), h = 5)\n\n    # Create future xreg matrix\n    xreg_future &lt;- cbind(\n        `3PAr` = fc_3par$mean,\n        `TS%` = fc_tspct$mean\n    )\n\n    # Forecast ORtg\n    fc_final &lt;- forecast(final_fit, xreg = xreg_future, h = 5)\n} else {\n    fc_final &lt;- forecast(final_fit, h = 5)\n}\n\n# Plot forecast\nautoplot(fc_final) +\n    labs(\n        title = \"ORtg Forecast: 2026-2030 (ARIMAX Model)\",\n        subtitle = paste0(\"Model: \", paste0(final_fit), \" | Using forecasted 3PAr and TS% as exogenous inputs\"),\n        x = \"Year\",\n        y = \"Offensive Rating (Points per 100 Possessions)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.subtitle = element_text(size = 9))\n\n\n\n\n\n\n\n\n\nCode\ncat(\"\\nPoint Forecasts:\\n\")\n\n\n\nPoint Forecasts:\n\n\nCode\nprint(fc_final$mean)\n\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 114.5323 114.5323 114.5323 114.5323 114.5323\n\n\nCode\ncat(\"\\n\\n80% Prediction Interval:\\n\")\n\n\n\n\n80% Prediction Interval:\n\n\nCode\nprint(fc_final$lower[, 1])\n\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 112.7087 111.9534 111.3738 110.8852 110.4547\n\n\nCode\nprint(fc_final$upper[, 1])\n\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 116.3558 117.1111 117.6907 118.1793 118.6098\n\n\n\n\nThe ARIMAX model tells a compelling story about modern basketball’s transformation.\nThe Analytics Advantage\nInterestingly, adding exogenous variables did not improve forecast accuracy in cross-validation. This suggests high multicollinearity between ORtg and its predictors.\nForecast Performance\nThe model achieved a test RMSE of 1.4 points per 100 possessions, corresponding to approximately 1.1% average error. To put this in context, the difference between the best and worst offense in 2024 was about 15 points per 100 possessions.\nWhat the Future Holds\nForecasts rely purely on historical ORtg patterns, capturing the league’s 45-year trajectory toward more efficient offense. The trend is clear, but the mechanism remains in the residuals.\nThe Basketball Insight\nHere’s what matters for teams: offensive efficiency isn’t magic, it’s a function of where you shoot (3PAr) and how well you shoot (TS%). The model equation makes this quantitative:\n\\[\n\\text{ORtg}_t = \\beta_0 + \\beta_1 \\times \\text{3PAr}_t + \\beta_2 \\times \\text{TS\\%}_t + N_t\n\\]\nwhere \\(N_t\\) captures autocorrelated shocks (momentum, injuries, schedule strength).\nTeams have two levers:\n\nStrategic: Shoot more threes (reallocate shot distribution)\nDevelopmental: Improve shooting accuracy (player development, coaching)\n\nThe analytics revolution validated a simple truth: efficiency is measurable and improvable.",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#covid-impact-on-attendance-arimax-with-intervention",
    "href": "multiTS_model.html#covid-impact-on-attendance-arimax-with-intervention",
    "title": "Multivariate Time Series Modeling",
    "section": "COVID Impact on Attendance (ARIMAX with Intervention)",
    "text": "COVID Impact on Attendance (ARIMAX with Intervention)\n\nResponse Variable: Total_Attendance\nExogenous Variables: ORtg, Pace, COVID_Dummy (pulse intervention)\n\nThe model includes three key variables: ORtg captures how better offensive performance leads to more entertaining games and higher attendance; Pace reflects whether faster games attract more fans; and the COVID_Dummy captures the structural break in 2020-2021 from empty arenas and capacity restrictions.\n\nEDAModel FittingCross-ValidationFinal Model & Interpretation\n\n\n\n\nCode\nleague_post2000 &lt;- league_avg %&gt;% filter(Season &gt;= 2000)\n\nts_attend &lt;- ts(league_post2000$Total_Attendance, start = 2000, frequency = 1)\nts_ortg_sub &lt;- ts(league_post2000$ORtg, start = 2000, frequency = 1)\nts_pace_sub &lt;- ts(league_post2000$Pace, start = 2000, frequency = 1)\nts_covid &lt;- ts(league_post2000$COVID_Dummy, start = 2000, frequency = 1)\n\np1 &lt;- autoplot(ts_attend / 1e6) +\n    labs(title = \"Total NBA Attendance\", y = \"Attendance (Millions)\") +\n    geom_vline(xintercept = 2020, linetype = \"dashed\", color = \"red\") +\n    annotate(\"text\", x = 2020, y = 23, label = \"COVID-19\", color = \"red\", hjust = -0.1) +\n    theme_minimal()\n\np2 &lt;- autoplot(ts_ortg_sub) +\n    labs(title = \"Offensive Rating\", y = \"ORtg\") +\n    theme_minimal()\n\np3 &lt;- autoplot(ts_pace_sub) +\n    labs(title = \"Pace\", y = \"Pace\") +\n    theme_minimal()\n\np1 / (p2 | p3)\n\n\n\n\n\n\n\n\n\nThe attendance plot tells a stark before-and-after story: from 2000-2019, the NBA showed remarkable stability around 22 million attendees per season as a reliable entertainment product. In 2020, attendance collapsed to essentially zero due to empty arenas, the bubble, and capacity restrictions. From 2021-2025, gradual recovery began but with visible scars. Meanwhile, ORtg and Pace continued their upward trends during COVID; games were still played, analytics still mattered, but no one was there to watch.\n\n\n\n\nCode\n# Prepare exogenous matrix\nxreg_attend &lt;- cbind(\n    ORtg = ts_ortg_sub,\n    Pace = ts_pace_sub,\n    COVID = ts_covid\n)\n\n# auto.arima() method\narimax_attend_auto &lt;- auto.arima(ts_attend, xreg = xreg_attend, seasonal = FALSE)\n\nprint(arimax_attend_auto)\n\n\nSeries: ts_attend \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n           ORtg      Pace      COVID\n      -105929.9  354412.4  -13855332\ns.e.   243282.0  278593.1    1905082\n\nsigma^2 = 6.555e+12:  log likelihood = -418.94\nAIC=845.89   AICc=847.79   BIC=850.92\n\n\nCode\n# Diagnostics\ncheckresiduals(arimax_attend_auto)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 4.1469, df = 5, p-value = 0.5285\n\nModel df: 0.   Total lags used: 5\n\n\n\n\nCode\n# Manual method: Regression + ARIMA\ndf_attend &lt;- data.frame(\n    Attendance = as.numeric(ts_attend),\n    ORtg = as.numeric(ts_ortg_sub),\n    Pace = as.numeric(ts_pace_sub),\n    COVID = as.numeric(ts_covid)\n)\n\nlm_attend &lt;- lm(Attendance ~ ORtg + Pace + COVID, data = df_attend)\n\ncat(\"Regression Results:\\n\")\n\n\nRegression Results:\n\n\nCode\nsummary(lm_attend)\n\n\n\nCall:\nlm(formula = Attendance ~ ORtg + Pace + COVID, data = df_attend)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-7856805  -474084   116291   715580  7856805 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1342578   16854399   0.080    0.937    \nORtg          -113325     280214  -0.404    0.690    \nPace           348598     311438   1.119    0.275    \nCOVID       -13793998    2208635  -6.245 2.76e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2617000 on 22 degrees of freedom\nMultiple R-squared:  0.658, Adjusted R-squared:  0.6114 \nF-statistic: 14.11 on 3 and 22 DF,  p-value: 2.398e-05\n\n\nThe regression reveals COVID’s devastating impact in stark numerical terms:\n\\[\n\\beta_{\\text{COVID}} = -13,793,998\n\\]\nThe pandemic reduced attendance by approximately 13.8 million attendees during 2020-2021. For context, total pre-pandemic attendance was around 22 million per season, this represents a near-complete collapse.\n\n\nARIMA model for residuals:\n\n\nSeries: resid_attend \nARIMA(0,0,1) with zero mean \n\nCoefficients:\n          ma1\n      -0.5235\ns.e.   0.2180\n\nsigma^2 = 4.872e+12:  log likelihood = -416.33\nAIC=836.67   AICc=837.19   BIC=839.18\n\n\nSeries: ts_attend \nRegression with ARIMA(0,0,1) errors \n\nCoefficients:\n          ma1  intercept       ORtg       Pace      COVID\n      -1.0000    5870566  -71441.76  253945.85  -14057500\ns.e.   0.1124    6486612  104863.14   98731.21    1384693\n\nsigma^2 = 4.513e+12:  log likelihood = -414.56\nAIC=841.12   AICc=845.54   BIC=848.67\n\n\n\n\n\n\nCode\ntrain_end_att &lt;- 2018\ntest_start_att &lt;- 2019\n\ntrain_attend &lt;- window(ts_attend, end = train_end_att)\ntrain_ortg_a &lt;- window(ts_ortg_sub, end = train_end_att)\ntrain_pace_a &lt;- window(ts_pace_sub, end = train_end_att)\ntrain_covid_a &lt;- window(ts_covid, end = train_end_att)\n\ntest_attend &lt;- window(ts_attend, start = test_start_att)\ntest_ortg_a &lt;- window(ts_ortg_sub, start = test_start_att)\ntest_pace_a &lt;- window(ts_pace_sub, start = test_start_att)\ntest_covid_a &lt;- window(ts_covid, start = test_start_att)\n\nh_att &lt;- length(test_attend)\n\nxreg_train_att &lt;- cbind(ORtg = train_ortg_a, Pace = train_pace_a, COVID = train_covid_a)\nxreg_test_att &lt;- cbind(ORtg = test_ortg_a, Pace = test_pace_a, COVID = test_covid_a)\n\n\n# Model 1: auto.arima() - use simpler constraints for small dataset\nfit_auto_att &lt;- tryCatch(\n    {\n        auto.arima(train_attend,\n            xreg = xreg_train_att, seasonal = FALSE,\n            max.p = 2, max.q = 2, max.d = 1, stepwise = TRUE\n        )\n    },\n    error = function(e) {\n        xreg_no_covid &lt;- cbind(ORtg = train_ortg_a, Pace = train_pace_a)\n        auto.arima(train_attend,\n            xreg = xreg_no_covid, seasonal = FALSE,\n            max.p = 2, max.q = 2, max.d = 1\n        )\n    }\n)\n\n# Model 2: Manual method - use simpler order if needed\nfit_manual_att &lt;- tryCatch(\n    {\n        Arima(train_attend,\n            order = arimaorder(arima_resid_attend)[1:3],\n            xreg = xreg_train_att\n        )\n    },\n    error = function(e) {\n        xreg_no_covid &lt;- cbind(ORtg = train_ortg_a, Pace = train_pace_a)\n        Arima(train_attend, order = c(0, 1, 0), xreg = xreg_no_covid)\n    }\n)\n\n# Model 3: Benchmark\nfit_bench_att &lt;- auto.arima(train_attend, seasonal = FALSE, max.p = 2, max.q = 2)\n\n# Forecasts - handle different xreg structures\nif (\"COVID\" %in% names(coef(fit_auto_att))) {\n    fc_auto_att &lt;- forecast(fit_auto_att, xreg = xreg_test_att, h = h_att)\n} else {\n    xreg_test_no_covid &lt;- cbind(ORtg = test_ortg_a, Pace = test_pace_a)\n    fc_auto_att &lt;- forecast(fit_auto_att, xreg = xreg_test_no_covid, h = h_att)\n}\n\nif (\"COVID\" %in% names(coef(fit_manual_att))) {\n    fc_manual_att &lt;- forecast(fit_manual_att, xreg = xreg_test_att, h = h_att)\n} else {\n    xreg_test_no_covid &lt;- cbind(ORtg = test_ortg_a, Pace = test_pace_a)\n    fc_manual_att &lt;- forecast(fit_manual_att, xreg = xreg_test_no_covid, h = h_att)\n}\n\nfc_bench_att &lt;- forecast(fit_bench_att, h = h_att)\n\n# Accuracy\nacc_auto_att &lt;- accuracy(fc_auto_att, test_attend)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_manual_att &lt;- accuracy(fc_manual_att, test_attend)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_bench_att &lt;- accuracy(fc_bench_att, test_attend)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"\\n=== ATTENDANCE MODEL CROSS-VALIDATION (Test: 2019-2024) ===\\n\\n\")\n\n\n\n=== ATTENDANCE MODEL CROSS-VALIDATION (Test: 2019-2024) ===\n\n\nCode\ncv_att_results &lt;- data.frame(\n    Model = c(\"ARIMAX\", \"ARIMAX\", \"ARIMA\"),\n    RMSE = c(acc_auto_att[\"RMSE\"], acc_manual_att[\"RMSE\"], acc_bench_att[\"RMSE\"]),\n    MAE = c(acc_auto_att[\"MAE\"], acc_manual_att[\"MAE\"], acc_bench_att[\"MAE\"]),\n    MAPE = c(acc_auto_att[\"MAPE\"], acc_manual_att[\"MAPE\"], acc_bench_att[\"MAPE\"])\n)\n\n# Display formatted table with RMSE in millions\ncv_att_display &lt;- cv_att_results\ncv_att_display$RMSE &lt;- cv_att_display$RMSE / 1e6\ncv_att_display$MAE &lt;- cv_att_display$MAE / 1e6\n\nkable(cv_att_display,\n    format = \"html\",\n    digits = 3,\n    caption = \"Cross-Validation Results: Attendance Models (Test Set: 2019-2024)\",\n    col.names = c(\"Model\", \"RMSE (Millions)\", \"MAE (Millions)\", \"MAPE (%)\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(which.min(cv_att_results$RMSE), bold = TRUE, color = \"white\", background = \"#006bb6\")\n\n\n\n\nCross-Validation Results: Attendance Models (Test Set: 2019-2024)\n\n\nModel\nRMSE (Millions)\nMAE (Millions)\nMAPE (%)\n\n\n\n\nARIMAX\n9.268\n5.840\n227.594\n\n\nARIMAX\n9.607\n6.436\n234.399\n\n\nARIMA\n7.815\n4.195\n194.025\n\n\n\n\n\n\n\n\nCode\nbest_idx_att &lt;- which.min(cv_att_results$RMSE)\ncat(\"\\n*** BEST MODEL: \", cv_att_results$Model[best_idx_att], \"***\\n\")\n\n\n\n*** BEST MODEL:  ARIMA ***\n\n\nWhen fitted on full data (2000-2025), the COVID dummy captures the unprecedented shock effectively.\n\n\n\n\nCode\n# Refit best model on full data\nif (cv_att_results$Model[best_idx_att] == \"ARIMAX (auto)\") {\n    final_attend &lt;- Arima(ts_attend,\n        order = arimaorder(arimax_attend_auto)[1:3],\n        xreg = xreg_attend\n    )\n} else if (cv_att_results$Model[best_idx_att] == \"ARIMAX (manual)\") {\n    final_attend &lt;- arimax_attend_manual\n} else {\n    final_attend &lt;- auto.arima(ts_attend, seasonal = FALSE)\n}\n\ncat(\"Final Attendance Model:\\n\")\n\n\nFinal Attendance Model:\n\n\nCode\nprint(summary(final_attend))\n\n\nSeries: ts_attend \nARIMA(0,0,0) with non-zero mean \n\nCoefficients:\n            mean\n      20953026.5\ns.e.    807373.2\n\nsigma^2 = 1.763e+13:  log likelihood = -432.89\nAIC=869.78   AICc=870.3   BIC=872.29\n\nTraining set error measures:\n                        ME    RMSE     MAE       MPE     MAPE      MASE\nTraining set -6.196877e-09 4116988 2045563 -45.69258 54.75919 0.9069228\n                 ACF1\nTraining set 0.163378\n\n\nCode\n# Forecast\nfc_ortg_fut &lt;- forecast(auto.arima(ts_ortg_sub), h = 5)\nfc_pace_fut &lt;- forecast(auto.arima(ts_pace_sub), h = 5)\n\n# Create xreg based on what variables the model has\nif (\"COVID\" %in% names(coef(final_attend))) {\n    xreg_future_att &lt;- cbind(\n        ORtg = fc_ortg_fut$mean,\n        Pace = fc_pace_fut$mean,\n        COVID = rep(0, 5)\n    )\n} else {\n    xreg_future_att &lt;- cbind(\n        ORtg = fc_ortg_fut$mean,\n        Pace = fc_pace_fut$mean\n    )\n}\n\nfc_attend_final &lt;- forecast(final_attend, xreg = xreg_future_att, h = 5)\n\nautoplot(fc_attend_final) +\n    labs(\n        title = \"NBA Attendance Forecast: 2026-2030\",\n        x = \"Year\",\n        y = \"Total Attendance (Millions)\"\n    ) +\n    scale_y_continuous(labels = function(x) x / 1e6) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nHere’s where cross-validation reveals a hard truth about forecasting: the COVID dummy was all zeros in our training data (2000-2018) because the pandemic didn’t exist yet. The model couldn’t learn what it hadn’t seen.\nWhen the test period arrived (2019-2024), actual attendance plummeted by 90% in 2020. However our model, trained on pre-pandemic patterns, had no mechanism to anticipate this. This demonstrates the fundamental challenge of time series forecasting: external shocks that have never occurred before are, by definition, unforecastable.\nThe model relies purely on time-series patterns, revealing that attendance follows its own momentum: yesterday’s crowds predict tomorrow’s. This makes sense; season ticket holders commit months in advance, and casual fans follow habits more than real-time performance metrics.",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#efficiency-drivers-var",
    "href": "multiTS_model.html#efficiency-drivers-var",
    "title": "Multivariate Time Series Modeling",
    "section": "Efficiency Drivers (VAR)",
    "text": "Efficiency Drivers (VAR)\nVariables: ORtg, Pace, 3PAr\nThe theoretical rationale for this VAR model involves multiple bidirectional relationships: faster tempo (Pace) creates more transition opportunities favoring quick 3PT attempts (3PAr), while teams shooting more 3s may adopt faster pace to maximize possessions. Similarly, efficient offense (ORtg) may enable teams to control tempo (Pace), while higher pace may increase transition scoring efficiency (ORtg). We use VAR rather than ARIMAX because we do not assume unidirectional causality.\n\nEDA & StationarityModel Selection & FittingCross-ValidationFinal Model & ForecastInterpretation\n\n\n\n\nCode\n# Create VAR dataset\nvar_data &lt;- ts(league_avg %&gt;% dplyr::select(ORtg, Pace, `3PAr`),\n    start = 1980, frequency = 1\n)\n\n# Plot all series\nautoplot(var_data, facets = TRUE) +\n    labs(\n        title = \"VAR Variables: ORtg, Pace, 3PAr (1980-2025)\",\n        x = \"Year\", y = \"Value\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Summary Statistics:\\n\")\n\n\nSummary Statistics:\n\n\nCode\nsummary(var_data)\n\n\n      ORtg            Pace             3PAr        \n Min.   :102.2   Min.   : 88.92   Min.   :0.02292  \n 1st Qu.:105.8   1st Qu.: 91.81   1st Qu.:0.08761  \n Median :107.5   Median : 95.77   Median :0.19584  \n Mean   :107.5   Mean   : 95.65   Mean   :0.19578  \n 3rd Qu.:108.2   3rd Qu.: 99.18   3rd Qu.:0.25958  \n Max.   :115.3   Max.   :103.06   Max.   :0.42119  \n\n\n\n\nCode\n# ADF tests for each series\nadf_ortg_var &lt;- adf.test(var_data[, \"ORtg\"])\nadf_pace_var &lt;- adf.test(var_data[, \"Pace\"])\nadf_3par_var &lt;- adf.test(var_data[, \"3PAr\"])\n\ncat(\n    \"ORtg: ADF p-value =\", round(adf_ortg_var$p.value, 4),\n    ifelse(adf_ortg_var$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\"\n)\n\n\nORtg: ADF p-value = 0.9233 (non-stationary) \n\n\nCode\ncat(\n    \"Pace: ADF p-value =\", round(adf_pace_var$p.value, 4),\n    ifelse(adf_pace_var$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\"\n)\n\n\nPace: ADF p-value = 0.8116 (non-stationary) \n\n\nCode\ncat(\n    \"3PAr: ADF p-value =\", round(adf_3par_var$p.value, 4),\n    ifelse(adf_3par_var$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\\n\"\n)\n\n\n3PAr: ADF p-value = 0.8303 (non-stationary) \n\n\nCode\n# Difference if non-stationary\nif (adf_ortg_var$p.value &gt; 0.05 | adf_pace_var$p.value &gt; 0.05 | adf_3par_var$p.value &gt; 0.05) {\n    var_data_diff &lt;- diff(var_data)\n\n    # Test differenced data\n    adf_ortg_diff &lt;- adf.test(var_data_diff[, \"ORtg\"])\n    adf_pace_diff &lt;- adf.test(var_data_diff[, \"Pace\"])\n    adf_3par_diff &lt;- adf.test(var_data_diff[, \"3PAr\"])\n\n    cat(\"\\nAfter first-differencing:\\n\")\n    cat(\"ORtg: ADF p-value =\", round(adf_ortg_diff$p.value, 4), \"\\n\")\n    cat(\"Pace: ADF p-value =\", round(adf_pace_diff$p.value, 4), \"\\n\")\n    cat(\"3PAr: ADF p-value =\", round(adf_3par_diff$p.value, 4), \"\\n\")\n\n    if (adf_ortg_diff$p.value &lt; 0.05 & adf_pace_diff$p.value &lt; 0.05 & adf_3par_diff$p.value &lt; 0.05) {\n        var_data_final &lt;- var_data_diff\n        differenced &lt;- TRUE\n    } else {\n        var_data_final &lt;- var_data_diff\n        differenced &lt;- TRUE\n    }\n} else {\n    var_data_final &lt;- var_data\n    differenced &lt;- FALSE\n}\n\n\n\nAfter first-differencing:\nORtg: ADF p-value = 0.109 \nPace: ADF p-value = 0.187 \n3PAr: ADF p-value = 0.0446 \n\n\n\n\n\n\nCode\n# Determine optimal lag order\nvar_select &lt;- VARselect(var_data_final, lag.max = 8, type = \"const\")\n\nprint(var_select$selection)\n\n\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     1      1      1      1 \n\n\nCode\nprint(var_select$criteria)\n\n\n                  1            2            3            4            5\nAIC(n) -6.680901553 -6.500274539 -6.382532287 -6.281073692 -5.992741694\nHQ(n)  -6.496671379 -6.177871734 -5.921956852 -5.682325625 -5.255820997\nSC(n)  -6.153061907 -5.576555158 -5.062933172 -4.565594842 -3.881383109\nFPE(n)  0.001258119  0.001525812  0.001768605  0.002072992  0.003049206\n                  6            7            8\nAIC(n) -6.029048961 -6.000186138 -5.898738412\nHQ(n)  -5.153955634 -4.986920179 -4.747299824\nSC(n)  -3.521810642 -3.097068084 -2.599740624\nFPE(n)  0.003436313  0.004504422  0.007252063\n\n\nCode\n# Fit models with different lag orders\nlags_to_fit &lt;- unique(var_select$selection[1:3])\n\n# Ensure we have at least one lag to fit\nif (length(lags_to_fit) == 0 || any(is.na(lags_to_fit))) {\n    lags_to_fit &lt;- 1\n}\n\ncat(\"\\nFitting VAR models with p =\", paste(lags_to_fit, collapse = \", \"), \"\\n\")\n\n\n\nFitting VAR models with p = 1 \n\n\nCode\nvar_models &lt;- list()\nfor (p in lags_to_fit) {\n    var_models[[paste0(\"VAR_\", p)]] &lt;- VAR(var_data_final, p = p, type = \"const\")\n}\n\n\n\n\nCode\nfor (name in names(var_models)) {\n    cat(\"========================================\\n\")\n    cat(name, \"Summary:\\n\")\n    cat(\"========================================\\n\\n\")\n    print(summary(var_models[[name]]))\n    cat(\"\\n\\n\")\n}\n\n\n========================================\nVAR_1 Summary:\n========================================\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: ORtg, Pace, X3PAr \nDeterministic variables: const \nSample size: 43 \nLog Likelihood: -24.745 \nRoots of the characteristic polynomial:\n0.3236 0.1471 0.1355\nCall:\nVAR(y = var_data_final, p = p, type = \"const\")\n\n\nEstimation results for equation ORtg: \n===================================== \nORtg = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)  \nORtg.l1  -0.38350    0.15583  -2.461   0.0184 *\nPace.l1   0.17639    0.15459   1.141   0.2608  \nX3PAr.l1 29.14282   14.02867   2.077   0.0444 *\nconst     0.02675    0.23554   0.114   0.9102  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.349 on 39 degrees of freedom\nMultiple R-Squared: 0.1733, Adjusted R-squared: 0.1097 \nF-statistic: 2.724 on 3 and 39 DF,  p-value: 0.05723 \n\n\nEstimation results for equation Pace: \n===================================== \nPace = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)\nORtg.l1  -0.03026    0.16414  -0.184    0.855\nPace.l1  -0.11711    0.16283  -0.719    0.476\nX3PAr.l1  6.57227   14.77673   0.445    0.659\nconst    -0.10617    0.24810  -0.428    0.671\n\n\nResidual standard error: 1.421 on 39 degrees of freedom\nMultiple R-Squared: 0.02201,    Adjusted R-squared: -0.05322 \nF-statistic: 0.2925 on 3 and 39 DF,  p-value: 0.8305 \n\n\nEstimation results for equation X3PAr: \n====================================== \nX3PAr = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n           Estimate Std. Error t value Pr(&gt;|t|)   \nORtg.l1  -0.0008257  0.0018756  -0.440   0.6622   \nPace.l1   0.0011681  0.0018606   0.628   0.5338   \nX3PAr.l1  0.1654261  0.1688517   0.980   0.3333   \nconst     0.0080410  0.0028350   2.836   0.0072 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.01623 on 39 degrees of freedom\nMultiple R-Squared: 0.02972,    Adjusted R-squared: -0.04492 \nF-statistic: 0.3982 on 3 and 39 DF,  p-value: 0.7551 \n\n\n\nCovariance matrix of residuals:\n          ORtg      Pace      X3PAr\nORtg  1.818853  0.407221  0.0052772\nPace  0.407221  2.018000 -0.0020829\nX3PAr 0.005277 -0.002083  0.0002635\n\nCorrelation matrix of residuals:\n        ORtg     Pace    X3PAr\nORtg  1.0000  0.21255  0.24106\nPace  0.2126  1.00000 -0.09033\nX3PAr 0.2411 -0.09033  1.00000\n\n\n\n\n\n\nCode\n# Split: Train 1980-2019, Test 2020-2024\ntrain_end_var &lt;- 2019\n\nif (differenced) {\n    train_var &lt;- window(var_data_final, end = train_end_var)\n    test_var &lt;- window(var_data_final, start = train_end_var + 1)\n} else {\n    train_var &lt;- window(var_data_final, end = train_end_var)\n    test_var &lt;- window(var_data_final, start = train_end_var + 1)\n}\n\nh_var &lt;- nrow(test_var)\n\n# Fit VAR models on training data with error handling\nvar_train_models &lt;- list()\nfor (p in lags_to_fit) {\n    model &lt;- tryCatch(\n        {\n            VAR(train_var, p = p, type = \"const\")\n        },\n        error = function(e) {\n            if (p &gt; 1) {\n                VAR(train_var, p = 1, type = \"const\")\n            } else {\n                NULL\n            }\n        }\n    )\n    if (!is.null(model)) {\n        var_train_models[[paste0(\"VAR_\", p)]] &lt;- model\n    }\n}\n\n# Generate forecasts\nrmse_results &lt;- data.frame()\n\ncat(\"Evaluating\", length(var_train_models), \"VAR model(s)\\n\")\n\n\nEvaluating 1 VAR model(s)\n\n\nCode\nfor (name in names(var_train_models)) {\n    fc &lt;- tryCatch(\n        {\n            predict(var_train_models[[name]], n.ahead = h_var)\n        },\n        error = function(e) {\n            cat(\"Warning: Forecast failed for\", name, \":\", e$message, \"\\n\")\n            NULL\n        }\n    )\n\n    if (is.null(fc)) next\n\n    fc_var_names &lt;- names(fc$fcst)\n\n    # Find ORtg\n    fc_ortg &lt;- fc$fcst$ORtg[, \"fcst\"]\n\n    # Find Pace\n    fc_pace &lt;- fc$fcst$Pace[, \"fcst\"]\n\n    # Find 3PAr (might be stored with backticks or without)\n    tpar_fc_name &lt;- fc_var_names[grep(\"3PAr|PAr\", fc_var_names, ignore.case = TRUE)]\n    if (length(tpar_fc_name) == 0) {\n        tpar_fc_name &lt;- fc_var_names[3] # Default to third variable\n    } else {\n        tpar_fc_name &lt;- tpar_fc_name[1] # Take first match\n    }\n    fc_3par &lt;- fc$fcst[[tpar_fc_name]][, \"fcst\"]\n\n    test_var_names &lt;- colnames(test_var)\n    test_ortg_vec &lt;- as.numeric(test_var[, \"ORtg\"])\n    test_pace_vec &lt;- as.numeric(test_var[, \"Pace\"])\n\n    tpar_test_name &lt;- test_var_names[grep(\"3PAr|PAr\", test_var_names, ignore.case = TRUE)]\n    if (length(tpar_test_name) == 0) {\n        tpar_test_name &lt;- test_var_names[3]\n    } else {\n        tpar_test_name &lt;- tpar_test_name[1]\n    }\n    test_3par_vec &lt;- as.numeric(test_var[, tpar_test_name])\n\n    # Ensure equal lengths (forecasts might be shorter if h_var is large)\n    n_compare &lt;- min(length(test_ortg_vec), length(fc_ortg), length(fc_pace), length(fc_3par))\n\n    # Calculate RMSE for each variable\n    rmse_ortg &lt;- sqrt(mean((test_ortg_vec[1:n_compare] - fc_ortg[1:n_compare])^2, na.rm = TRUE))\n    rmse_pace &lt;- sqrt(mean((test_pace_vec[1:n_compare] - fc_pace[1:n_compare])^2, na.rm = TRUE))\n    rmse_3par &lt;- sqrt(mean((test_3par_vec[1:n_compare] - fc_3par[1:n_compare])^2, na.rm = TRUE))\n\n    # Average RMSE across variables\n    rmse_avg &lt;- mean(c(rmse_ortg, rmse_pace, rmse_3par), na.rm = TRUE)\n\n    cat(\"  \", name, \": ORtg RMSE =\", round(rmse_ortg, 4),\n        \"| Pace RMSE =\", round(rmse_pace, 4),\n        \"| 3PAr RMSE =\", round(rmse_3par, 4),\n        \"| Avg =\", round(rmse_avg, 4), \"\\n\")\n\n    # Create descriptive model name\n    lag_num &lt;- as.numeric(gsub(\"VAR_\", \"\", name))\n    model_label &lt;- paste0(\"VAR(\", lag_num, \")\")\n\n    rmse_results &lt;- rbind(rmse_results, data.frame(\n        Model = model_label,\n        Lags = lag_num,\n        RMSE_ORtg = rmse_ortg,\n        RMSE_Pace = rmse_pace,\n        RMSE_3PAr = rmse_3par,\n        RMSE_Avg = rmse_avg\n    ))\n}\n\n\n   VAR_1 : ORtg RMSE = 1.3634 | Pace RMSE = 0.8628 | 3PAr RMSE = 0.0126 | Avg = 0.7463 \n\n\nCode\ncat(\"\\n=== CROSS-VALIDATION RESULTS ===\\n\\n\")\n\n\n\n=== CROSS-VALIDATION RESULTS ===\n\n\n\n\nCross-Validation Results: VAR Models (Test Set: 2020-2024)\n\n\nModel\nLags\nRMSE (ORtg)\nRMSE (Pace)\nRMSE (3PAr)\nAvg RMSE\n\n\n\n\nVAR(1)\n1\n1.3634\n0.8628\n0.0126\n0.7463\n\n\n\n\n\n\n\n\nCode\nif (exists(\"rmse_results\") && nrow(rmse_results) &gt; 0) {\n    # Plot RMSEs\n    ggplot(rmse_results, aes(x = factor(Lags), y = RMSE_Avg, fill = Model)) +\n        geom_bar(stat = \"identity\", width = 0.6) +\n        geom_text(aes(label = round(RMSE_Avg, 3)), vjust = -0.5, fontface = \"bold\") +\n        labs(\n            title = \"VAR Model Cross-Validation: Average RMSE\",\n            subtitle = \"Lower RMSE = Better out-of-sample forecast performance\",\n            x = \"Number of Lags (p)\",\n            y = \"Average RMSE across ORtg, Pace, 3PAr\"\n        ) +\n        theme_minimal() +\n        theme(legend.position = \"none\") +\n        scale_fill_brewer(palette = \"Set2\")\n}\n\n\n\n\n\n\n\n\n\n\n\nCode\nif (exists(\"rmse_results\") && nrow(rmse_results) &gt; 0) {\n    best_var_idx &lt;- which.min(rmse_results$RMSE_Avg)\n    cat(\"\\nBest model:\", rmse_results$Model[best_var_idx], \"(Average RMSE =\", round(rmse_results$RMSE_Avg[best_var_idx], 3), \")\\n\")\n} else {\n    best_var_idx &lt;- 1\n}\n\n\n\nBest model: VAR(1) (Average RMSE = 0.746 )\n\n\n\n\n\n\nCode\n# Select best model\nif (exists(\"rmse_results\") && nrow(rmse_results) &gt; 0 && exists(\"best_var_idx\")) {\n    best_var_name &lt;- rmse_results$Model[best_var_idx]\n    best_var_lags &lt;- rmse_results$Lags[best_var_idx]\n} else {\n    best_var_lags &lt;- min(lags_to_fit)\n}\n\ncat(\"Final VAR Model: VAR(\", best_var_lags, \")\\n\\n\", sep = \"\")\n\n\nFinal VAR Model: VAR(1)\n\n\nCode\n# Refit on full data\nfinal_var &lt;- tryCatch(\n    {\n        VAR(var_data_final, p = best_var_lags, type = \"const\")\n    },\n    error = function(e) {\n        VAR(var_data_final, p = 1, type = \"const\")\n    }\n)\n\nprint(summary(final_var))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: ORtg, Pace, X3PAr \nDeterministic variables: const \nSample size: 43 \nLog Likelihood: -24.745 \nRoots of the characteristic polynomial:\n0.3236 0.1471 0.1355\nCall:\nVAR(y = var_data_final, p = best_var_lags, type = \"const\")\n\n\nEstimation results for equation ORtg: \n===================================== \nORtg = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)  \nORtg.l1  -0.38350    0.15583  -2.461   0.0184 *\nPace.l1   0.17639    0.15459   1.141   0.2608  \nX3PAr.l1 29.14282   14.02867   2.077   0.0444 *\nconst     0.02675    0.23554   0.114   0.9102  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.349 on 39 degrees of freedom\nMultiple R-Squared: 0.1733, Adjusted R-squared: 0.1097 \nF-statistic: 2.724 on 3 and 39 DF,  p-value: 0.05723 \n\n\nEstimation results for equation Pace: \n===================================== \nPace = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)\nORtg.l1  -0.03026    0.16414  -0.184    0.855\nPace.l1  -0.11711    0.16283  -0.719    0.476\nX3PAr.l1  6.57227   14.77673   0.445    0.659\nconst    -0.10617    0.24810  -0.428    0.671\n\n\nResidual standard error: 1.421 on 39 degrees of freedom\nMultiple R-Squared: 0.02201,    Adjusted R-squared: -0.05322 \nF-statistic: 0.2925 on 3 and 39 DF,  p-value: 0.8305 \n\n\nEstimation results for equation X3PAr: \n====================================== \nX3PAr = ORtg.l1 + Pace.l1 + X3PAr.l1 + const \n\n           Estimate Std. Error t value Pr(&gt;|t|)   \nORtg.l1  -0.0008257  0.0018756  -0.440   0.6622   \nPace.l1   0.0011681  0.0018606   0.628   0.5338   \nX3PAr.l1  0.1654261  0.1688517   0.980   0.3333   \nconst     0.0080410  0.0028350   2.836   0.0072 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.01623 on 39 degrees of freedom\nMultiple R-Squared: 0.02972,    Adjusted R-squared: -0.04492 \nF-statistic: 0.3982 on 3 and 39 DF,  p-value: 0.7551 \n\n\n\nCovariance matrix of residuals:\n          ORtg      Pace      X3PAr\nORtg  1.818853  0.407221  0.0052772\nPace  0.407221  2.018000 -0.0020829\nX3PAr 0.005277 -0.002083  0.0002635\n\nCorrelation matrix of residuals:\n        ORtg     Pace    X3PAr\nORtg  1.0000  0.21255  0.24106\nPace  0.2126  1.00000 -0.09033\nX3PAr 0.2411 -0.09033  1.00000\n\n\nCode\n# Forecast 5 periods ahead\nfc_var_final &lt;- predict(final_var, n.ahead = 5)\n\n# Get variable names\nfc_var_names &lt;- names(fc_var_final$fcst)\ntpar_fc_name &lt;- fc_var_names[grep(\"3PAr|PAr\", fc_var_names, ignore.case = TRUE)]\nif (length(tpar_fc_name) == 0) tpar_fc_name &lt;- fc_var_names[3]\n\n# Display forecasts\ncat(\"\\n=== 5-Period Forecasts ===\\n\\n\")\n\n\n\n=== 5-Period Forecasts ===\n\n\nCode\ncat(\"ORtg:\\n\")\n\n\nORtg:\n\n\nCode\nprint(fc_var_final$fcst$ORtg)\n\n\n            fcst     lower    upper       CI\n[1,]  1.14541147 -1.497891 3.788714 2.643302\n[2,] -0.01197986 -2.904812 2.880852 2.892832\n[3,]  0.29428871 -2.615367 3.203944 2.909656\n[4,]  0.18514537 -2.726620 3.096911 2.911765\n[5,]  0.21921670 -2.692756 3.131190 2.911973\n\n\nCode\ncat(\"\\n\", tpar_fc_name, \":\\n\", sep = \"\")\n\n\n\nX3PAr:\n\n\nCode\nprint(fc_var_final$fcst[[tpar_fc_name]])\n\n\n            fcst       lower      upper         CI\n[1,] 0.013430804 -0.01838448 0.04524608 0.03181528\n[2,] 0.009377456 -0.02292743 0.04168234 0.03230489\n[3,] 0.009533668 -0.02277694 0.04184428 0.03231061\n[4,] 0.009331515 -0.02297983 0.04164286 0.03231135\n[5,] 0.009375650 -0.02293573 0.04168703 0.03231138\n\n\nCode\ncat(\"\\nPace:\\n\")\n\n\n\nPace:\n\n\nCode\nprint(fc_var_final$fcst$Pace)\n\n\n            fcst     lower    upper       CI\n[1,]  0.05172503 -2.732528 2.835978 2.784253\n[2,] -0.05861195 -2.873542 2.756318 2.814930\n[3,] -0.03730962 -2.852842 2.778223 2.815533\n[4,] -0.04804480 -2.863606 2.767516 2.815561\n[5,] -0.04481374 -2.860377 2.770749 2.815563\n\n\nCode\n# Plot forecasts\nfor (vname in fc_var_names) {\n    plot(fc_var_final, names = vname)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGranger Causality Tests:\n\n\nCode\n# Granger causality tests\nvar_names &lt;- names(final_var$varresult)\ntpar_name &lt;- var_names[grep(\"3PAr|PAr\", var_names, ignore.case = TRUE)]\nif (length(tpar_name) == 0) tpar_name &lt;- var_names[3]\n\ngranger_3par_ortg &lt;- causality(final_var, cause = tpar_name)$Granger\ngranger_pace &lt;- causality(final_var, cause = \"Pace\")$Granger\ngranger_ortg &lt;- causality(final_var, cause = \"ORtg\")$Granger\n\ncat(\"3PAr → {ORtg, Pace}: F =\", round(granger_3par_ortg$statistic, 3),\n    \", p =\", round(granger_3par_ortg$p.value, 4), \"\\n\")\n\n\n3PAr → {ORtg, Pace}: F = 2.158 , p = 0.1202 \n\n\nCode\ncat(\"Pace → {ORtg, 3PAr}: F =\", round(granger_pace$statistic, 3),\n    \", p =\", round(granger_pace$p.value, 4), \"\\n\")\n\n\nPace → {ORtg, 3PAr}: F = 0.717 , p = 0.4903 \n\n\nCode\ncat(\"ORtg → {Pace, 3PAr}: F =\", round(granger_ortg$statistic, 3),\n    \", p =\", round(granger_ortg$p.value, 4), \"\\n\")\n\n\nORtg → {Pace, 3PAr}: F = 0.122 , p = 0.8851 \n\n\nGranger causality tests reveal the temporal ordering of the analytics revolution by determining which variables’ past values predict other variables’ future changes. These tests show whether the rise in three-point shooting preceded changes in offensive efficiency and pace, or whether successful offenses drove teams to adopt more three-pointers, providing empirical evidence about the direction of causation during the NBA’s transformation.\n\n\nCode\nvar_names_irf &lt;- names(final_var$varresult)\ntpar_name_irf &lt;- var_names_irf[grep(\"3PAr|PAr\", var_names_irf, ignore.case = TRUE)]\nif (length(tpar_name_irf) == 0) tpar_name_irf &lt;- var_names_irf[3]\n\n# IRFs\nirf_3par_ortg &lt;- irf(final_var, impulse = tpar_name_irf, response = \"ORtg\", n.ahead = 10)\nplot(irf_3par_ortg, main = paste(\"Impulse:\", tpar_name_irf, \"→ Response: ORtg\"))\n\n\n\n\n\n\n\n\n\nCode\nirf_pace_ortg &lt;- irf(final_var, impulse = \"Pace\", response = \"ORtg\", n.ahead = 10)\nplot(irf_pace_ortg, main = \"Impulse: Pace → Response: ORtg\")\n\n\n\n\n\n\n\n\n\nCode\nirf_ortg_3par &lt;- irf(final_var, impulse = \"ORtg\", response = tpar_name_irf, n.ahead = 10)\nplot(irf_ortg_3par, main = paste(\"Impulse: ORtg → Response:\", tpar_name_irf))\n\n\n\n\n\n\n\n\n\n\n\nThe VAR model reveals meaningful relationships among offensive efficiency, three-point shooting, and pace across NBA history. Granger causality tests quantify whether past values of one variable help predict future values of others, addressing the question of whether the rise in three-point shooting preceded changes in offensive efficiency or vice versa.\nImpulse response functions trace how a one-time shock to one variable cascades through the system, showing whether innovations in one aspect of basketball strategy have lasting effects on others or if defensive adjustments eventually neutralize advantages. Together, these tools demonstrate that while the analytics revolution transformed the NBA, reflecting the complex adaptive nature of elite competition where strategic innovations provoke counter-responses.",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#pace-dynamics-var",
    "href": "multiTS_model.html#pace-dynamics-var",
    "title": "Multivariate Time Series Modeling",
    "section": "Pace Dynamics (VAR)",
    "text": "Pace Dynamics (VAR)\nVariables: Pace, 3PAr, eFG%\nThis VAR model investigates the bidirectional relationships between pace of play, three-point attempt rate, and effective field goal percentage. The theoretical motivation is multifaceted: faster pace may facilitate more three-point attempts through transition opportunities, while teams that shoot more threes may adopt faster tempos to maximize possessions. Additionally, better shooting efficiency (eFG%) may enable teams to control tempo more effectively, while faster pace could create higher-quality shot opportunities through defensive breakdowns. Unlike ARIMAX models that assume one-directional causality, VAR allows all three variables to influence each other dynamically.\n\nEDA & StationarityModel Selection & FittingCross-ValidationFinal Model & ForecastInterpretation\n\n\n\n\nCode\n# Create VAR dataset for pace dynamics\nvar_pace_data &lt;- ts(league_avg %&gt;% dplyr::select(Pace, `3PAr`, `eFG%`),\n    start = 1980, frequency = 1\n)\n\n# Plot all series\nautoplot(var_pace_data, facets = TRUE) +\n    labs(\n        title = \"VAR Variables: Pace, 3PAr, eFG% (1980-2025)\",\n        x = \"Year\", y = \"Value\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Summary Statistics:\\n\")\n\n\nSummary Statistics:\n\n\nCode\nsummary(var_pace_data)\n\n\n      Pace             3PAr              eFG%       \n Min.   : 88.92   Min.   :0.02292   Min.   :0.4659  \n 1st Qu.: 91.81   1st Qu.:0.08761   1st Qu.:0.4877  \n Median : 95.77   Median :0.19584   Median :0.4945  \n Mean   : 95.65   Mean   :0.19578   Mean   :0.4981  \n 3rd Qu.: 99.18   3rd Qu.:0.25958   3rd Qu.:0.5009  \n Max.   :103.06   Max.   :0.42119   Max.   :0.5465  \n\n\n\n\nCode\n# ADF tests for each series\nadf_pace_var2 &lt;- adf.test(var_pace_data[, \"Pace\"])\nadf_3par_var2 &lt;- adf.test(var_pace_data[, \"3PAr\"])\nadf_efg_var2 &lt;- adf.test(var_pace_data[, \"eFG%\"])\n\ncat(\n    \"Pace: ADF p-value =\", round(adf_pace_var2$p.value, 4),\n    ifelse(adf_pace_var2$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\"\n)\n\n\nPace: ADF p-value = 0.8116 (non-stationary) \n\n\nCode\ncat(\n    \"3PAr: ADF p-value =\", round(adf_3par_var2$p.value, 4),\n    ifelse(adf_3par_var2$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\"\n)\n\n\n3PAr: ADF p-value = 0.8303 (non-stationary) \n\n\nCode\ncat(\n    \"eFG%: ADF p-value =\", round(adf_efg_var2$p.value, 4),\n    ifelse(adf_efg_var2$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\\n\"\n)\n\n\neFG%: ADF p-value = 0.9072 (non-stationary) \n\n\nCode\n# Difference if non-stationary\nif (adf_pace_var2$p.value &gt; 0.05 | adf_3par_var2$p.value &gt; 0.05 | adf_efg_var2$p.value &gt; 0.05) {\n    var_pace_data_diff &lt;- diff(var_pace_data)\n\n    # Test differenced data\n    adf_pace_diff &lt;- adf.test(var_pace_data_diff[, \"Pace\"])\n    adf_3par_diff &lt;- adf.test(var_pace_data_diff[, \"3PAr\"])\n    adf_efg_diff &lt;- adf.test(var_pace_data_diff[, \"eFG%\"])\n\n    cat(\"After first-differencing:\\n\")\n    cat(\"Pace: ADF p-value =\", round(adf_pace_diff$p.value, 4), \"\\n\")\n    cat(\"3PAr: ADF p-value =\", round(adf_3par_diff$p.value, 4), \"\\n\")\n    cat(\"eFG%: ADF p-value =\", round(adf_efg_diff$p.value, 4), \"\\n\")\n\n    if (adf_pace_diff$p.value &lt; 0.05 & adf_3par_diff$p.value &lt; 0.05 & adf_efg_diff$p.value &lt; 0.05) {\n        var_pace_data_final &lt;- var_pace_data_diff\n        differenced_pace &lt;- TRUE\n    } else {\n        var_pace_data_final &lt;- var_pace_data_diff\n        differenced_pace &lt;- TRUE\n    }\n} else {\n    var_pace_data_final &lt;- var_pace_data\n    differenced_pace &lt;- FALSE\n}\n\n\nAfter first-differencing:\nPace: ADF p-value = 0.187 \n3PAr: ADF p-value = 0.0446 \neFG%: ADF p-value = 0.0333 \n\n\nThe time series reveal distinct evolutionary patterns. Pace shows the famous U-shape: declining from the fast-paced 1980s through the defensive mid-2000s, then rebounding in the modern era as analytics embraced tempo. Three-point attempt rate explodes post-2012, capturing the shot selection revolution. Effective field goal percentage rises steadily, reflecting improved shooting skill and better shot selection. All three series exhibit non-stationary trends, necessitating differencing for valid VAR estimation.\n\n\n\n\nCode\n# Determine optimal lag order\nvar_pace_select &lt;- VARselect(var_pace_data_final, lag.max = 8, type = \"const\")\n\nprint(var_pace_select$selection)\n\n\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     1      1      1      1 \n\n\nCode\nprint(var_pace_select$criteria)\n\n\n                   1             2             3             4             5\nAIC(n) -1.742587e+01 -1.725714e+01 -1.709067e+01 -1.689179e+01 -1.661036e+01\nHQ(n)  -1.724164e+01 -1.693473e+01 -1.663009e+01 -1.629304e+01 -1.587344e+01\nSC(n)  -1.689803e+01 -1.633342e+01 -1.577107e+01 -1.517631e+01 -1.449900e+01\nFPE(n)  2.711707e-08  3.249790e-08  3.955006e-08  5.110035e-08  7.464754e-08\n                   6             7             8\nAIC(n) -1.659041e+01 -1.642678e+01 -1.653350e+01\nHQ(n)  -1.571532e+01 -1.541351e+01 -1.538207e+01\nSC(n)  -1.408317e+01 -1.352366e+01 -1.323451e+01\nFPE(n)  8.899211e-08  1.334835e-07  1.745185e-07\n\n\nCode\n# Fit models with different lag orders\nlags_pace_to_fit &lt;- unique(var_pace_select$selection[1:3])\n\n# Ensure we have at least one lag to fit\nif (length(lags_pace_to_fit) == 0 || any(is.na(lags_pace_to_fit))) {\n    lags_pace_to_fit &lt;- 1\n}\n\ncat(\"\\nFitting VAR models with p =\", paste(lags_pace_to_fit, collapse = \", \"), \"\\n\")\n\n\n\nFitting VAR models with p = 1 \n\n\nCode\nvar_pace_models &lt;- list()\nfor (p in lags_pace_to_fit) {\n    var_pace_models[[paste0(\"VAR_\", p)]] &lt;- VAR(var_pace_data_final, p = p, type = \"const\")\n}\n\n\n\n\nCode\nfor (name in names(var_pace_models)) {\n    cat(\"========================================\\n\")\n    cat(name, \"Summary:\\n\")\n    cat(\"========================================\\n\\n\")\n    print(summary(var_pace_models[[name]]))\n    cat(\"\\n\\n\")\n}\n\n\n========================================\nVAR_1 Summary:\n========================================\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Pace, X3PAr, eFG. \nDeterministic variables: const \nSample size: 43 \nLog Likelihood: 208.868 \nRoots of the characteristic polynomial:\n0.3072 0.1708 0.1454\nCall:\nVAR(y = var_pace_data_final, p = p, type = \"const\")\n\n\nEstimation results for equation Pace: \n===================================== \nPace = Pace.l1 + X3PAr.l1 + eFG..l1 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)\nPace.l1   -0.1350     0.1729  -0.781    0.440\nX3PAr.l1   4.3987    16.3643   0.269    0.790\neFG..l1    6.0838    38.5944   0.158    0.876\nconst     -0.1038     0.2486  -0.418    0.679\n\n\nResidual standard error: 1.421 on 39 degrees of freedom\nMultiple R-Squared: 0.02178,    Adjusted R-squared: -0.05347 \nF-statistic: 0.2894 on 3 and 39 DF,  p-value: 0.8327 \n\n\nEstimation results for equation X3PAr: \n====================================== \nX3PAr = Pace.l1 + X3PAr.l1 + eFG..l1 + const \n\n          Estimate Std. Error t value Pr(&gt;|t|)   \nPace.l1  0.0009159  0.0019800   0.463  0.64624   \nX3PAr.l1 0.1347711  0.1874200   0.719  0.47637   \neFG..l1  0.0343928  0.4420217   0.078  0.93838   \nconst    0.0080525  0.0028473   2.828  0.00735 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.01627 on 39 degrees of freedom\nMultiple R-Squared: 0.02505,    Adjusted R-squared: -0.04995 \nF-statistic: 0.334 on 3 and 39 DF,  p-value: 0.8008 \n\n\nEstimation results for equation eFG.: \n===================================== \neFG. = Pace.l1 + X3PAr.l1 + eFG..l1 + const \n\n           Estimate Std. Error t value Pr(&gt;|t|)  \nPace.l1   7.028e-04  8.245e-04   0.852   0.3992  \nX3PAr.l1  1.801e-01  7.805e-02   2.308   0.0264 *\neFG..l1  -2.816e-01  1.841e-01  -1.530   0.1341  \nconst     4.086e-06  1.186e-03   0.003   0.9973  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.006776 on 39 degrees of freedom\nMultiple R-Squared: 0.1236, Adjusted R-squared: 0.05617 \nF-statistic: 1.833 on 3 and 39 DF,  p-value: 0.1571 \n\n\n\nCovariance matrix of residuals:\n           Pace      X3PAr      eFG.\nPace   2.018472 -2.042e-03 3.238e-03\nX3PAr -0.002042  2.648e-04 4.819e-05\neFG.   0.003238  4.819e-05 4.591e-05\n\nCorrelation matrix of residuals:\n          Pace    X3PAr   eFG.\nPace   1.00000 -0.08834 0.3364\nX3PAr -0.08834  1.00000 0.4371\neFG.   0.33640  0.43708 1.0000\n\n\n\n\n\n\nCode\n# Split: Train 1980-2019, Test 2020-2024\ntrain_end_var_pace &lt;- 2019\n\nif (differenced_pace) {\n    train_var_pace &lt;- window(var_pace_data_final, end = train_end_var_pace)\n    test_var_pace &lt;- window(var_pace_data_final, start = train_end_var_pace + 1)\n} else {\n    train_var_pace &lt;- window(var_pace_data_final, end = train_end_var_pace)\n    test_var_pace &lt;- window(var_pace_data_final, start = train_end_var_pace + 1)\n}\n\nh_var_pace &lt;- nrow(test_var_pace)\n\n# Fit VAR models on training data with error handling\nvar_pace_train_models &lt;- list()\nfor (p in lags_pace_to_fit) {\n    model &lt;- tryCatch(\n        {\n            VAR(train_var_pace, p = p, type = \"const\")\n        },\n        error = function(e) {\n            if (p &gt; 1) {\n                VAR(train_var_pace, p = 1, type = \"const\")\n            } else {\n                NULL\n            }\n        }\n    )\n    if (!is.null(model)) {\n        var_pace_train_models[[paste0(\"VAR_\", p)]] &lt;- model\n    }\n}\n\n# Generate forecasts\nrmse_pace_results &lt;- data.frame()\n\ncat(\"Evaluating\", length(var_pace_train_models), \"VAR model(s)\\n\")\n\n\nEvaluating 1 VAR model(s)\n\n\nCode\nfor (name in names(var_pace_train_models)) {\n    fc &lt;- tryCatch(\n        {\n            predict(var_pace_train_models[[name]], n.ahead = h_var_pace)\n        },\n        error = function(e) {\n            cat(\"Warning: Forecast failed for\", name, \":\", e$message, \"\\n\")\n            NULL\n        }\n    )\n\n    if (is.null(fc)) next\n\n    # Extract forecasts for each variable\n    fc_var_names_pace &lt;- names(fc$fcst)\n\n    # Find Pace\n    fc_pace_var &lt;- fc$fcst$Pace[, \"fcst\"]\n\n    # Find 3PAr\n    tpar_fc_name_pace &lt;- fc_var_names_pace[grep(\"3PAr|PAr\", fc_var_names_pace, ignore.case = TRUE)]\n    if (length(tpar_fc_name_pace) == 0) {\n        tpar_fc_name_pace &lt;- fc_var_names_pace[2]\n    } else {\n        tpar_fc_name_pace &lt;- tpar_fc_name_pace[1]\n    }\n    fc_3par_pace &lt;- fc$fcst[[tpar_fc_name_pace]][, \"fcst\"]\n\n    # Find eFG%\n    efg_fc_name_pace &lt;- fc_var_names_pace[grep(\"eFG%|eFG\", fc_var_names_pace, ignore.case = TRUE)]\n    if (length(efg_fc_name_pace) == 0) {\n        efg_fc_name_pace &lt;- fc_var_names_pace[3]\n    } else {\n        efg_fc_name_pace &lt;- efg_fc_name_pace[1]\n    }\n    fc_efg_pace &lt;- fc$fcst[[efg_fc_name_pace]][, \"fcst\"]\n\n    # Convert test data to numeric vectors\n    test_var_names_pace &lt;- colnames(test_var_pace)\n    test_pace_vec &lt;- as.numeric(test_var_pace[, \"Pace\"])\n\n    tpar_test_name_pace &lt;- test_var_names_pace[grep(\"3PAr|PAr\", test_var_names_pace, ignore.case = TRUE)]\n    if (length(tpar_test_name_pace) == 0) {\n        tpar_test_name_pace &lt;- test_var_names_pace[2]\n    } else {\n        tpar_test_name_pace &lt;- tpar_test_name_pace[1]\n    }\n    test_3par_pace &lt;- as.numeric(test_var_pace[, tpar_test_name_pace])\n\n    efg_test_name_pace &lt;- test_var_names_pace[grep(\"eFG%|eFG\", test_var_names_pace, ignore.case = TRUE)]\n    if (length(efg_test_name_pace) == 0) {\n        efg_test_name_pace &lt;- test_var_names_pace[3]\n    } else {\n        efg_test_name_pace &lt;- efg_test_name_pace[1]\n    }\n    test_efg_pace &lt;- as.numeric(test_var_pace[, efg_test_name_pace])\n\n    # Ensure equal lengths\n    n_compare &lt;- min(length(test_pace_vec), length(fc_pace_var), length(fc_3par_pace), length(fc_efg_pace))\n\n    # Calculate RMSE for each variable\n    rmse_pace &lt;- sqrt(mean((test_pace_vec[1:n_compare] - fc_pace_var[1:n_compare])^2, na.rm = TRUE))\n    rmse_3par_pace &lt;- sqrt(mean((test_3par_pace[1:n_compare] - fc_3par_pace[1:n_compare])^2, na.rm = TRUE))\n    rmse_efg_pace &lt;- sqrt(mean((test_efg_pace[1:n_compare] - fc_efg_pace[1:n_compare])^2, na.rm = TRUE))\n\n    # Average RMSE across variables\n    rmse_avg_pace &lt;- mean(c(rmse_pace, rmse_3par_pace, rmse_efg_pace), na.rm = TRUE)\n\n    cat(\"  \", name, \": Pace RMSE =\", round(rmse_pace, 4),\n        \"| 3PAr RMSE =\", round(rmse_3par_pace, 4),\n        \"| eFG% RMSE =\", round(rmse_efg_pace, 4),\n        \"| Avg =\", round(rmse_avg_pace, 4), \"\\n\")\n\n    # Create descriptive model name\n    lag_num &lt;- as.numeric(gsub(\"VAR_\", \"\", name))\n    model_label &lt;- paste0(\"VAR(\", lag_num, \")\")\n\n    rmse_pace_results &lt;- rbind(rmse_pace_results, data.frame(\n        Model = model_label,\n        Lags = lag_num,\n        RMSE_Pace = rmse_pace,\n        RMSE_3PAr = rmse_3par_pace,\n        RMSE_eFG = rmse_efg_pace,\n        RMSE_Avg = rmse_avg_pace\n    ))\n}\n\n\n   VAR_1 : Pace RMSE = 0.8628 | 3PAr RMSE = 0.0125 | eFG% RMSE = 0.0074 | Avg = 0.2942 \n\n\nCode\ncat(\"\\n=== CROSS-VALIDATION RESULTS ===\\n\\n\")\n\n\n\n=== CROSS-VALIDATION RESULTS ===\n\n\n\n\nCross-Validation Results: Pace VAR Models (Test Set: 2020-2024)\n\n\nModel\nLags\nRMSE (Pace)\nRMSE (3PAr)\nRMSE (eFG%)\nAvg RMSE\n\n\n\n\nVAR(1)\n1\n0.8628\n0.0125\n0.0074\n0.2942\n\n\n\n\n\n\n\n\nCode\nif (exists(\"rmse_pace_results\") && nrow(rmse_pace_results) &gt; 0) {\n    # Plot RMSEs\n    ggplot(rmse_pace_results, aes(x = factor(Lags), y = RMSE_Avg, fill = Model)) +\n        geom_bar(stat = \"identity\", width = 0.6) +\n        geom_text(aes(label = round(RMSE_Avg, 3)), vjust = -0.5, fontface = \"bold\") +\n        labs(\n            title = \"Pace VAR Model Cross-Validation: Average RMSE\",\n            subtitle = \"Lower RMSE = Better out-of-sample forecast performance\",\n            x = \"Number of Lags (p)\",\n            y = \"Average RMSE across Pace, 3PAr, eFG%\"\n        ) +\n        theme_minimal() +\n        theme(legend.position = \"none\") +\n        scale_fill_brewer(palette = \"Set2\")\n}\n\n\n\n\n\n\n\n\n\n\n\nCode\nif (exists(\"rmse_pace_results\") && nrow(rmse_pace_results) &gt; 0) {\n    best_var_pace_idx &lt;- which.min(rmse_pace_results$RMSE_Avg)\n    cat(\"\\nBest model:\", rmse_pace_results$Model[best_var_pace_idx], \"(Average RMSE =\", round(rmse_pace_results$RMSE_Avg[best_var_pace_idx], 3), \")\\n\")\n} else {\n    best_var_pace_idx &lt;- 1\n}\n\n\n\nBest model: VAR(1) (Average RMSE = 0.294 )\n\n\n\n\n\n\nCode\n# Select best model\nif (exists(\"rmse_pace_results\") && nrow(rmse_pace_results) &gt; 0 && exists(\"best_var_pace_idx\")) {\n    best_var_pace_name &lt;- rmse_pace_results$Model[best_var_pace_idx]\n    best_var_pace_lags &lt;- rmse_pace_results$Lags[best_var_pace_idx]\n} else {\n    best_var_pace_lags &lt;- min(lags_pace_to_fit)\n}\n\ncat(\"Final VAR Model: VAR(\", best_var_pace_lags, \")\\n\\n\", sep = \"\")\n\n\nFinal VAR Model: VAR(1)\n\n\nCode\n# Refit on full data\nfinal_var_pace &lt;- tryCatch(\n    {\n        VAR(var_pace_data_final, p = best_var_pace_lags, type = \"const\")\n    },\n    error = function(e) {\n        VAR(var_pace_data_final, p = 1, type = \"const\")\n    }\n)\n\nprint(summary(final_var_pace))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Pace, X3PAr, eFG. \nDeterministic variables: const \nSample size: 43 \nLog Likelihood: 208.868 \nRoots of the characteristic polynomial:\n0.3072 0.1708 0.1454\nCall:\nVAR(y = var_pace_data_final, p = best_var_pace_lags, type = \"const\")\n\n\nEstimation results for equation Pace: \n===================================== \nPace = Pace.l1 + X3PAr.l1 + eFG..l1 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)\nPace.l1   -0.1350     0.1729  -0.781    0.440\nX3PAr.l1   4.3987    16.3643   0.269    0.790\neFG..l1    6.0838    38.5944   0.158    0.876\nconst     -0.1038     0.2486  -0.418    0.679\n\n\nResidual standard error: 1.421 on 39 degrees of freedom\nMultiple R-Squared: 0.02178,    Adjusted R-squared: -0.05347 \nF-statistic: 0.2894 on 3 and 39 DF,  p-value: 0.8327 \n\n\nEstimation results for equation X3PAr: \n====================================== \nX3PAr = Pace.l1 + X3PAr.l1 + eFG..l1 + const \n\n          Estimate Std. Error t value Pr(&gt;|t|)   \nPace.l1  0.0009159  0.0019800   0.463  0.64624   \nX3PAr.l1 0.1347711  0.1874200   0.719  0.47637   \neFG..l1  0.0343928  0.4420217   0.078  0.93838   \nconst    0.0080525  0.0028473   2.828  0.00735 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.01627 on 39 degrees of freedom\nMultiple R-Squared: 0.02505,    Adjusted R-squared: -0.04995 \nF-statistic: 0.334 on 3 and 39 DF,  p-value: 0.8008 \n\n\nEstimation results for equation eFG.: \n===================================== \neFG. = Pace.l1 + X3PAr.l1 + eFG..l1 + const \n\n           Estimate Std. Error t value Pr(&gt;|t|)  \nPace.l1   7.028e-04  8.245e-04   0.852   0.3992  \nX3PAr.l1  1.801e-01  7.805e-02   2.308   0.0264 *\neFG..l1  -2.816e-01  1.841e-01  -1.530   0.1341  \nconst     4.086e-06  1.186e-03   0.003   0.9973  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.006776 on 39 degrees of freedom\nMultiple R-Squared: 0.1236, Adjusted R-squared: 0.05617 \nF-statistic: 1.833 on 3 and 39 DF,  p-value: 0.1571 \n\n\n\nCovariance matrix of residuals:\n           Pace      X3PAr      eFG.\nPace   2.018472 -2.042e-03 3.238e-03\nX3PAr -0.002042  2.648e-04 4.819e-05\neFG.   0.003238  4.819e-05 4.591e-05\n\nCorrelation matrix of residuals:\n          Pace    X3PAr   eFG.\nPace   1.00000 -0.08834 0.3364\nX3PAr -0.08834  1.00000 0.4371\neFG.   0.33640  0.43708 1.0000\n\n\nCode\n# Forecast 5 periods ahead\nfc_var_pace_final &lt;- predict(final_var_pace, n.ahead = 5)\n\n# Get variable names\nfc_var_pace_names &lt;- names(fc_var_pace_final$fcst)\ntpar_fc_name_final &lt;- fc_var_pace_names[grep(\"3PAr|PAr\", fc_var_pace_names, ignore.case = TRUE)]\nif (length(tpar_fc_name_final) == 0) tpar_fc_name_final &lt;- fc_var_pace_names[2]\n\nefg_fc_name_final &lt;- fc_var_pace_names[grep(\"eFG%|eFG\", fc_var_pace_names, ignore.case = TRUE)]\nif (length(efg_fc_name_final) == 0) efg_fc_name_final &lt;- fc_var_pace_names[3]\n\n# Display forecasts\ncat(\"\\n=== 5-Period Forecasts ===\\n\\n\")\n\n\n\n=== 5-Period Forecasts ===\n\n\nCode\ncat(\"Pace:\\n\")\n\n\nPace:\n\n\nCode\nprint(fc_var_pace_final$fcst$Pace)\n\n\n             fcst     lower    upper       CI\n[1,] -0.053381013 -2.837959 2.731197 2.784578\n[2,] -0.008244123 -2.822531 2.806043 2.814287\n[3,] -0.057124869 -2.872244 2.757994 2.815119\n[4,] -0.044817235 -2.859962 2.770328 2.815145\n[5,] -0.049536647 -2.864684 2.765611 2.815148\n\n\nCode\ncat(\"\\n\", tpar_fc_name_final, \":\\n\", sep = \"\")\n\n\n\nX3PAr:\n\n\nCode\nprint(fc_var_pace_final$fcst[[tpar_fc_name_final]])\n\n\n            fcst       lower      upper         CI\n[1,] 0.011806217 -0.02008553 0.04369797 0.03189175\n[2,] 0.009800696 -0.02249224 0.04209363 0.03229294\n[3,] 0.009379742 -0.02292588 0.04168536 0.03230562\n[4,] 0.009320975 -0.02298498 0.04162693 0.03230595\n[5,] 0.009308504 -0.02299746 0.04161446 0.03230596\n\n\nCode\ncat(\"\\n\", efg_fc_name_final, \":\\n\", sep = \"\")\n\n\n\neFG.:\n\n\nCode\nprint(fc_var_pace_final$fcst[[efg_fc_name_final]])\n\n\n             fcst       lower      upper         CI\n[1,] 0.0059894985 -0.00729126 0.01927026 0.01328076\n[2,] 0.0004066626 -0.01378868 0.01460200 0.01419534\n[3,] 0.0016492776 -0.01258105 0.01587960 0.01423032\n[4,] 0.0011891645 -0.01304598 0.01542431 0.01423515\n[5,] 0.0013167992 -0.01291869 0.01555229 0.01423549\n\n\nCode\n# Plot forecasts\nfor (vname in fc_var_pace_names) {\n    plot(fc_var_pace_final, names = vname)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGranger Causality Tests:\n\n\nCode\n# Granger causality tests\nvar_pace_names &lt;- names(final_var_pace$varresult)\ntpar_name_granger &lt;- var_pace_names[grep(\"3PAr|PAr\", var_pace_names, ignore.case = TRUE)]\nif (length(tpar_name_granger) == 0) tpar_name_granger &lt;- var_pace_names[2]\n\nefg_name_granger &lt;- var_pace_names[grep(\"eFG%|eFG\", var_pace_names, ignore.case = TRUE)]\nif (length(efg_name_granger) == 0) efg_name_granger &lt;- var_pace_names[3]\n\ngranger_pace_all &lt;- causality(final_var_pace, cause = \"Pace\")$Granger\ngranger_3par_pace &lt;- causality(final_var_pace, cause = tpar_name_granger)$Granger\ngranger_efg_pace &lt;- causality(final_var_pace, cause = efg_name_granger)$Granger\n\ncat(\"Pace → {3PAr, eFG%}: F =\", round(granger_pace_all$statistic, 3),\n    \", p =\", round(granger_pace_all$p.value, 4), \"\\n\")\n\n\nPace → {3PAr, eFG%}: F = 0.368 , p = 0.6928 \n\n\nCode\ncat(\"3PAr → {Pace, eFG%}: F =\", round(granger_3par_pace$statistic, 3),\n    \", p =\", round(granger_3par_pace$p.value, 4), \"\\n\")\n\n\n3PAr → {Pace, eFG%}: F = 2.809 , p = 0.0643 \n\n\nCode\ncat(\"eFG% → {Pace, 3PAr}: F =\", round(granger_efg_pace$statistic, 3),\n    \", p =\", round(granger_efg_pace$p.value, 4), \"\\n\")\n\n\neFG% → {Pace, 3PAr}: F = 0.017 , p = 0.9835 \n\n\nGranger causality tests reveal which strategic variables lead versus follow in NBA evolution. If 3PAr Granger-causes Pace, this suggests teams first adopted three-point shooting, then adjusted their tempo accordingly. Conversely, if Pace Granger-causes 3PAr, faster play may have created the transition opportunities that made three-point volume feasible. The eFG% tests reveal whether shooting skill improvements preceded or followed strategic changes.\n\n\nCode\n# IRFs\nirf_3par_pace &lt;- irf(final_var_pace, impulse = tpar_name_granger, response = \"Pace\", n.ahead = 10)\nplot(irf_3par_pace, main = paste(\"Impulse:\", tpar_name_granger, \"→ Response: Pace\"))\n\n\n\n\n\n\n\n\n\nCode\nirf_efg_pace &lt;- irf(final_var_pace, impulse = efg_name_granger, response = \"Pace\", n.ahead = 10)\nplot(irf_efg_pace, main = paste(\"Impulse:\", efg_name_granger, \"→ Response: Pace\"))\n\n\n\n\n\n\n\n\n\nCode\nirf_pace_3par &lt;- irf(final_var_pace, impulse = \"Pace\", response = tpar_name_granger, n.ahead = 10)\nplot(irf_pace_3par, main = paste(\"Impulse: Pace → Response:\", tpar_name_granger))\n\n\n\n\n\n\n\n\n\n\n\nThe Pace Dynamics VAR model uncovers the temporal relationships between game tempo, shot selection, and shooting efficiency. This addresses a fundamental question in basketball analytics: did the analytics revolution’s emphasis on three-point shooting cause teams to speed up, or did faster pace create opportunities for more three-point attempts?\nImpulse response functions trace the dynamic effects of shocks. If a one-time increase in Pace produces a lasting increase in 3PAr, this supports the hypothesis that faster tempo facilitates three-point volume through transition opportunities and defensive breakdowns. Conversely, if innovations in 3PAr lead to sustained changes in Pace, this suggests strategic shot selection drives tempo decisions. The eFG% impulses reveal whether shooting skill improvements enable teams to control pace or result from pace-driven shot quality.\nCross-validation results demonstrate the model’s out-of-sample forecasting accuracy, with lower RMSE indicating that the bidirectional relationships captured by VAR improve predictions beyond univariate models. The optimal lag structure reveals how long past values influence future changes. A longer optimal lag suggests persistent momentum effects, while shorter lags indicate rapid strategic adjustments where teams quickly respond to innovations.\nThis VAR framework captures the complex adaptive dynamics of NBA strategy, where teams continuously adjust multiple dimensions simultaneously rather than optimizing single variables in isolation. The analytics revolution wasn’t just about shooting more threes; it was about reconfiguring pace, shot selection, and skill development in an interconnected system where each element reinforces the others.",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "multiTS_model.html#sports-betting-nba-recovery-var-1",
    "href": "multiTS_model.html#sports-betting-nba-recovery-var-1",
    "title": "Multivariate Time Series Modeling",
    "section": "Sports Betting & NBA Recovery (VAR)",
    "text": "Sports Betting & NBA Recovery (VAR)\nVariables: DKNG (DraftKings stock price), Attendance, ORtg\nThis VAR model tests a provocative hypothesis: does NBA performance influence sports betting industry valuations? The 2020-2025 period offers a natural experiment: COVID decimated attendance and disrupted seasons, while sports betting stocks experienced extreme volatility. By modeling bidirectional relationships between DraftKings’ stock price, NBA attendance, and offensive efficiency, we test whether the on-court analytics revolution created measurable financial value in connected industries. If ORtg Granger-causes DKNG, this suggests exciting offensive basketball drives betting engagement and stock valuations. If Attendance Granger-causes DKNG, this indicates fan engagement metrics predict betting industry performance.\n\nEDA & StationarityModel Selection & FittingCross-ValidationFinal Model & InterpretationInterpretation\n\n\n\n\nCode\n# Create VAR dataset (2020-2025 only, when DKNG data exists)\nvar_dkng_data &lt;- ts(league_avg_dkng %&gt;% dplyr::select(DKNG_Price, Total_Attendance, ORtg),\n    start = min(league_avg_dkng$Season), frequency = 1\n)\n\n# Plot all series\nautoplot(var_dkng_data, facets = TRUE) +\n    labs(\n        title = \"VAR Variables: DKNG Price, Attendance, ORtg (2020-2025)\",\n        x = \"Year\", y = \"Value\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Summary Statistics:\\n\")\n\n\nSummary Statistics:\n\n\nCode\nsummary(var_dkng_data)\n\n\n   DKNG_Price    Total_Attendance        ORtg      \n Min.   :20.79   Min.   : 1533769   Min.   :110.5  \n 1st Qu.:25.95   1st Qu.:18777753   1st Qu.:112.1  \n Median :36.86   Median :22248246   Median :113.4  \n Mean   :35.02   Mean   :18372584   Mean   :113.2  \n 3rd Qu.:39.24   3rd Qu.:23033042   3rd Qu.:114.7  \n Max.   :53.28   Max.   :23289681   Max.   :115.3  \n\n\nThe visualization captures post-COVID dynamics: DKNG stock exhibits extreme volatility as the sports betting industry navigated legalization waves and market uncertainty. Attendance shows the pandemic collapse in 2020-2021 and gradual recovery. ORtg continues its steady climb, demonstrating that on-court analytics proceeded independent of external shocks. The limited sample size (only 5-6 observations) presents statistical challenges but reflects the natural constraint that DKNG only became public in 2020.\n\n\nCode\n# ADF tests for each series\n# For very short series, ADF tests may be unreliable\n# We'll run them but interpret cautiously\n# adf_dkng_var &lt;- tryCatch(adf.test(var_dkng_data[, \"DKNG_Price\"]), error = function(e) NULL)\n# adf_attend_var &lt;- tryCatch(adf.test(var_dkng_data[, \"Total_Attendance\"]), error = function(e) NULL)\n# adf_ortg_var_dkng &lt;- tryCatch(adf.test(var_dkng_data[, \"ORtg\"]), error = function(e) NULL)\n\n# if (!is.null(adf_dkng_var)) {\n#     cat(\n#         \"DKNG Price: ADF p-value =\", round(adf_dkng_var$p.value, 4),\n#         ifelse(adf_dkng_var$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\"\n#     )\n# } else {\n#     cat(\"DKNG Price: ADF test failed (insufficient data)\\n\")\n# }\n\n# if (!is.null(adf_attend_var)) {\n#     cat(\n#         \"Attendance: ADF p-value =\", round(adf_attend_var$p.value, 4),\n#         ifelse(adf_attend_var$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\"\n#     )\n# } else {\n#     cat(\"Attendance: ADF test failed (insufficient data)\\n\")\n# }\n\n# if (!is.null(adf_ortg_var_dkng)) {\n#     cat(\n#         \"ORtg: ADF p-value =\", round(adf_ortg_var_dkng$p.value, 4),\n#         ifelse(adf_ortg_var_dkng$p.value &lt; 0.05, \"(stationary)\", \"(non-stationary)\"), \"\\n\"\n#     )\n# } else {\n#     cat(\"ORtg: ADF test failed (insufficient data)\\n\")\n# }\n\nvar_dkng_data_final &lt;- var_dkng_data\ndifferenced_dkng &lt;- FALSE\n\n\n\n\n\n\nCode\n# Determine optimal lag order\n# With very short series, limit lag.max\nvar_dkng_select &lt;- tryCatch(\n    {\n        VARselect(var_dkng_data_final, lag.max = 2, type = \"const\")\n    },\n    error = function(e) {\n        list(selection = c(AIC = 1, HQ = 1, SC = 1, FPE = 1))\n    }\n)\n\nprint(var_dkng_select$selection)\n\n\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     1      1      1      2 \n\n\nCode\nif (\"criteria\" %in% names(var_dkng_select)) {\n    print(var_dkng_select$criteria)\n}\n\n\n          1    2\nAIC(n) -Inf -Inf\nHQ(n)  -Inf -Inf\nSC(n)  -Inf -Inf\nFPE(n)  NaN    0\n\n\nCode\n# Fit models with different lag orders\nlags_dkng_to_fit &lt;- unique(var_dkng_select$selection[1:3])\n\n# Ensure we have at least one lag to fit\nif (length(lags_dkng_to_fit) == 0 || any(is.na(lags_dkng_to_fit))) {\n    lags_dkng_to_fit &lt;- 1\n}\n\n# Given short series, limit to max lag 1\nlags_dkng_to_fit &lt;- lags_dkng_to_fit[lags_dkng_to_fit &lt;= 1]\nif (length(lags_dkng_to_fit) == 0) lags_dkng_to_fit &lt;- 1\n\ncat(\"\\nFitting VAR models with p =\", paste(lags_dkng_to_fit, collapse = \", \"), \"\\n\")\n\n\n\nFitting VAR models with p = 1 \n\n\nCode\nvar_dkng_models &lt;- list()\nfor (p in lags_dkng_to_fit) {\n    model_fit &lt;- tryCatch(\n        {\n            VAR(var_dkng_data_final, p = p, type = \"const\")\n        },\n        error = function(e) {\n            NULL\n        }\n    )\n    if (!is.null(model_fit)) {\n        var_dkng_models[[paste0(\"VAR_\", p)]] &lt;- model_fit\n    }\n}\n\n\n\n\nCode\nfor (name in names(var_dkng_models)) {\n    cat(\"========================================\\n\")\n    cat(name, \"Summary:\\n\")\n    cat(\"========================================\\n\\n\")\n\n    # Try to print summary, but catch errors due to near-singular covariance matrices\n    summary_result &lt;- tryCatch(\n        {\n            summary(var_dkng_models[[name]])\n        },\n        error = function(e) {\n            print(coef(var_dkng_models[[name]]))\n            NULL\n        }\n    )\n\n    if (!is.null(summary_result)) {\n        print(summary_result)\n    }\n\n    cat(\"\\n\")\n}\n\n\n========================================\nVAR_1 Summary:\n========================================\n\n\n$DKNG_Price\n                         Estimate   Std. Error    t value  Pr(&gt;|t|)\nDKNG_Price.l1        9.498217e-01 1.425621e+00  0.6662513 0.6258490\nTotal_Attendance.l1  1.928166e-06 2.182555e-06  0.8834444 0.5393466\nORtg.l1             -2.628827e+00 5.556130e+00 -0.4731399 0.7186589\nconst                2.658871e+02 6.044889e+02  0.4398545 0.7361944\n\n$Total_Attendance\n                         Estimate   Std. Error    t value  Pr(&gt;|t|)\nDKNG_Price.l1       -5.214812e+05 7.099819e+05 -0.7344993 0.5966971\nTotal_Attendance.l1 -9.539561e-01 1.086947e+00 -0.8776471 0.5414254\nORtg.l1              4.410249e+06 2.767041e+06  1.5938501 0.3567182\nconst               -4.454000e+08 3.010451e+08 -1.4795125 0.3783855\n\n$ORtg\n                         Estimate   Std. Error     t value  Pr(&gt;|t|)\nDKNG_Price.l1       -8.234494e-02 4.804422e-02 -1.71394054 0.3362384\nTotal_Attendance.l1  2.112814e-09 7.355332e-08  0.02872492 0.9817182\nORtg.l1              3.948660e-01 1.872447e-01  2.10882339 0.2818913\nconst                7.194670e+01 2.037162e+01  3.53171326 0.1756600\n\n\n\n\n\n\nCode\n# Calculate in-sample RMSE for each model\nrmse_dkng_results &lt;- data.frame()\n\nfor (name in names(var_dkng_models)) {\n    model &lt;- var_dkng_models[[name]]\n\n    # Get fitted values\n    fitted_vals &lt;- fitted(model)\n\n    # Calculate RMSE for each variable\n    rmse_dkng_price &lt;- sqrt(mean((var_dkng_data_final[, \"DKNG_Price\"] - fitted_vals[, \"DKNG_Price\"])^2, na.rm = TRUE))\n    rmse_attendance &lt;- sqrt(mean((var_dkng_data_final[, \"Total_Attendance\"] - fitted_vals[, \"Total_Attendance\"])^2, na.rm = TRUE))\n    rmse_ortg_dkng &lt;- sqrt(mean((var_dkng_data_final[, \"ORtg\"] - fitted_vals[, \"ORtg\"])^2, na.rm = TRUE))\n\n    # Average RMSE\n    rmse_avg_dkng &lt;- mean(c(rmse_dkng_price, rmse_attendance, rmse_ortg_dkng), na.rm = TRUE)\n\n    cat(name, \": DKNG RMSE =\", round(rmse_dkng_price, 2),\n        \"| Attendance RMSE =\", round(rmse_attendance / 1e6, 2), \"M\",\n        \"| ORtg RMSE =\", round(rmse_ortg_dkng, 2), \"\\n\")\n\n    # Create descriptive model name\n    lag_num &lt;- as.numeric(gsub(\"VAR_\", \"\", name))\n    model_label &lt;- paste0(\"VAR(\", lag_num, \")\")\n\n    rmse_dkng_results &lt;- rbind(rmse_dkng_results, data.frame(\n        Model = model_label,\n        Lags = lag_num,\n        RMSE_DKNG = rmse_dkng_price,\n        RMSE_Attendance = rmse_attendance,\n        RMSE_ORtg = rmse_ortg_dkng,\n        RMSE_Avg = rmse_avg_dkng\n    ))\n}\n\n\nVAR_1 : DKNG RMSE = 13.92 | Attendance RMSE = 11.93 M | ORtg RMSE = 1.6 \n\n\n\n\nIn-Sample Fit: DKNG VAR Models (2020-2025)\n\n\nModel\nLags\nRMSE (DKNG $)\nRMSE (Attend. M)\nRMSE (ORtg)\nAvg RMSE\n\n\n\n\nVAR(1)\n1\n13.9204\n11.9268\n1.6014\n3975595\n\n\n\n\n\n\n\n\nCode\nif (exists(\"rmse_dkng_results\") && nrow(rmse_dkng_results) &gt; 0) {\n    best_var_dkng_idx &lt;- which.min(rmse_dkng_results$RMSE_Avg)\n    cat(\"\\nBest model:\", rmse_dkng_results$Model[best_var_dkng_idx], \"(Average RMSE =\", round(rmse_dkng_results$RMSE_Avg[best_var_dkng_idx], 2), \")\\n\")\n} else {\n    best_var_dkng_idx &lt;- 1\n}\n\n\n\nBest model: VAR(1) (Average RMSE = 3975595 )\n\n\n\n\n\n\nCode\n# Select best model\nif (exists(\"rmse_dkng_results\") && nrow(rmse_dkng_results) &gt; 0 && exists(\"best_var_dkng_idx\")) {\n    best_var_dkng_name &lt;- rmse_dkng_results$Model[best_var_dkng_idx]\n    best_var_dkng_lags &lt;- rmse_dkng_results$Lags[best_var_dkng_idx]\n} else {\n    best_var_dkng_lags &lt;- 1\n}\n\ncat(\"Final VAR Model: VAR(\", best_var_dkng_lags, \")\\n\\n\", sep = \"\")\n\n\nFinal VAR Model: VAR(1)\n\n\nCode\n# Use the best fitted model\nfinal_var_dkng &lt;- var_dkng_models[[paste0(\"VAR_\", best_var_dkng_lags)]]\n\n# Try to print summary, catch errors due to near-singular covariance\nsummary_result &lt;- tryCatch(\n    {\n        summary(final_var_dkng)\n    },\n    error = function(e) {\n        print(coef(final_var_dkng))\n        NULL\n    }\n)\n\n\n$DKNG_Price\n                         Estimate   Std. Error    t value  Pr(&gt;|t|)\nDKNG_Price.l1        9.498217e-01 1.425621e+00  0.6662513 0.6258490\nTotal_Attendance.l1  1.928166e-06 2.182555e-06  0.8834444 0.5393466\nORtg.l1             -2.628827e+00 5.556130e+00 -0.4731399 0.7186589\nconst                2.658871e+02 6.044889e+02  0.4398545 0.7361944\n\n$Total_Attendance\n                         Estimate   Std. Error    t value  Pr(&gt;|t|)\nDKNG_Price.l1       -5.214812e+05 7.099819e+05 -0.7344993 0.5966971\nTotal_Attendance.l1 -9.539561e-01 1.086947e+00 -0.8776471 0.5414254\nORtg.l1              4.410249e+06 2.767041e+06  1.5938501 0.3567182\nconst               -4.454000e+08 3.010451e+08 -1.4795125 0.3783855\n\n$ORtg\n                         Estimate   Std. Error     t value  Pr(&gt;|t|)\nDKNG_Price.l1       -8.234494e-02 4.804422e-02 -1.71394054 0.3362384\nTotal_Attendance.l1  2.112814e-09 7.355332e-08  0.02872492 0.9817182\nORtg.l1              3.948660e-01 1.872447e-01  2.10882339 0.2818913\nconst                7.194670e+01 2.037162e+01  3.53171326 0.1756600\n\n\nCode\nif (!is.null(summary_result)) {\n    print(summary_result)\n} else {\n    cat(\"\\n\")\n}\n\n\nCode\n# Forecast 3 periods ahead (conservative given data limitations)\nfc_var_dkng_final &lt;- predict(final_var_dkng, n.ahead = 3)\n\n# Display forecasts\ncat(\"\\n3-Period Forecasts (2026-2028):\\n\\n\")\n\n\n\n3-Period Forecasts (2026-2028):\n\n\nCode\ncat(\"DKNG Price:\\n\")\n\n\nDKNG Price:\n\n\nCode\nprint(fc_var_dkng_final$fcst$DKNG_Price)\n\n\n         fcst    lower    upper       CI\n[1,] 46.96449 7.898581 86.03040 39.06591\n[2,] 43.75018 4.565139 82.93523 39.18504\n[3,] 41.77703 2.547918 81.00614 39.22911\n\n\nCode\ncat(\"\\nTotal Attendance:\\n\")\n\n\n\nTotal Attendance:\n\n\nCode\nprint(fc_var_dkng_final$fcst$Total_Attendance)\n\n\n         fcst    lower    upper       CI\n[1,] 17020013 -2435433 36475458 19455445\n[2,] 16432957 -4461065 37326979 20894022\n[3,] 14958552 -8647399 38564503 23605951\n\n\nCode\ncat(\"\\nORtg:\\n\")\n\n\n\nORtg:\n\n\nCode\nprint(fc_var_dkng_final$fcst$ORtg)\n\n\n         fcst    lower    upper       CI\n[1,] 113.9528 112.6363 115.2694 1.316543\n[2,] 113.1115 109.1108 117.1122 4.000673\n[3,] 113.0427 108.6723 117.4131 4.370399\n\n\nCode\n# Plot forecasts\nfor (vname in names(fc_var_dkng_final$fcst)) {\n    plot(fc_var_dkng_final, names = vname)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGranger Causality Tests:\n\n\nCode\n# Granger causality tests\ngranger_dkng &lt;- tryCatch(causality(final_var_dkng, cause = \"DKNG_Price\")$Granger, error = function(e) NULL)\ngranger_attend_dkng &lt;- tryCatch(causality(final_var_dkng, cause = \"Total_Attendance\")$Granger, error = function(e) NULL)\ngranger_ortg_dkng &lt;- tryCatch(causality(final_var_dkng, cause = \"ORtg\")$Granger, error = function(e) NULL)\n\nif (!is.null(granger_dkng)) {\n    cat(\"DKNG Price → {Attendance, ORtg}: F =\", round(granger_dkng$statistic, 3),\n        \", p =\", round(granger_dkng$p.value, 4), \"\\n\")\n}\n\nif (!is.null(granger_attend_dkng)) {\n    cat(\"Attendance → {DKNG, ORtg}: F =\", round(granger_attend_dkng$statistic, 3),\n        \", p =\", round(granger_attend_dkng$p.value, 4), \"\\n\")\n}\n\nif (!is.null(granger_ortg_dkng)) {\n    cat(\"ORtg → {DKNG, Attendance}: F =\", round(granger_ortg_dkng$statistic, 3),\n        \", p =\", round(granger_ortg_dkng$p.value, 4), \"\\n\")\n}\n\n\n\n\nCode\n# IRFs\nirf_dkng_attend &lt;- tryCatch(\n    {\n        irf(final_var_dkng, impulse = \"DKNG_Price\", response = \"Total_Attendance\", n.ahead = 5)\n    },\n    error = function(e) NULL\n)\nif (!is.null(irf_dkng_attend)) {\n    plot(irf_dkng_attend, main = \"Impulse: DKNG Price → Response: Attendance\")\n}\n\nirf_attend_dkng &lt;- tryCatch(\n    {\n        irf(final_var_dkng, impulse = \"Total_Attendance\", response = \"DKNG_Price\", n.ahead = 5)\n    },\n    error = function(e) NULL\n)\nif (!is.null(irf_attend_dkng)) {\n    plot(irf_attend_dkng, main = \"Impulse: Attendance → Response: DKNG Price\")\n}\n\nirf_ortg_dkng &lt;- tryCatch(\n    {\n        irf(final_var_dkng, impulse = \"ORtg\", response = \"DKNG_Price\", n.ahead = 5)\n    },\n    error = function(e) NULL\n)\nif (!is.null(irf_ortg_dkng)) {\n    plot(irf_ortg_dkng, main = \"Impulse: ORtg → Response: DKNG Price\")\n}\n\n\n\n\nThe Sports Betting & NBA Recovery VAR model explores whether basketball performance metrics influence financial markets in the connected sports betting industry. This analysis addresses a provocative question: does the on-court analytics revolution—quantified by rising offensive efficiency—drive betting engagement and industry valuations?\nData Limitations & Methodological Constraints\nThis model confronts severe data limitations inherent to the research question. DraftKings only became publicly traded in April 2020, giving us merely 5-6 annual observations (2020-2025). Traditional time series methods require far more data for reliable inference, making this analysis exploratory rather than definitive.\nWith such limited data, cross-validation is impossible. We report in-sample fit statistics while acknowledging they overstate predictive accuracy. Granger causality tests and impulse response functions may lack statistical power to detect true relationships. The 2020-2021 COVID period introduces extreme outliers that dominate the short sample, potentially obscuring underlying relationships.\nWhat We Can Learn Despite Limitations\nEven with constraints, this VAR framework provides insights unavailable from simple correlations. The model quantifies whether past NBA attendance predicts future DKNG stock movements, testing if fan engagement leads betting industry valuations. Similarly, if ORtg Granger-causes DKNG, this suggests exciting offensive basketball may drive betting interest beyond traditional fan metrics.\nThe impulse response functions trace dynamic effects: does a surge in NBA attendance produce a lasting boost to DKNG stock, or do markets quickly price in the information? Conversely, do DKNG stock movements predict future NBA attendance, potentially reflecting forward-looking investor sentiment about sports engagement?\nThe Bigger Picture\nThis model demonstrates the interconnected nature of modern sports ecosystems. The NBA isn’t just an entertainment product; it’s the foundation for a multi-billion dollar betting industry. The analytics revolution that transformed on-court play may have simultaneously created financial value in adjacent markets. While our limited data prevents definitive conclusions, the VAR framework establishes a methodological template for analyzing sports-finance linkages as more data accumulates.\nFuture research with longer time series (2025-2030 and beyond) can test whether the relationships identified here persist or whether the COVID period was sui generis. The model also highlights how external shocks like pandemics can simultaneously disrupt multiple interconnected variables, creating identification challenges that VAR approaches are uniquely designed to address.",
    "crumbs": [
      "Home",
      "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
    ]
  },
  {
    "objectID": "other.html",
    "href": "other.html",
    "title": "Interrupted Time Series Analysis",
    "section": "",
    "text": "Intervention Date: January 8, 2022\nOn January 8, 2022, New York State launched online sports betting, marking one of the most significant events in the U.S. sports betting industry. New York quickly became the largest sports betting market in the United States by revenue, fundamentally changing the competitive landscape for sports betting operators.\n\n\n\nMarket Size: New York represents approximately 20% of the total U.S. online sports betting market\nPopulation: ~20 million residents in the state, with high sports betting engagement\nMajor Operators: All four stocks in our analysis (DraftKings, Penn Entertainment, Caesars, MGM) launched operations in NY\nRevenue Impact: Within months, NY operators generated hundreds of millions in revenue\n\n\n\n\nDid the New York sports betting legalization on January 8, 2022, produce a significant change in the level and/or trend of sports betting stock prices?"
  },
  {
    "objectID": "other.html#the-intervention-new-york-online-sports-betting-launch",
    "href": "other.html#the-intervention-new-york-online-sports-betting-launch",
    "title": "Interrupted Time Series Analysis",
    "section": "",
    "text": "Intervention Date: January 8, 2022\nOn January 8, 2022, New York State launched online sports betting, marking one of the most significant events in the U.S. sports betting industry. New York quickly became the largest sports betting market in the United States by revenue, fundamentally changing the competitive landscape for sports betting operators.\n\n\n\nMarket Size: New York represents approximately 20% of the total U.S. online sports betting market\nPopulation: ~20 million residents in the state, with high sports betting engagement\nMajor Operators: All four stocks in our analysis (DraftKings, Penn Entertainment, Caesars, MGM) launched operations in NY\nRevenue Impact: Within months, NY operators generated hundreds of millions in revenue\n\n\n\n\nDid the New York sports betting legalization on January 8, 2022, produce a significant change in the level and/or trend of sports betting stock prices?"
  },
  {
    "objectID": "other.html#dataset-overview",
    "href": "other.html#dataset-overview",
    "title": "Interrupted Time Series Analysis",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\nTime Period: January 2, 2020 to December 31, 2024 (~5 years)\nFrequency: Daily stock prices\nCompanies Analyzed:\n\nDraftKings (DKNG): Pure-play online sports betting and DFS\nPenn Entertainment (PENN): Casino/sports betting (ESPN BET)\nCaesars Entertainment (CZR): Casino operator with Caesars Sportsbook\nMGM Resorts (MGM): Resort/casino operator with BetMGM\n\nOutcome Variable: Daily adjusted close price (in USD)\nPre-intervention Period: January 2, 2020 - January 7, 2022 (~2 years)\nPost-intervention Period: January 8, 2022 - December 31, 2024 (~3 years)\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(gridExtra)\nlibrary(kableExtra)\n\ntheme_set(theme_minimal(base_size = 12))\n\n\n\n\nCode\n# Load stock data\ndkng &lt;- read_csv(\"data/financial/DKNG_daily.csv\", show_col_types = FALSE)\npenn &lt;- read_csv(\"data/financial/PENN_daily.csv\", show_col_types = FALSE)\nczr &lt;- read_csv(\"data/financial/CZR_daily.csv\", show_col_types = FALSE)\nmgm &lt;- read_csv(\"data/financial/MGM_daily.csv\", show_col_types = FALSE)\n\n# Rename columns for easier handling\ncolnames(dkng) &lt;- gsub(\"Adj Close\", \"Adj_Close\", colnames(dkng))\ncolnames(penn) &lt;- gsub(\"Adj Close\", \"Adj_Close\", colnames(penn))\ncolnames(czr) &lt;- gsub(\"Adj\\\\.Close\", \"Adj_Close\", colnames(czr))\ncolnames(mgm) &lt;- gsub(\"Adj\\\\.Close\", \"Adj_Close\", colnames(mgm))\n\n# Convert dates\ndkng$Date &lt;- as.Date(dkng$Date)\npenn$Date &lt;- as.Date(penn$Date)\nczr$Date &lt;- as.Date(czr$Date)\nmgm$Date &lt;- as.Date(mgm$Date)\n\n# Define intervention date\nintervention_date &lt;- as.Date(\"2022-01-08\")\n\n# Filter to relevant time period (2020-2024)\ndkng &lt;- dkng %&gt;% filter(Date &gt;= as.Date(\"2020-01-01\"), Date &lt;= as.Date(\"2024-12-31\"))\npenn &lt;- penn %&gt;% filter(Date &gt;= as.Date(\"2020-01-01\"), Date &lt;= as.Date(\"2024-12-31\"))\nczr &lt;- czr %&gt;% filter(Date &gt;= as.Date(\"2020-01-01\"), Date &lt;= as.Date(\"2024-12-31\"))\nmgm &lt;- mgm %&gt;% filter(Date &gt;= as.Date(\"2020-01-01\"), Date &lt;= as.Date(\"2024-12-31\"))\n\n# Create intervention indicators\ndkng$Post_Intervention &lt;- ifelse(dkng$Date &gt;= intervention_date, 1, 0)\ndkng$Time &lt;- as.numeric(dkng$Date - min(dkng$Date))\ndkng$Time_Since_Intervention &lt;- ifelse(dkng$Post_Intervention == 1,\n    as.numeric(dkng$Date - intervention_date), 0\n)\n\npenn$Post_Intervention &lt;- ifelse(penn$Date &gt;= intervention_date, 1, 0)\npenn$Time &lt;- as.numeric(penn$Date - min(penn$Date))\npenn$Time_Since_Intervention &lt;- ifelse(penn$Post_Intervention == 1,\n    as.numeric(penn$Date - intervention_date), 0\n)\n\nczr$Post_Intervention &lt;- ifelse(czr$Date &gt;= intervention_date, 1, 0)\nczr$Time &lt;- as.numeric(czr$Date - min(czr$Date))\nczr$Time_Since_Intervention &lt;- ifelse(czr$Post_Intervention == 1,\n    as.numeric(czr$Date - intervention_date), 0\n)\n\nmgm$Post_Intervention &lt;- ifelse(mgm$Date &gt;= intervention_date, 1, 0)\nmgm$Time &lt;- as.numeric(mgm$Date - min(mgm$Date))\nmgm$Time_Since_Intervention &lt;- ifelse(mgm$Post_Intervention == 1,\n    as.numeric(mgm$Date - intervention_date), 0\n)\n\ncat(\"Data Summary:\\n\")\n\n\nData Summary:\n\n\nCode\ncat(\n    \"DraftKings (DKNG):\", nrow(dkng), \"observations from\",\n    as.character(min(dkng$Date)), \"to\", as.character(max(dkng$Date)), \"\\n\"\n)\n\n\nDraftKings (DKNG): 1181 observations from 2020-04-23 to 2024-12-31 \n\n\nCode\ncat(\n    \"Penn (PENN):\", nrow(penn), \"observations from\",\n    as.character(min(penn$Date)), \"to\", as.character(max(penn$Date)), \"\\n\"\n)\n\n\nPenn (PENN): 1258 observations from 2020-01-02 to 2024-12-31 \n\n\nCode\ncat(\n    \"Caesars (CZR):\", nrow(czr), \"observations from\",\n    as.character(min(czr$Date)), \"to\", as.character(max(czr$Date)), \"\\n\"\n)\n\n\nCaesars (CZR): 1258 observations from 2020-01-02 to 2024-12-31 \n\n\nCode\ncat(\n    \"MGM (MGM):\", nrow(mgm), \"observations from\",\n    as.character(min(mgm$Date)), \"to\", as.character(max(mgm$Date)), \"\\n\\n\"\n)\n\n\nMGM (MGM): 1258 observations from 2020-01-02 to 2024-12-31 \n\n\nCode\ncat(\"Intervention Date:\", as.character(intervention_date), \"\\n\")\n\n\nIntervention Date: 2022-01-08 \n\n\nCode\ncat(\"Pre-intervention observations (DKNG):\", sum(dkng$Post_Intervention == 0), \"\\n\")\n\n\nPre-intervention observations (DKNG): 433 \n\n\nCode\ncat(\"Post-intervention observations (DKNG):\", sum(dkng$Post_Intervention == 1), \"\\n\")\n\n\nPost-intervention observations (DKNG): 748"
  },
  {
    "objectID": "other.html#model-specification",
    "href": "other.html#model-specification",
    "title": "Interrupted Time Series Analysis",
    "section": "Model Specification",
    "text": "Model Specification\nThe interrupted time series (ITS) model takes the form:\n\\[\nY_t = \\beta_0 + \\beta_1 \\cdot Time_t + \\beta_2 \\cdot Intervention_t + \\beta_3 \\cdot Time\\_Since\\_Intervention_t + \\epsilon_t\n\\]\nWhere:\n\n\\(Y_t\\) = Stock price at time \\(t\\)\n\\(Time_t\\) = Continuous time variable (days since start)\n\\(Intervention_t\\) = Binary indicator (0 before Jan 8, 2022; 1 after)\n\\(Time\\_Since\\_Intervention_t\\) = Days since intervention (0 before intervention)\n\nInterpretation:\n\n\\(\\beta_1\\) = Pre-intervention trend (slope before NY launch)\n\\(\\beta_2\\) = Immediate level change (jump/drop on intervention date)\n\\(\\beta_3\\) = Change in trend (difference in slope after intervention)"
  },
  {
    "objectID": "other.html#draftkings-dkng-analysis",
    "href": "other.html#draftkings-dkng-analysis",
    "title": "Interrupted Time Series Analysis",
    "section": "DraftKings (DKNG) Analysis",
    "text": "DraftKings (DKNG) Analysis\n\n\nCode\n# Fit ITS model\nmodel_dkng &lt;- lm(Adj_Close ~ Time + Post_Intervention + Time_Since_Intervention,\n    data = dkng\n)\n\n# Summary\nsummary(model_dkng)\n\n\n\nCall:\nlm(formula = Adj_Close ~ Time + Post_Intervention + Time_Since_Intervention, \n    data = dkng)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.8740  -4.3779  -0.7542   5.0838  25.3243 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              41.153918   0.758165  54.281  &lt; 2e-16 ***\nTime                      0.016672   0.002099   7.942 4.63e-15 ***\nPost_Intervention       -40.788305   0.953569 -42.774  &lt; 2e-16 ***\nTime_Since_Intervention   0.013421   0.002292   5.856 6.16e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.905 on 1177 degrees of freedom\nMultiple R-squared:  0.7002,    Adjusted R-squared:  0.6994 \nF-statistic: 916.1 on 3 and 1177 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Extract coefficients\ncoef_dkng &lt;- summary(model_dkng)$coefficients\n\n# Create nice table\nresults_dkng &lt;- data.frame(\n    Parameter = c(\n        \"Intercept (Baseline)\", \"Pre-intervention Trend\",\n        \"Level Change (Immediate)\", \"Trend Change (Slope Difference)\"\n    ),\n    Coefficient = coef_dkng[, 1],\n    Std_Error = coef_dkng[, 2],\n    t_value = coef_dkng[, 3],\n    p_value = coef_dkng[, 4]\n)\n\nkable(results_dkng,\n    format = \"html\",\n    digits = 4,\n    caption = \"DraftKings (DKNG) Interrupted Time Series Results\",\n    col.names = c(\"Parameter\", \"Coefficient\", \"Std. Error\", \"t-value\", \"p-value\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(0, bold = TRUE, background = \"#006bb6\", color = \"white\") %&gt;%\n    row_spec(which(results_dkng$p_value &lt; 0.05), background = \"#e6f2ff\")\n\n\n\n\nDraftKings (DKNG) Interrupted Time Series Results\n\n\n\nParameter\nCoefficient\nStd. Error\nt-value\np-value\n\n\n\n\n(Intercept)\nIntercept (Baseline)\n41.1539\n0.7582\n54.2810\n0\n\n\nTime\nPre-intervention Trend\n0.0167\n0.0021\n7.9417\n0\n\n\nPost_Intervention\nLevel Change (Immediate)\n-40.7883\n0.9536\n-42.7744\n0\n\n\nTime_Since_Intervention\nTrend Change (Slope Difference)\n0.0134\n0.0023\n5.8555\n0\n\n\n\n\n\n\n\n\n\nFitted Values Visualization\n\n\nCode\n# Add fitted values\ndkng$Fitted &lt;- fitted(model_dkng)\n\nggplot(dkng, aes(x = Date)) +\n    geom_line(aes(y = Adj_Close), color = \"gray60\", alpha = 0.6, linewidth = 0.5) +\n    geom_line(aes(y = Fitted), color = \"#006bb6\", linewidth = 1.2) +\n    geom_vline(\n        xintercept = intervention_date, color = \"red\",\n        linetype = \"dashed\", linewidth = 1.2\n    ) +\n    annotate(\"text\",\n        x = intervention_date, y = max(dkng$Adj_Close, na.rm = TRUE) * 0.95,\n        label = \"NY Launch\", color = \"red\", size = 5, fontface = \"bold\"\n    ) +\n    labs(\n        title = \"DraftKings (DKNG): Actual vs. Fitted Values from ITS Model\",\n        subtitle = \"Gray = Actual prices | Blue = Fitted trend lines\",\n        x = \"Date\",\n        y = \"Adjusted Close Price ($)\"\n    ) +\n    theme_minimal(base_size = 13) +\n    theme(plot.title = element_text(face = \"bold\", size = 15))"
  },
  {
    "objectID": "other.html#penn-entertainment-penn-analysis",
    "href": "other.html#penn-entertainment-penn-analysis",
    "title": "Interrupted Time Series Analysis",
    "section": "Penn Entertainment (PENN) Analysis",
    "text": "Penn Entertainment (PENN) Analysis\n\n\nCode\nmodel_penn &lt;- lm(Adj_Close ~ Time + Post_Intervention + Time_Since_Intervention,\n    data = penn\n)\n\ncoef_penn &lt;- summary(model_penn)$coefficients\n\nresults_penn &lt;- data.frame(\n    Parameter = c(\n        \"Intercept (Baseline)\", \"Pre-intervention Trend\",\n        \"Level Change (Immediate)\", \"Trend Change (Slope Difference)\"\n    ),\n    Coefficient = coef_penn[, 1],\n    Std_Error = coef_penn[, 2],\n    t_value = coef_penn[, 3],\n    p_value = coef_penn[, 4]\n)\n\nkable(results_penn,\n    format = \"html\",\n    digits = 4,\n    caption = \"Penn Entertainment (PENN) Interrupted Time Series Results\",\n    col.names = c(\"Parameter\", \"Coefficient\", \"Std. Error\", \"t-value\", \"p-value\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(0, bold = TRUE, background = \"darkred\", color = \"white\") %&gt;%\n    row_spec(which(results_penn$p_value &lt; 0.05), background = \"#ffe6e6\")\n\n\n\n\nPenn Entertainment (PENN) Interrupted Time Series Results\n\n\n\nParameter\nCoefficient\nStd. Error\nt-value\np-value\n\n\n\n\n(Intercept)\nIntercept (Baseline)\n32.4775\n1.3729\n23.6569\n0\n\n\nTime\nPre-intervention Trend\n0.0805\n0.0032\n24.9912\n0\n\n\nPost_Intervention\nLevel Change (Immediate)\n-52.3190\n1.7797\n-29.3985\n0\n\n\nTime_Since_Intervention\nTrend Change (Slope Difference)\n-0.1036\n0.0037\n-28.0498\n0"
  },
  {
    "objectID": "other.html#caesars-entertainment-czr-analysis",
    "href": "other.html#caesars-entertainment-czr-analysis",
    "title": "Interrupted Time Series Analysis",
    "section": "Caesars Entertainment (CZR) Analysis",
    "text": "Caesars Entertainment (CZR) Analysis\n\n\nCode\nmodel_czr &lt;- lm(Adj_Close ~ Time + Post_Intervention + Time_Since_Intervention,\n    data = czr\n)\n\ncoef_czr &lt;- summary(model_czr)$coefficients\n\nresults_czr &lt;- data.frame(\n    Parameter = c(\n        \"Intercept (Baseline)\", \"Pre-intervention Trend\",\n        \"Level Change (Immediate)\", \"Trend Change (Slope Difference)\"\n    ),\n    Coefficient = coef_czr[, 1],\n    Std_Error = coef_czr[, 2],\n    t_value = coef_czr[, 3],\n    p_value = coef_czr[, 4]\n)\n\nkable(results_czr,\n    format = \"html\",\n    digits = 4,\n    caption = \"Caesars Entertainment (CZR) Interrupted Time Series Results\",\n    col.names = c(\"Parameter\", \"Coefficient\", \"Std. Error\", \"t-value\", \"p-value\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(0, bold = TRUE, background = \"darkgreen\", color = \"white\") %&gt;%\n    row_spec(which(results_czr$p_value &lt; 0.05), background = \"#e6ffe6\")\n\n\n\n\nCaesars Entertainment (CZR) Interrupted Time Series Results\n\n\n\nParameter\nCoefficient\nStd. Error\nt-value\np-value\n\n\n\n\n(Intercept)\nIntercept (Baseline)\n27.3711\n1.0552\n25.9392\n0\n\n\nTime\nPre-intervention Trend\n0.1179\n0.0025\n47.5772\n0\n\n\nPost_Intervention\nLevel Change (Immediate)\n-53.4128\n1.3679\n-39.0480\n0\n\n\nTime_Since_Intervention\nTrend Change (Slope Difference)\n-0.1423\n0.0028\n-50.1363\n0"
  },
  {
    "objectID": "other.html#mgm-resorts-mgm-analysis",
    "href": "other.html#mgm-resorts-mgm-analysis",
    "title": "Interrupted Time Series Analysis",
    "section": "MGM Resorts (MGM) Analysis",
    "text": "MGM Resorts (MGM) Analysis\n\n\nCode\nmodel_mgm &lt;- lm(Adj_Close ~ Time + Post_Intervention + Time_Since_Intervention,\n    data = mgm\n)\n\ncoef_mgm &lt;- summary(model_mgm)$coefficients\n\nresults_mgm &lt;- data.frame(\n    Parameter = c(\n        \"Intercept (Baseline)\", \"Pre-intervention Trend\",\n        \"Level Change (Immediate)\", \"Trend Change (Slope Difference)\"\n    ),\n    Coefficient = coef_mgm[, 1],\n    Std_Error = coef_mgm[, 2],\n    t_value = coef_mgm[, 3],\n    p_value = coef_mgm[, 4]\n)\n\nkable(results_mgm,\n    format = \"html\",\n    digits = 4,\n    caption = \"MGM Resorts (MGM) Interrupted Time Series Results\",\n    col.names = c(\"Parameter\", \"Coefficient\", \"Std. Error\", \"t-value\", \"p-value\")\n) %&gt;%\n    kable_styling(\n        full_width = FALSE,\n        bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n    ) %&gt;%\n    row_spec(0, bold = TRUE, background = \"darkorange\", color = \"white\") %&gt;%\n    row_spec(which(results_mgm$p_value &lt; 0.05), background = \"#fff4e6\")\n\n\n\n\nMGM Resorts (MGM) Interrupted Time Series Results\n\n\n\nParameter\nCoefficient\nStd. Error\nt-value\np-value\n\n\n\n\n(Intercept)\nIntercept (Baseline)\n15.4288\n0.4584\n33.6595\n0\n\n\nTime\nPre-intervention Trend\n0.0429\n0.0011\n39.9028\n0\n\n\nPost_Intervention\nLevel Change (Immediate)\n-9.1403\n0.5942\n-15.3825\n0\n\n\nTime_Since_Intervention\nTrend Change (Slope Difference)\n-0.0398\n0.0012\n-32.2568\n0"
  },
  {
    "objectID": "other.html#statistical-significance",
    "href": "other.html#statistical-significance",
    "title": "Interrupted Time Series Analysis",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nBased on the interrupted time series analysis, we examine two key questions:\n\nWas there an immediate level change? (Did prices jump/drop on January 8, 2022?)\nWas there a trend change? (Did the growth trajectory change after the intervention?)"
  },
  {
    "objectID": "other.html#interpretation-guidelines",
    "href": "other.html#interpretation-guidelines",
    "title": "Interrupted Time Series Analysis",
    "section": "Interpretation Guidelines",
    "text": "Interpretation Guidelines\n\nLevel Change (β₂):\n\nPositive & significant → Stock price jumped immediately after NY launch\nNegative & significant → Stock price dropped immediately after NY launch\nNot significant → No immediate reaction to the intervention\n\nTrend Change (β₃):\n\nPositive & significant → Stocks grew faster after NY legalization\nNegative & significant → Growth slowed after NY legalization\nNot significant → No change in long-term trajectory"
  }
]