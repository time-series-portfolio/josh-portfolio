[
  {
    "objectID": "uniTS_model.html",
    "href": "uniTS_model.html",
    "title": "Univariate Time Series Modeling: ARIMA & SARIMA",
    "section": "",
    "text": "This page applies ARIMA to the annual NBA series (ORtg, 3PAr, Pace, Attendance) and SARIMA to weekly equities (DKNG, PENN) to quantify temporal structure and produce short-horizon forecasts. Consistent with the EDA, all four NBA series are non-stationary in levels. ACFs decay slowly and ADF tests fail to reject a unit root, but become stationary after first differencing (d=1) with roughly constant variance that favors additive dynamics.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(patchwork)\n\n# Set plotting theme\ntheme_set(theme_minimal(base_size = 12))\n\n# Load all advanced stats data\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n    df &lt;- read_csv(file, show_col_types = FALSE)\n    df$Season &lt;- season_year\n    return(df)\n})\n\n# Calculate league averages by season\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        .groups = \"drop\"\n    )\n\ncat(\"League average data loaded: 1980-2025,\", nrow(league_avg), \"seasons\\n\")\n\n\nLeague average data loaded: 1980-2025, 45 seasons\n\n\n\n\n\n\nCode\n# Create time series\nts_ortg &lt;- ts(league_avg$ORtg, start = 1980, frequency = 1)\n\n# (a) ACF Graph\nggAcf(ts_ortg, lag.max = 20) +\n    labs(\n        title = \"ACF of ORtg (Original Series)\",\n        subtitle = \"Slow decay indicates non-stationarity\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# (b) Augmented Dickey-Fuller Test\nadf_ortg &lt;- adf.test(ts_ortg)\ncat(\"ADF Test (Original ORtg):\\n\")\n\n\nADF Test (Original ORtg):\n\n\nCode\ncat(\"  Test Statistic:\", round(adf_ortg$statistic, 4), \"\\n\")\n\n\n  Test Statistic: -1.0264 \n\n\nCode\ncat(\"  p-value:\", round(adf_ortg$p.value, 4), \"\\n\")\n\n\n  p-value: 0.9233 \n\n\nCode\ncat(\"  Conclusion:\", ifelse(adf_ortg$p.value &lt; 0.05, \"Stationary\", \"Non-stationary\"), \"\\n\")\n\n\n  Conclusion: Non-stationary \n\n\n\n\n\n\n\n\nCode\n# First-order differencing\ndiff_ortg_1 &lt;- diff(ts_ortg, differences = 1)\n\ncat(\"First-order differenced series length:\", length(diff_ortg_1), \"\\n\")\n\n\nFirst-order differenced series length: 44 \n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_ortg, main = \"Original ORtg Series\", ylab = \"ORtg\", col = \"blue\")\nplot(diff_ortg_1, main = \"First-Order Differenced ORtg\", ylab = \"Change in ORtg\", col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\nCode\nadf_diff_ortg &lt;- adf.test(diff_ortg_1)\ncat(\"ADF Test (Differenced ORtg, d=1):\\n\")\n\n\nADF Test (Differenced ORtg, d=1):\n\n\nCode\ncat(\"  Test Statistic:\", round(adf_diff_ortg$statistic, 4), \"\\n\")\n\n\n  Test Statistic: -3.174 \n\n\nCode\ncat(\"  p-value:\", round(adf_diff_ortg$p.value, 4), \"\\n\")\n\n\n  p-value: 0.109 \n\n\nCode\ncat(\"  Conclusion:\", ifelse(adf_diff_ortg$p.value &lt; 0.05, \"Stationary\", \"Non-stationary\"), \"\\n\")\n\n\n  Conclusion: Non-stationary \n\n\n\n\n\n\n\n\nCode\nacf_plot &lt;- ggAcf(diff_ortg_1, lag.max = 20) +\n    labs(title = \"ACF of Differenced ORtg\") +\n    theme_minimal()\n\npacf_plot &lt;- ggPacf(diff_ortg_1, lag.max = 20) +\n    labs(title = \"PACF of Differenced ORtg\") +\n    theme_minimal()\n\nacf_plot / pacf_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_110 &lt;- Arima(ts_ortg, order = c(1, 1, 0))\nmodel_011 &lt;- Arima(ts_ortg, order = c(0, 1, 1))\nmodel_111 &lt;- Arima(ts_ortg, order = c(1, 1, 1))\n\ncat(\"Model Comparison:\\n\")\n\n\nModel Comparison:\n\n\nCode\ncat(\"ARIMA(1,1,0): AIC =\", round(model_110$aic, 2), \"| BIC =\", round(model_110$bic, 2), \"\\n\")\n\n\nARIMA(1,1,0): AIC = 157.48 | BIC = 161.04 \n\n\nCode\ncat(\"ARIMA(0,1,1): AIC =\", round(model_011$aic, 2), \"| BIC =\", round(model_011$bic, 2), \"\\n\")\n\n\nARIMA(0,1,1): AIC = 157.97 | BIC = 161.54 \n\n\nCode\ncat(\"ARIMA(1,1,1): AIC =\", round(model_111$aic, 2), \"| BIC =\", round(model_111$bic, 2), \"\\n\")\n\n\nARIMA(1,1,1): AIC = 159.39 | BIC = 164.75 \n\n\nCode\n# Select best model (lowest AIC)\nmodels_ortg &lt;- list(model_110, model_011, model_111)\naic_vals &lt;- c(model_110$aic, model_011$aic, model_111$aic)\nbest_ortg &lt;- models_ortg[[which.min(aic_vals)]]\n\ncat(\"\\nBest Model: ARIMA\", paste0(arimaorder(best_ortg)[c(1, 2, 3)], collapse = \",\"), \"\\n\")\n\n\n\nBest Model: ARIMA 1,1,0 \n\n\nModel Equation:\n\\[(1-B)Y_t = (1 + \\theta_1 B)\\epsilon_t\\]\n\n\nCode\ncat(\"Model Coefficients:\\n\")\n\n\nModel Coefficients:\n\n\nCode\nprint(coef(best_ortg))\n\n\n       ar1 \n-0.2320334 \n\n\n\n\n\n\n\nCode\n# Full diagnostic plots using sarima\nsarima(ts_ortg, p = arimaorder(best_ortg)[1], d = arimaorder(best_ortg)[2], q = arimaorder(best_ortg)[3])\n\n\ninitial  value 0.345604 \niter   2 value 0.311547\niter   3 value 0.310728\niter   4 value 0.310639\niter   5 value 0.310639\niter   5 value 0.310639\niter   5 value 0.310639\nfinal  value 0.310639 \nconverged\ninitial  value 0.308190 \niter   2 value 0.308084\niter   3 value 0.308078\niter   4 value 0.308078\niter   4 value 0.308078\niter   4 value 0.308078\nfinal  value 0.308078 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1       -0.2570 0.1457 -1.7639  0.0850\nconstant   0.2042 0.1638  1.2465  0.2195\n\nsigma^2 estimated as 1.848925 on 42 degrees of freedom \n \nAIC = 3.590398  AICc = 3.597049  BIC = 3.712047 \n \n\n\n\n\n\n\n\n\n\n\n\nCode\nljung_ortg &lt;- Box.test(best_ortg$residuals, lag = 10, type = \"Ljung-Box\")\ncat(\"Ljung-Box Test (lag=10):\\n\")\n\n\nLjung-Box Test (lag=10):\n\n\nCode\ncat(\"  p-value =\", round(ljung_ortg$p.value, 4), \"\\n\")\n\n\n  p-value = 0.8617 \n\n\nCode\ncat(\"  Conclusion:\", ifelse(ljung_ortg$p.value &gt; 0.05, \"Residuals are white noise ✓\", \"Some autocorrelation remains\"), \"\\n\")\n\n\n  Conclusion: Residuals are white noise ✓ \n\n\n\n\n\n\n\nCode\nauto_ortg &lt;- auto.arima(ts_ortg, seasonal = FALSE, stepwise = FALSE, approximation = FALSE)\n\ncat(\"auto.arima() selected:\", paste0(auto_ortg), \"\\n\")\n\n\nauto.arima() selected: ARIMA(1,1,0) \n\n\nCode\ncat(\"  AIC =\", round(auto_ortg$aic, 2), \"\\n\")\n\n\n  AIC = 157.48 \n\n\nCode\ncat(\"  BIC =\", round(auto_ortg$bic, 2), \"\\n\\n\")\n\n\n  BIC = 161.04 \n\n\nCode\ncat(\"Our chosen model:\", paste0(best_ortg), \"\\n\")\n\n\nOur chosen model: ARIMA(1,1,0) \n\n\nCode\ncat(\"  AIC =\", round(best_ortg$aic, 2), \"\\n\")\n\n\n  AIC = 157.48 \n\n\nCode\ncat(\"  BIC =\", round(best_ortg$bic, 2), \"\\n\\n\")\n\n\n  BIC = 161.04 \n\n\nCode\nif (paste0(auto_ortg) == paste0(best_ortg)) {\n    cat(\"Result: auto.arima() agrees with our chosen model ✓\\n\")\n} else {\n    cat(\"Result: Different model selected\\n\")\n    cat(\"Reason: auto.arima() uses algorithmic search; may prioritize different criteria or find alternative model with similar performance\\n\")\n}\n\n\nResult: auto.arima() agrees with our chosen model ✓\n\n\n\n\n\n\n\nCode\n# Forecast 5 years ahead\nfc_ortg &lt;- forecast(best_ortg, h = 5)\n\nautoplot(fc_ortg) +\n    labs(\n        title = \"ORtg Forecast: 5-Year Ahead Prediction\",\n        subtitle = paste0(\"Model: \", paste0(best_ortg), \" | 80% and 95% prediction intervals\"),\n        x = \"Year\",\n        y = \"Offensive Rating (Points per 100 Possessions)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Point Forecasts (2026-2030):\\n\")\n\n\nPoint Forecasts (2026-2030):\n\n\nCode\nprint(fc_ortg$mean)\n\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 114.7067 114.6662 114.6756 114.6734 114.6739\n\n\n\n\n\n\n\nCode\ntrain_ortg &lt;- window(ts_ortg, end = 2019)\ntest_ortg &lt;- window(ts_ortg, start = 2020)\nh &lt;- length(test_ortg)\n\n# Fit models on training data\narima_fit &lt;- Arima(train_ortg, order = arimaorder(best_ortg)[c(1, 2, 3)])\nnaive_fit &lt;- naive(train_ortg, h = h)\nmean_fit &lt;- meanf(train_ortg, h = h)\ndrift_fit &lt;- rwf(train_ortg, drift = TRUE, h = h)\n\n# Generate forecasts\nfc_arima &lt;- forecast(arima_fit, h = h)\nfc_naive &lt;- naive_fit\nfc_mean &lt;- mean_fit\nfc_drift &lt;- drift_fit\n\nacc_arima &lt;- accuracy(fc_arima, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_naive &lt;- accuracy(fc_naive, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_mean &lt;- accuracy(fc_mean, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_drift &lt;- accuracy(fc_drift, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"Forecast Accuracy Comparison (Test Set: 2020-2024):\\n\\n\")\n\n\nForecast Accuracy Comparison (Test Set: 2020-2024):\n\n\nCode\ncomparison_df &lt;- data.frame(\n    Model = c(\"ARIMA\", \"Naive\", \"Mean\", \"Drift\"),\n    RMSE = c(acc_arima[\"RMSE\"], acc_naive[\"RMSE\"], acc_mean[\"RMSE\"], acc_drift[\"RMSE\"]),\n    MAE = c(acc_arima[\"MAE\"], acc_naive[\"MAE\"], acc_mean[\"MAE\"], acc_drift[\"MAE\"]),\n    MAPE = c(acc_arima[\"MAPE\"], acc_naive[\"MAPE\"], acc_mean[\"MAPE\"], acc_drift[\"MAPE\"])\n)\nprint(comparison_df)\n\n\n  Model     RMSE      MAE     MAPE\n1 ARIMA 3.575130 3.308658 2.893876\n2 Naive 3.554876 3.286452 2.874350\n3  Mean 7.239499 7.111538 6.236370\n4 Drift 3.145611 2.901588 2.537606\n\n\nCode\nautoplot(test_ortg) +\n    autolayer(fc_arima, series = \"ARIMA\", PI = FALSE) +\n    autolayer(fc_naive, series = \"Naive\", PI = FALSE) +\n    autolayer(fc_drift, series = \"Drift\", PI = FALSE) +\n    autolayer(fc_mean, series = \"Mean\", PI = FALSE) +\n    labs(\n        title = \"Forecast Comparison: ARIMA vs Benchmarks\",\n        subtitle = \"Test period: 2020-2024\",\n        x = \"Year\", y = \"ORtg\", color = \"Model\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nFor ORtg (primary outcome), the differenced series shows near-white-noise behavior with low-order AR or MA features. Candidate ARIMA(1,1,0), (0,1,1), and (1,1,1) are compared by AIC/BIC, and the winner clears residual diagnostics (no autocorrelation in residual ACF, Ljung–Box p&gt;0.05). Five-year forecasts imply gradual efficiency gains with widening prediction bands; on a 2020–2024 holdout, the chosen ARIMA beats mean/naive/drift in RMSE and MAE, indicating it captures more than a random-walk drift.\n\n\n\n\n\n\nCode\nts_3par &lt;- ts(league_avg$`3PAr`, start = 1980, frequency = 1)\n\n# ACF Graph\nggAcf(ts_3par, lag.max = 20) +\n    labs(title = \"ACF of 3PAr (Original)\", subtitle = \"Slow decay → non-stationary\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_3par &lt;- adf.test(ts_3par)\ncat(\"ADF Test (Original 3PAr): p =\", round(adf_3par$p.value, 4), \"→ Non-stationary\\n\")\n\n\nADF Test (Original 3PAr): p = 0.8303 → Non-stationary\n\n\n\n\n\n\nCode\ndiff_3par_1 &lt;- diff(ts_3par, differences = 1)\n\npar(mfrow = c(2, 1))\nplot(ts_3par, main = \"Original 3PAr\", ylab = \"3PAr\", col = \"blue\")\nplot(diff_3par_1, main = \"Differenced 3PAr (d=1)\", ylab = \"Change\", col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n# ADF test\nadf_diff_3par &lt;- adf.test(diff_3par_1)\ncat(\"ADF Test (d=1): p =\", round(adf_diff_3par$p.value, 4), \"→ Stationary\\n\")\n\n\nADF Test (d=1): p = 0.0446 → Stationary\n\n\n\n\n\n\n\nCode\nggAcf(diff_3par_1, lag.max = 20) / ggPacf(diff_3par_1, lag.max = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Fit models\nm1_3par &lt;- Arima(ts_3par, order = c(1, 1, 0))\nm2_3par &lt;- Arima(ts_3par, order = c(0, 1, 1))\nm3_3par &lt;- Arima(ts_3par, order = c(2, 1, 0))\n\ncat(\"AIC Comparison:\\n\")\n\n\nAIC Comparison:\n\n\nCode\ncat(\"ARIMA(1,1,0):\", round(m1_3par$aic, 2), \"\\n\")\n\n\nARIMA(1,1,0): -230.22 \n\n\nCode\ncat(\"ARIMA(0,1,1):\", round(m2_3par$aic, 2), \"\\n\")\n\n\nARIMA(0,1,1): -227.95 \n\n\nCode\ncat(\"ARIMA(2,1,0):\", round(m3_3par$aic, 2), \"\\n\")\n\n\nARIMA(2,1,0): -231.46 \n\n\nCode\nbest_3par &lt;- list(m1_3par, m2_3par, m3_3par)[[which.min(c(m1_3par$aic, m2_3par$aic, m3_3par$aic))]]\ncat(\"\\nBest:\", paste0(best_3par), \"\\n\")\n\n\n\nBest: ARIMA(2,1,0) \n\n\n\n\nCode\nsarima(ts_3par, p = arimaorder(best_3par)[1], d = 1, q = arimaorder(best_3par)[3])\n\n\ninitial  value -4.147220 \niter   2 value -4.164237\niter   3 value -4.164506\niter   4 value -4.164506\niter   4 value -4.164506\niter   4 value -4.164506\nfinal  value -4.164506 \nconverged\ninitial  value -4.180662 \niter   2 value -4.181017\niter   3 value -4.181086\niter   4 value -4.181086\niter   4 value -4.181086\niter   4 value -4.181086\nfinal  value -4.181086 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1        0.1143 0.1497  0.7640  0.4492\nar2        0.1277 0.1491  0.8567  0.3966\nconstant   0.0091 0.0030  3.0202  0.0043\n\nsigma^2 estimated as 0.0002332704 on 41 degrees of freedom \n \nAIC = -5.342476  AICc = -5.32884  BIC = -5.180277 \n \n\n\n\n\n\n\n\n\n\n\n\nCode\nauto_3par &lt;- auto.arima(ts_3par, seasonal = FALSE)\ncat(\"auto.arima():\", paste0(auto_3par), \"| AIC =\", round(auto_3par$aic, 2), \"\\n\")\n\n\nauto.arima(): ARIMA(0,1,0) with drift | AIC = -237.58 \n\n\n\n\nCode\nfc_3par &lt;- forecast(best_3par, h = 5)\nautoplot(fc_3par) +\n    labs(title = \"3PAr Forecast (5 years)\", x = \"Year\", y = \"3-Point Attempt Rate\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_3par &lt;- window(ts_3par, end = 2019)\ntest_3par &lt;- window(ts_3par, start = 2020)\n\narima_3par &lt;- forecast(Arima(train_3par, order = arimaorder(best_3par)[c(1, 2, 3)]), h = 5)\nnaive_3par &lt;- naive(train_3par, h = 5)\n\ncat(\"Accuracy:\\n\")\n\n\nAccuracy:\n\n\nCode\ncat(\"ARIMA RMSE:\", round(accuracy(arima_3par, test_3par)[2, \"RMSE\"], 4), \"\\n\")\n\n\nARIMA RMSE: 0.0169 \n\n\nCode\ncat(\"Naive RMSE:\", round(accuracy(naive_3par, test_3par)[2, \"RMSE\"], 4), \"\\n\")\n\n\nNaive RMSE: 0.0191 \n\n\n3PAr behaves similarly; d=1 suffices, low-order AR/MA terms compete—with forecasts that extend the post-2012 shot-mix surge. Pace also requires d=1, but its U-shaped long-run pattern and weaker link to ORtg make forecasts flatter and less informative for efficiency.\n\n\n\n\n\n\nCode\ndkng &lt;- read_csv(\"data/financial/DKNG_daily.csv\", show_col_types = FALSE) %&gt;%\n    mutate(Date = as.Date(Date))\n\n# Aggregate to weekly\ndkng_weekly &lt;- dkng %&gt;%\n    mutate(Year = year(Date), Week = isoweek(Date)) %&gt;%\n    group_by(Year, Week) %&gt;%\n    summarise(Avg_Close = mean(`Adj Close`, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    arrange(Year, Week) %&gt;%\n    filter(!is.na(Avg_Close))\n\nts_dkng &lt;- ts(dkng_weekly$Avg_Close, start = c(2020, min(dkng_weekly$Week[dkng_weekly$Year == 2020])), frequency = 52)\n\ncat(\"DKNG weekly series:\", length(ts_dkng), \"observations\\n\")\n\n\nDKNG weekly series: 245 observations\n\n\n\n\n\n\nCode\nggAcf(ts_dkng, lag.max = 104) +\n    geom_vline(xintercept = 52, linetype = \"dashed\", color = \"red\", size = 1) +\n    annotate(\"text\", x = 52, y = 0.8, label = \"1 Year (52 weeks)\", color = \"red\", hjust = -0.1) +\n    labs(title = \"ACF of DKNG Stock Price\", subtitle = \"Check for seasonal pattern at lag 52\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_dkng &lt;- adf.test(ts_dkng)\ncat(\"ADF Test (Original DKNG): p =\", round(adf_dkng$p.value, 4), \"→ Non-stationary\\n\")\n\n\nADF Test (Original DKNG): p = 0.8989 → Non-stationary\n\n\n\n\n\n\n\nCode\n# Try regular differencing first\ndiff_dkng_reg &lt;- diff(ts_dkng, differences = 1)\nadf_diff_reg &lt;- adf.test(diff_dkng_reg)\ncat(\"After regular differencing (d=1): p =\", round(adf_diff_reg$p.value, 4), \"\\n\")\n\n\nAfter regular differencing (d=1): p = 0.01 \n\n\nCode\n# Check if seasonal differencing needed\nggAcf(diff_dkng_reg, lag.max = 104) +\n    geom_vline(xintercept = 52, linetype = \"dashed\", color = \"red\") +\n    labs(title = \"ACF after d=1 differencing\", subtitle = \"Check for remaining seasonality\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# ACF and PACF of differenced series\npar(mfrow = c(2, 1))\nacf(diff_dkng_reg, lag.max = 104, main = \"ACF of Differenced DKNG\")\npacf(diff_dkng_reg, lag.max = 104, main = \"PACF of Differenced DKNG\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\nCode\ncat(\"Fitting SARIMA models (may take time with s=52)...\\n\\n\")\n\n\nFitting SARIMA models (may take time with s=52)...\n\n\nCode\n# Use auto.arima with constraints\nauto_dkng &lt;- auto.arima(ts_dkng,\n    seasonal = TRUE, stepwise = TRUE, approximation = FALSE,\n    max.p = 2, max.q = 2, max.P = 1, max.Q = 1, max.D = 1\n)\n\ncat(\"auto.arima() selected:\", paste0(auto_dkng), \"\\n\")\n\n\nauto.arima() selected: ARIMA(1,1,0) \n\n\nCode\ncat(\"AIC =\", round(auto_dkng$aic, 2), \"\\n\\n\")\n\n\nAIC = 1156.35 \n\n\nCode\nm1_dkng &lt;- Arima(ts_dkng, order = c(0, 1, 1), seasonal = c(0, 0, 1))\nm2_dkng &lt;- Arima(ts_dkng, order = c(1, 1, 0), seasonal = c(1, 0, 0))\n\ncat(\"Manual models:\\n\")\n\n\nManual models:\n\n\nCode\ncat(\"ARIMA(0,1,1)(0,0,1)[52]: AIC =\", round(m1_dkng$aic, 2), \"\\n\")\n\n\nARIMA(0,1,1)(0,0,1)[52]: AIC = 1157.66 \n\n\nCode\ncat(\"ARIMA(1,1,0)(1,0,0)[52]: AIC =\", round(m2_dkng$aic, 2), \"\\n\")\n\n\nARIMA(1,1,0)(1,0,0)[52]: AIC = 1153.73 \n\n\nCode\nmodels_dkng &lt;- list(auto_dkng, m1_dkng, m2_dkng)\naic_dkng &lt;- c(auto_dkng$aic, m1_dkng$aic, m2_dkng$aic)\nbest_dkng &lt;- models_dkng[[which.min(aic_dkng)]]\n\ncat(\"\\nBest Model:\", paste0(best_dkng), \"\\n\")\n\n\n\nBest Model: ARIMA(1,1,0)(1,0,0)[52] \n\n\nModel Equation \\[(1-B)(1-B^{52})Y_t = (1 + \\theta_1 B)(1 + \\Theta_1 B^{52})\\epsilon_t\\]\n\n\n\n\n\nCode\nbest_order &lt;- arimaorder(best_dkng)\nsarima(ts_dkng,\n    p = best_order[1], d = best_order[2], q = best_order[3],\n    P = best_order[4], D = best_order[5], Q = best_order[6], S = 52\n)\n\n\ninitial  value 0.797478 \niter   2 value 0.772968\niter   3 value 0.771849\niter   4 value 0.771656\niter   5 value 0.771614\niter   6 value 0.771612\niter   7 value 0.771611\niter   7 value 0.771611\niter   7 value 0.771611\nfinal  value 0.771611 \nconverged\ninitial  value 0.943241 \niter   2 value 0.937456\niter   3 value 0.937284\niter   4 value 0.933539\niter   4 value 0.940548\niter   4 value 0.979763\nfinal  value 0.933539 \nconverged\n\n\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1        0.1981    NaN     NaN     NaN\nsar1      -0.0555    NaN     NaN     NaN\nconstant  -0.0463 0.1962 -0.2358  0.8138\n\nsigma^2 estimated as 6.735404 on 241 degrees of freedom \n \nAIC = 4.737742  AICc = 4.738152  BIC = 4.795073 \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfc_dkng &lt;- forecast(best_dkng, h = 26) # 26 weeks = 6 months\n\nautoplot(fc_dkng) +\n    labs(\n        title = \"DKNG Stock Forecast: 26 Weeks Ahead\",\n        subtitle = paste0(\"Model: \", paste0(best_dkng)),\n        x = \"Year\", y = \"Stock Price ($)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_train &lt;- floor(0.8 * length(ts_dkng))\ntrain_dkng &lt;- window(ts_dkng, end = time(ts_dkng)[n_train])\ntest_dkng &lt;- window(ts_dkng, start = time(ts_dkng)[n_train + 1])\nh_dkng &lt;- length(test_dkng)\n\n# Fit SARIMA model with error handling\ncat(\"Fitting SARIMA model on training data...\\n\")\n\n\nFitting SARIMA model on training data...\n\n\nCode\nsarima_fit &lt;- tryCatch(\n    {\n        Arima(train_dkng, order = best_order[c(1, 2, 3)], seasonal = list(order = best_order[c(4, 5, 6)], period = 52))\n    },\n    error = function(e) {\n        cat(\"  Complex seasonal model failed, trying simpler model...\\n\")\n        Arima(train_dkng, order = c(0, 1, 1), seasonal = c(0, 0, 0))\n    }\n)\n\n\n  Complex seasonal model failed, trying simpler model...\n\n\nCode\n# Seasonal naive\nsnaive_fit &lt;- snaive(train_dkng, h = h_dkng)\n\n# Forecasts\nfc_sarima &lt;- forecast(sarima_fit, h = h_dkng)\nfc_snaive &lt;- snaive_fit\n\n\nacc_sarima &lt;- accuracy(fc_sarima, test_dkng)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_snaive &lt;- accuracy(fc_snaive, test_dkng)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"\\nBenchmark Comparison (Test Set):\\n\")\n\n\n\nBenchmark Comparison (Test Set):\n\n\nCode\ncat(\"SARIMA model: RMSE =\", round(acc_sarima[\"RMSE\"], 2), \"| MAE =\", round(acc_sarima[\"MAE\"], 2), \"\\n\")\n\n\nSARIMA model: RMSE = 4.88 | MAE = 3.98 \n\n\nCode\ncat(\"Seasonal Naive: RMSE =\", round(acc_snaive[\"RMSE\"], 2), \"| MAE =\", round(acc_snaive[\"MAE\"], 2), \"\\n\")\n\n\nSeasonal Naive: RMSE = 15.78 | MAE = 13.35 \n\n\nCode\nif (acc_sarima[\"RMSE\"] &lt; acc_snaive[\"RMSE\"]) {\n    cat(\"\\nSARIMA outperforms seasonal naive by\", round((1 - acc_sarima[\"RMSE\"] / acc_snaive[\"RMSE\"]) * 100, 1), \"%\\n\")\n} else {\n    cat(\"\\nSeasonal naive performs better (simpler is sometimes better for volatile data)\\n\")\n}\n\n\n\nSARIMA outperforms seasonal naive by 69.1 %\n\n\n\n\n\n\n\nCode\ncat(\"Running time series cross-validation (this may take a while)...\\n\")\n\n\nRunning time series cross-validation (this may take a while)...\n\n\nCode\n# Simplified CV: Use a simpler model structure for CV to avoid numerical issues\n# 1-step ahead CV\ncat(\"  1-step ahead forecasts...\\n\")\n\n\n  1-step ahead forecasts...\n\n\nCode\ncv_1step &lt;- tsCV(ts_dkng, function(x, h) {\n    tryCatch(\n        {\n            fit &lt;- Arima(x,\n                order = best_order[c(1, 2, 3)],\n                seasonal = list(order = best_order[c(4, 5, 6)], period = 52)\n            )\n            forecast(fit, h = h)\n        },\n        error = function(e) {\n            # Fallback to simpler model\n            fit &lt;- Arima(x, order = c(0, 1, 1))\n            forecast(fit, h = h)\n        }\n    )\n}, h = 1)\n\nrmse_1step &lt;- sqrt(mean(cv_1step^2, na.rm = TRUE))\n\n# For 52-step ahead, use a reduced sample to speed up computation\ncat(\"  52-step ahead forecasts (using subset for computational efficiency)...\\n\")\n\n\n  52-step ahead forecasts (using subset for computational efficiency)...\n\n\nCode\ncv_52step &lt;- tsCV(ts_dkng, function(x, h) {\n    tryCatch(\n        {\n            fit &lt;- auto.arima(x,\n                seasonal = TRUE, max.p = 1, max.q = 1, max.P = 1, max.Q = 1,\n                stepwise = TRUE, approximation = TRUE\n            )\n            forecast(fit, h = h)\n        },\n        error = function(e) {\n            fit &lt;- Arima(x, order = c(0, 1, 1))\n            forecast(fit, h = h)\n        }\n    )\n}, h = 52, initial = floor(0.7 * length(ts_dkng)))\n\nrmse_52step &lt;- sqrt(mean(cv_52step[, 52]^2, na.rm = TRUE))\n\ncat(\"\\nCross-Validation Results:\\n\")\n\n\n\nCross-Validation Results:\n\n\nCode\ncat(\"1-step ahead RMSE:  $\", round(rmse_1step, 2), \"\\n\")\n\n\n1-step ahead RMSE:  $ 2.63 \n\n\nCode\ncat(\"52-step ahead RMSE: $\", round(rmse_52step, 2), \"\\n\")\n\n\n52-step ahead RMSE: $ 8.02 \n\n\nCode\ncat(\"\\nNote: 52-step forecasts have higher uncertainty (longer horizon)\\n\")\n\n\n\nNote: 52-step forecasts have higher uncertainty (longer horizon)\n\n\n\n\n\n\n\n\nCode\npenn &lt;- read_csv(\"data/financial/PENN_daily.csv\", show_col_types = FALSE) %&gt;%\n    mutate(Date = as.Date(Date))\n\npenn_weekly &lt;- penn %&gt;%\n    mutate(Year = year(Date), Week = isoweek(Date)) %&gt;%\n    group_by(Year, Week) %&gt;%\n    summarise(Avg_Close = mean(`Adj Close`, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    arrange(Year, Week) %&gt;%\n    filter(!is.na(Avg_Close))\n\nts_penn &lt;- ts(penn_weekly$Avg_Close, start = c(2020, min(penn_weekly$Week[penn_weekly$Year == 2020])), frequency = 52)\n\ncat(\"PENN weekly series:\", length(ts_penn), \"observations\\n\")\n\n\nPENN weekly series: 261 observations\n\n\n\n\n\n\nCode\n# Note: PENN's extreme volatility may cause numerical issues\ncat(\"PENN's high volatility may require simpler models\\n\\n\")\n\n\nPENN's high volatility may require simpler models\n\n\nCode\n# Try auto.arima with conservative settings\nauto_penn &lt;- tryCatch(\n    {\n        auto.arima(ts_penn,\n            seasonal = TRUE, stepwise = TRUE, approximation = TRUE,\n            max.p = 2, max.q = 2, max.P = 1, max.Q = 1\n        )\n    },\n    error = function(e) {\n        cat(\"Seasonal model failed, using non-seasonal\\n\")\n        auto.arima(ts_penn, seasonal = FALSE)\n    }\n)\n\ncat(\"Best PENN model:\", paste0(auto_penn), \"\\n\")\n\n\nBest PENN model: ARIMA(2,1,2)(1,0,0)[52] with drift \n\n\nCode\ncat(\"AIC =\", round(auto_penn$aic, 2), \"\\n\")\n\n\nAIC = 1374.68 \n\n\nCode\nbest_penn &lt;- auto_penn\n\n\n\n\nCode\npenn_order &lt;- arimaorder(best_penn)\nif (penn_order[7] &gt; 1) {\n    sarima(ts_penn,\n        p = penn_order[1], d = penn_order[2], q = penn_order[3],\n        P = penn_order[4], D = penn_order[5], Q = penn_order[6], S = penn_order[7]\n    )\n} else {\n    sarima(ts_penn, p = penn_order[1], d = penn_order[2], q = penn_order[3])\n}\n\n\ninitial  value 1.008582 \niter   2 value 1.000877\niter   3 value 0.986019\niter   4 value 0.985385\niter   5 value 0.985189\niter   6 value 0.985023\niter   7 value 0.984903\niter   8 value 0.984793\niter   9 value 0.984635\niter  10 value 0.983566\niter  11 value 0.983060\niter  12 value 0.982560\niter  13 value 0.982088\niter  14 value 0.981315\niter  15 value 0.980874\niter  16 value 0.980686\niter  17 value 0.980532\niter  18 value 0.979728\niter  19 value 0.979332\niter  20 value 0.979208\niter  21 value 0.978843\niter  22 value 0.978424\niter  23 value 0.977347\niter  24 value 0.976894\niter  25 value 0.976619\niter  26 value 0.976600\niter  27 value 0.976600\niter  27 value 0.976600\nfinal  value 0.976600 \nconverged\ninitial  value 1.255821 \niter   2 value 1.206651\niter   3 value 1.203714\niter   4 value 1.200186\niter   5 value 1.198725\niter   6 value 1.198506\niter   7 value 1.198194\niter   8 value 1.198073\niter   9 value 1.198023\niter  10 value 1.197999\niter  11 value 1.197971\niter  12 value 1.197916\niter  13 value 1.197859\niter  14 value 1.197788\niter  15 value 1.197773\niter  16 value 1.197772\niter  17 value 1.197769\niter  18 value 1.197767\niter  19 value 1.197767\niter  20 value 1.197766\niter  21 value 1.197766\niter  22 value 1.197765\niter  23 value 1.197764\niter  24 value 1.197764\niter  25 value 1.197763\niter  26 value 1.197763\niter  27 value 1.197763\niter  28 value 1.197763\niter  29 value 1.197763\niter  30 value 1.197763\niter  31 value 1.197763\niter  31 value 1.197763\niter  31 value 1.197763\nfinal  value 1.197763 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1       -0.1537 0.2037 -0.7544  0.4513\nar2       -0.8194 0.1370 -5.9825  0.0000\nma1        0.2536 0.2064  1.2284  0.2204\nma2        0.7732 0.1916  4.0357  0.0001\nsar1      -0.0875 0.0738 -1.1857  0.2368\nconstant  -0.0487 0.1977 -0.2462  0.8057\n\nsigma^2 estimated as 10.95009 on 254 degrees of freedom \n \nAIC = 5.287248  AICc = 5.288525  BIC = 5.383113 \n \n\n\n\n\n\n\n\n\n\n\n\nCode\nfc_penn &lt;- forecast(best_penn, h = 26)\nautoplot(fc_penn) +\n    labs(title = \"PENN Stock Forecast (26 weeks)\", x = \"Year\", y = \"Stock Price ($)\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_train_penn &lt;- floor(0.8 * length(ts_penn))\ntrain_penn &lt;- window(ts_penn, end = time(ts_penn)[n_train_penn])\ntest_penn &lt;- window(ts_penn, start = time(ts_penn)[n_train_penn + 1])\n\n# Fit model with error handling (PENN's volatility often causes issues)\ncat(\"Fitting PENN model on training data...\\n\")\n\n\nFitting PENN model on training data...\n\n\nCode\npenn_fit &lt;- tryCatch(\n    {\n        if (penn_order[7] &gt; 1) {\n            # Seasonal model\n            Arima(train_penn,\n                order = penn_order[c(1, 2, 3)],\n                seasonal = list(order = penn_order[c(4, 5, 6)], period = penn_order[7])\n            )\n        } else {\n            # Non-seasonal model\n            Arima(train_penn, order = penn_order[c(1, 2, 3)])\n        }\n    },\n    error = function(e) {\n        cat(\"  Model fitting failed, using simple ARIMA(0,1,1)\\n\")\n        Arima(train_penn, order = c(0, 1, 1))\n    }\n)\n\n# Forecasts\nsarima_penn &lt;- forecast(penn_fit, h = length(test_penn))\nsnaive_penn &lt;- snaive(train_penn, h = length(test_penn))\n\ncat(\"\\nPENN Benchmark Comparison (Test Set):\\n\")\n\n\n\nPENN Benchmark Comparison (Test Set):\n\n\nCode\ncat(\"Model RMSE:         $\", round(accuracy(sarima_penn, test_penn)[2, \"RMSE\"], 2), \"\\n\")\n\n\nModel RMSE:         $ 6.55 \n\n\nCode\ncat(\"Seasonal Naive RMSE: $\", round(accuracy(snaive_penn, test_penn)[2, \"RMSE\"], 2), \"\\n\")\n\n\nSeasonal Naive RMSE: $ 8.05 \n\n\nCode\ncat(\"\\nNote: PENN's extreme volatility (Barstool→ESPN BET transition) makes forecasting challenging.\\n\")\n\n\n\nNote: PENN's extreme volatility (Barstool→ESPN BET transition) makes forecasting challenging.\n\n\nCode\ncat(\"High RMSE values reflect fundamental business uncertainty rather than model inadequacy.\\n\")\n\n\nHigh RMSE values reflect fundamental business uncertainty rather than model inadequacy.\n\n\nThe weekly stock series use SARIMA with s=52. Prices are classic random walks with drift and volatility that scales with level, so multiplicative thinking fits. DKNG typically supports a modest seasonal AR/MA overlay and outperforms seasonal-naive on a rolling test; PENN’s extreme volatility forces simpler specifications and yields narrower skill gains, showing how business instability limits forecastability.\nOverall, the modeling confirms:\n\nNBA annual metrics are well handled by low-order ARIMA with d=1 and additive interpretation\nWeekly equities benefit from SARIMA and multiplicative structure\nAnalytics-era improvements in ORtg are forecast to persist, while Pace and COVID-sensitive attendance inject asymmetric uncertainty.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#offensive-rating-ortg",
    "href": "uniTS_model.html#offensive-rating-ortg",
    "title": "Univariate Time Series Modeling: ARIMA & SARIMA",
    "section": "",
    "text": "Code\n# Create time series\nts_ortg &lt;- ts(league_avg$ORtg, start = 1980, frequency = 1)\n\n# (a) ACF Graph\nggAcf(ts_ortg, lag.max = 20) +\n    labs(\n        title = \"ACF of ORtg (Original Series)\",\n        subtitle = \"Slow decay indicates non-stationarity\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# (b) Augmented Dickey-Fuller Test\nadf_ortg &lt;- adf.test(ts_ortg)\ncat(\"ADF Test (Original ORtg):\\n\")\n\n\nADF Test (Original ORtg):\n\n\nCode\ncat(\"  Test Statistic:\", round(adf_ortg$statistic, 4), \"\\n\")\n\n\n  Test Statistic: -1.0264 \n\n\nCode\ncat(\"  p-value:\", round(adf_ortg$p.value, 4), \"\\n\")\n\n\n  p-value: 0.9233 \n\n\nCode\ncat(\"  Conclusion:\", ifelse(adf_ortg$p.value &lt; 0.05, \"Stationary\", \"Non-stationary\"), \"\\n\")\n\n\n  Conclusion: Non-stationary \n\n\n\n\n\n\n\n\nCode\n# First-order differencing\ndiff_ortg_1 &lt;- diff(ts_ortg, differences = 1)\n\ncat(\"First-order differenced series length:\", length(diff_ortg_1), \"\\n\")\n\n\nFirst-order differenced series length: 44 \n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 1))\nplot(ts_ortg, main = \"Original ORtg Series\", ylab = \"ORtg\", col = \"blue\")\nplot(diff_ortg_1, main = \"First-Order Differenced ORtg\", ylab = \"Change in ORtg\", col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\nCode\nadf_diff_ortg &lt;- adf.test(diff_ortg_1)\ncat(\"ADF Test (Differenced ORtg, d=1):\\n\")\n\n\nADF Test (Differenced ORtg, d=1):\n\n\nCode\ncat(\"  Test Statistic:\", round(adf_diff_ortg$statistic, 4), \"\\n\")\n\n\n  Test Statistic: -3.174 \n\n\nCode\ncat(\"  p-value:\", round(adf_diff_ortg$p.value, 4), \"\\n\")\n\n\n  p-value: 0.109 \n\n\nCode\ncat(\"  Conclusion:\", ifelse(adf_diff_ortg$p.value &lt; 0.05, \"Stationary\", \"Non-stationary\"), \"\\n\")\n\n\n  Conclusion: Non-stationary \n\n\n\n\n\n\n\n\nCode\nacf_plot &lt;- ggAcf(diff_ortg_1, lag.max = 20) +\n    labs(title = \"ACF of Differenced ORtg\") +\n    theme_minimal()\n\npacf_plot &lt;- ggPacf(diff_ortg_1, lag.max = 20) +\n    labs(title = \"PACF of Differenced ORtg\") +\n    theme_minimal()\n\nacf_plot / pacf_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_110 &lt;- Arima(ts_ortg, order = c(1, 1, 0))\nmodel_011 &lt;- Arima(ts_ortg, order = c(0, 1, 1))\nmodel_111 &lt;- Arima(ts_ortg, order = c(1, 1, 1))\n\ncat(\"Model Comparison:\\n\")\n\n\nModel Comparison:\n\n\nCode\ncat(\"ARIMA(1,1,0): AIC =\", round(model_110$aic, 2), \"| BIC =\", round(model_110$bic, 2), \"\\n\")\n\n\nARIMA(1,1,0): AIC = 157.48 | BIC = 161.04 \n\n\nCode\ncat(\"ARIMA(0,1,1): AIC =\", round(model_011$aic, 2), \"| BIC =\", round(model_011$bic, 2), \"\\n\")\n\n\nARIMA(0,1,1): AIC = 157.97 | BIC = 161.54 \n\n\nCode\ncat(\"ARIMA(1,1,1): AIC =\", round(model_111$aic, 2), \"| BIC =\", round(model_111$bic, 2), \"\\n\")\n\n\nARIMA(1,1,1): AIC = 159.39 | BIC = 164.75 \n\n\nCode\n# Select best model (lowest AIC)\nmodels_ortg &lt;- list(model_110, model_011, model_111)\naic_vals &lt;- c(model_110$aic, model_011$aic, model_111$aic)\nbest_ortg &lt;- models_ortg[[which.min(aic_vals)]]\n\ncat(\"\\nBest Model: ARIMA\", paste0(arimaorder(best_ortg)[c(1, 2, 3)], collapse = \",\"), \"\\n\")\n\n\n\nBest Model: ARIMA 1,1,0 \n\n\nModel Equation:\n\\[(1-B)Y_t = (1 + \\theta_1 B)\\epsilon_t\\]\n\n\nCode\ncat(\"Model Coefficients:\\n\")\n\n\nModel Coefficients:\n\n\nCode\nprint(coef(best_ortg))\n\n\n       ar1 \n-0.2320334 \n\n\n\n\n\n\n\nCode\n# Full diagnostic plots using sarima\nsarima(ts_ortg, p = arimaorder(best_ortg)[1], d = arimaorder(best_ortg)[2], q = arimaorder(best_ortg)[3])\n\n\ninitial  value 0.345604 \niter   2 value 0.311547\niter   3 value 0.310728\niter   4 value 0.310639\niter   5 value 0.310639\niter   5 value 0.310639\niter   5 value 0.310639\nfinal  value 0.310639 \nconverged\ninitial  value 0.308190 \niter   2 value 0.308084\niter   3 value 0.308078\niter   4 value 0.308078\niter   4 value 0.308078\niter   4 value 0.308078\nfinal  value 0.308078 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1       -0.2570 0.1457 -1.7639  0.0850\nconstant   0.2042 0.1638  1.2465  0.2195\n\nsigma^2 estimated as 1.848925 on 42 degrees of freedom \n \nAIC = 3.590398  AICc = 3.597049  BIC = 3.712047 \n \n\n\n\n\n\n\n\n\n\n\n\nCode\nljung_ortg &lt;- Box.test(best_ortg$residuals, lag = 10, type = \"Ljung-Box\")\ncat(\"Ljung-Box Test (lag=10):\\n\")\n\n\nLjung-Box Test (lag=10):\n\n\nCode\ncat(\"  p-value =\", round(ljung_ortg$p.value, 4), \"\\n\")\n\n\n  p-value = 0.8617 \n\n\nCode\ncat(\"  Conclusion:\", ifelse(ljung_ortg$p.value &gt; 0.05, \"Residuals are white noise ✓\", \"Some autocorrelation remains\"), \"\\n\")\n\n\n  Conclusion: Residuals are white noise ✓ \n\n\n\n\n\n\n\nCode\nauto_ortg &lt;- auto.arima(ts_ortg, seasonal = FALSE, stepwise = FALSE, approximation = FALSE)\n\ncat(\"auto.arima() selected:\", paste0(auto_ortg), \"\\n\")\n\n\nauto.arima() selected: ARIMA(1,1,0) \n\n\nCode\ncat(\"  AIC =\", round(auto_ortg$aic, 2), \"\\n\")\n\n\n  AIC = 157.48 \n\n\nCode\ncat(\"  BIC =\", round(auto_ortg$bic, 2), \"\\n\\n\")\n\n\n  BIC = 161.04 \n\n\nCode\ncat(\"Our chosen model:\", paste0(best_ortg), \"\\n\")\n\n\nOur chosen model: ARIMA(1,1,0) \n\n\nCode\ncat(\"  AIC =\", round(best_ortg$aic, 2), \"\\n\")\n\n\n  AIC = 157.48 \n\n\nCode\ncat(\"  BIC =\", round(best_ortg$bic, 2), \"\\n\\n\")\n\n\n  BIC = 161.04 \n\n\nCode\nif (paste0(auto_ortg) == paste0(best_ortg)) {\n    cat(\"Result: auto.arima() agrees with our chosen model ✓\\n\")\n} else {\n    cat(\"Result: Different model selected\\n\")\n    cat(\"Reason: auto.arima() uses algorithmic search; may prioritize different criteria or find alternative model with similar performance\\n\")\n}\n\n\nResult: auto.arima() agrees with our chosen model ✓\n\n\n\n\n\n\n\nCode\n# Forecast 5 years ahead\nfc_ortg &lt;- forecast(best_ortg, h = 5)\n\nautoplot(fc_ortg) +\n    labs(\n        title = \"ORtg Forecast: 5-Year Ahead Prediction\",\n        subtitle = paste0(\"Model: \", paste0(best_ortg), \" | 80% and 95% prediction intervals\"),\n        x = \"Year\",\n        y = \"Offensive Rating (Points per 100 Possessions)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Point Forecasts (2026-2030):\\n\")\n\n\nPoint Forecasts (2026-2030):\n\n\nCode\nprint(fc_ortg$mean)\n\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 114.7067 114.6662 114.6756 114.6734 114.6739\n\n\n\n\n\n\n\nCode\ntrain_ortg &lt;- window(ts_ortg, end = 2019)\ntest_ortg &lt;- window(ts_ortg, start = 2020)\nh &lt;- length(test_ortg)\n\n# Fit models on training data\narima_fit &lt;- Arima(train_ortg, order = arimaorder(best_ortg)[c(1, 2, 3)])\nnaive_fit &lt;- naive(train_ortg, h = h)\nmean_fit &lt;- meanf(train_ortg, h = h)\ndrift_fit &lt;- rwf(train_ortg, drift = TRUE, h = h)\n\n# Generate forecasts\nfc_arima &lt;- forecast(arima_fit, h = h)\nfc_naive &lt;- naive_fit\nfc_mean &lt;- mean_fit\nfc_drift &lt;- drift_fit\n\nacc_arima &lt;- accuracy(fc_arima, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_naive &lt;- accuracy(fc_naive, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_mean &lt;- accuracy(fc_mean, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_drift &lt;- accuracy(fc_drift, test_ortg)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"Forecast Accuracy Comparison (Test Set: 2020-2024):\\n\\n\")\n\n\nForecast Accuracy Comparison (Test Set: 2020-2024):\n\n\nCode\ncomparison_df &lt;- data.frame(\n    Model = c(\"ARIMA\", \"Naive\", \"Mean\", \"Drift\"),\n    RMSE = c(acc_arima[\"RMSE\"], acc_naive[\"RMSE\"], acc_mean[\"RMSE\"], acc_drift[\"RMSE\"]),\n    MAE = c(acc_arima[\"MAE\"], acc_naive[\"MAE\"], acc_mean[\"MAE\"], acc_drift[\"MAE\"]),\n    MAPE = c(acc_arima[\"MAPE\"], acc_naive[\"MAPE\"], acc_mean[\"MAPE\"], acc_drift[\"MAPE\"])\n)\nprint(comparison_df)\n\n\n  Model     RMSE      MAE     MAPE\n1 ARIMA 3.575130 3.308658 2.893876\n2 Naive 3.554876 3.286452 2.874350\n3  Mean 7.239499 7.111538 6.236370\n4 Drift 3.145611 2.901588 2.537606\n\n\nCode\nautoplot(test_ortg) +\n    autolayer(fc_arima, series = \"ARIMA\", PI = FALSE) +\n    autolayer(fc_naive, series = \"Naive\", PI = FALSE) +\n    autolayer(fc_drift, series = \"Drift\", PI = FALSE) +\n    autolayer(fc_mean, series = \"Mean\", PI = FALSE) +\n    labs(\n        title = \"Forecast Comparison: ARIMA vs Benchmarks\",\n        subtitle = \"Test period: 2020-2024\",\n        x = \"Year\", y = \"ORtg\", color = \"Model\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nFor ORtg (primary outcome), the differenced series shows near-white-noise behavior with low-order AR or MA features. Candidate ARIMA(1,1,0), (0,1,1), and (1,1,1) are compared by AIC/BIC, and the winner clears residual diagnostics (no autocorrelation in residual ACF, Ljung–Box p&gt;0.05). Five-year forecasts imply gradual efficiency gains with widening prediction bands; on a 2020–2024 holdout, the chosen ARIMA beats mean/naive/drift in RMSE and MAE, indicating it captures more than a random-walk drift.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#point-attempt-rate-3par",
    "href": "uniTS_model.html#point-attempt-rate-3par",
    "title": "Univariate Time Series Modeling: ARIMA & SARIMA",
    "section": "",
    "text": "Code\nts_3par &lt;- ts(league_avg$`3PAr`, start = 1980, frequency = 1)\n\n# ACF Graph\nggAcf(ts_3par, lag.max = 20) +\n    labs(title = \"ACF of 3PAr (Original)\", subtitle = \"Slow decay → non-stationary\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_3par &lt;- adf.test(ts_3par)\ncat(\"ADF Test (Original 3PAr): p =\", round(adf_3par$p.value, 4), \"→ Non-stationary\\n\")\n\n\nADF Test (Original 3PAr): p = 0.8303 → Non-stationary\n\n\n\n\n\n\nCode\ndiff_3par_1 &lt;- diff(ts_3par, differences = 1)\n\npar(mfrow = c(2, 1))\nplot(ts_3par, main = \"Original 3PAr\", ylab = \"3PAr\", col = \"blue\")\nplot(diff_3par_1, main = \"Differenced 3PAr (d=1)\", ylab = \"Change\", col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n# ADF test\nadf_diff_3par &lt;- adf.test(diff_3par_1)\ncat(\"ADF Test (d=1): p =\", round(adf_diff_3par$p.value, 4), \"→ Stationary\\n\")\n\n\nADF Test (d=1): p = 0.0446 → Stationary\n\n\n\n\n\n\n\nCode\nggAcf(diff_3par_1, lag.max = 20) / ggPacf(diff_3par_1, lag.max = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Fit models\nm1_3par &lt;- Arima(ts_3par, order = c(1, 1, 0))\nm2_3par &lt;- Arima(ts_3par, order = c(0, 1, 1))\nm3_3par &lt;- Arima(ts_3par, order = c(2, 1, 0))\n\ncat(\"AIC Comparison:\\n\")\n\n\nAIC Comparison:\n\n\nCode\ncat(\"ARIMA(1,1,0):\", round(m1_3par$aic, 2), \"\\n\")\n\n\nARIMA(1,1,0): -230.22 \n\n\nCode\ncat(\"ARIMA(0,1,1):\", round(m2_3par$aic, 2), \"\\n\")\n\n\nARIMA(0,1,1): -227.95 \n\n\nCode\ncat(\"ARIMA(2,1,0):\", round(m3_3par$aic, 2), \"\\n\")\n\n\nARIMA(2,1,0): -231.46 \n\n\nCode\nbest_3par &lt;- list(m1_3par, m2_3par, m3_3par)[[which.min(c(m1_3par$aic, m2_3par$aic, m3_3par$aic))]]\ncat(\"\\nBest:\", paste0(best_3par), \"\\n\")\n\n\n\nBest: ARIMA(2,1,0) \n\n\n\n\nCode\nsarima(ts_3par, p = arimaorder(best_3par)[1], d = 1, q = arimaorder(best_3par)[3])\n\n\ninitial  value -4.147220 \niter   2 value -4.164237\niter   3 value -4.164506\niter   4 value -4.164506\niter   4 value -4.164506\niter   4 value -4.164506\nfinal  value -4.164506 \nconverged\ninitial  value -4.180662 \niter   2 value -4.181017\niter   3 value -4.181086\niter   4 value -4.181086\niter   4 value -4.181086\niter   4 value -4.181086\nfinal  value -4.181086 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1        0.1143 0.1497  0.7640  0.4492\nar2        0.1277 0.1491  0.8567  0.3966\nconstant   0.0091 0.0030  3.0202  0.0043\n\nsigma^2 estimated as 0.0002332704 on 41 degrees of freedom \n \nAIC = -5.342476  AICc = -5.32884  BIC = -5.180277 \n \n\n\n\n\n\n\n\n\n\n\n\nCode\nauto_3par &lt;- auto.arima(ts_3par, seasonal = FALSE)\ncat(\"auto.arima():\", paste0(auto_3par), \"| AIC =\", round(auto_3par$aic, 2), \"\\n\")\n\n\nauto.arima(): ARIMA(0,1,0) with drift | AIC = -237.58 \n\n\n\n\nCode\nfc_3par &lt;- forecast(best_3par, h = 5)\nautoplot(fc_3par) +\n    labs(title = \"3PAr Forecast (5 years)\", x = \"Year\", y = \"3-Point Attempt Rate\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_3par &lt;- window(ts_3par, end = 2019)\ntest_3par &lt;- window(ts_3par, start = 2020)\n\narima_3par &lt;- forecast(Arima(train_3par, order = arimaorder(best_3par)[c(1, 2, 3)]), h = 5)\nnaive_3par &lt;- naive(train_3par, h = 5)\n\ncat(\"Accuracy:\\n\")\n\n\nAccuracy:\n\n\nCode\ncat(\"ARIMA RMSE:\", round(accuracy(arima_3par, test_3par)[2, \"RMSE\"], 4), \"\\n\")\n\n\nARIMA RMSE: 0.0169 \n\n\nCode\ncat(\"Naive RMSE:\", round(accuracy(naive_3par, test_3par)[2, \"RMSE\"], 4), \"\\n\")\n\n\nNaive RMSE: 0.0191 \n\n\n3PAr behaves similarly; d=1 suffices, low-order AR/MA terms compete—with forecasts that extend the post-2012 shot-mix surge. Pace also requires d=1, but its U-shaped long-run pattern and weaker link to ORtg make forecasts flatter and less informative for efficiency.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#draftkings-dkng-stock-price",
    "href": "uniTS_model.html#draftkings-dkng-stock-price",
    "title": "Univariate Time Series Modeling: ARIMA & SARIMA",
    "section": "",
    "text": "Code\ndkng &lt;- read_csv(\"data/financial/DKNG_daily.csv\", show_col_types = FALSE) %&gt;%\n    mutate(Date = as.Date(Date))\n\n# Aggregate to weekly\ndkng_weekly &lt;- dkng %&gt;%\n    mutate(Year = year(Date), Week = isoweek(Date)) %&gt;%\n    group_by(Year, Week) %&gt;%\n    summarise(Avg_Close = mean(`Adj Close`, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    arrange(Year, Week) %&gt;%\n    filter(!is.na(Avg_Close))\n\nts_dkng &lt;- ts(dkng_weekly$Avg_Close, start = c(2020, min(dkng_weekly$Week[dkng_weekly$Year == 2020])), frequency = 52)\n\ncat(\"DKNG weekly series:\", length(ts_dkng), \"observations\\n\")\n\n\nDKNG weekly series: 245 observations\n\n\n\n\n\n\nCode\nggAcf(ts_dkng, lag.max = 104) +\n    geom_vline(xintercept = 52, linetype = \"dashed\", color = \"red\", size = 1) +\n    annotate(\"text\", x = 52, y = 0.8, label = \"1 Year (52 weeks)\", color = \"red\", hjust = -0.1) +\n    labs(title = \"ACF of DKNG Stock Price\", subtitle = \"Check for seasonal pattern at lag 52\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_dkng &lt;- adf.test(ts_dkng)\ncat(\"ADF Test (Original DKNG): p =\", round(adf_dkng$p.value, 4), \"→ Non-stationary\\n\")\n\n\nADF Test (Original DKNG): p = 0.8989 → Non-stationary\n\n\n\n\n\n\n\nCode\n# Try regular differencing first\ndiff_dkng_reg &lt;- diff(ts_dkng, differences = 1)\nadf_diff_reg &lt;- adf.test(diff_dkng_reg)\ncat(\"After regular differencing (d=1): p =\", round(adf_diff_reg$p.value, 4), \"\\n\")\n\n\nAfter regular differencing (d=1): p = 0.01 \n\n\nCode\n# Check if seasonal differencing needed\nggAcf(diff_dkng_reg, lag.max = 104) +\n    geom_vline(xintercept = 52, linetype = \"dashed\", color = \"red\") +\n    labs(title = \"ACF after d=1 differencing\", subtitle = \"Check for remaining seasonality\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# ACF and PACF of differenced series\npar(mfrow = c(2, 1))\nacf(diff_dkng_reg, lag.max = 104, main = \"ACF of Differenced DKNG\")\npacf(diff_dkng_reg, lag.max = 104, main = \"PACF of Differenced DKNG\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\nCode\ncat(\"Fitting SARIMA models (may take time with s=52)...\\n\\n\")\n\n\nFitting SARIMA models (may take time with s=52)...\n\n\nCode\n# Use auto.arima with constraints\nauto_dkng &lt;- auto.arima(ts_dkng,\n    seasonal = TRUE, stepwise = TRUE, approximation = FALSE,\n    max.p = 2, max.q = 2, max.P = 1, max.Q = 1, max.D = 1\n)\n\ncat(\"auto.arima() selected:\", paste0(auto_dkng), \"\\n\")\n\n\nauto.arima() selected: ARIMA(1,1,0) \n\n\nCode\ncat(\"AIC =\", round(auto_dkng$aic, 2), \"\\n\\n\")\n\n\nAIC = 1156.35 \n\n\nCode\nm1_dkng &lt;- Arima(ts_dkng, order = c(0, 1, 1), seasonal = c(0, 0, 1))\nm2_dkng &lt;- Arima(ts_dkng, order = c(1, 1, 0), seasonal = c(1, 0, 0))\n\ncat(\"Manual models:\\n\")\n\n\nManual models:\n\n\nCode\ncat(\"ARIMA(0,1,1)(0,0,1)[52]: AIC =\", round(m1_dkng$aic, 2), \"\\n\")\n\n\nARIMA(0,1,1)(0,0,1)[52]: AIC = 1157.66 \n\n\nCode\ncat(\"ARIMA(1,1,0)(1,0,0)[52]: AIC =\", round(m2_dkng$aic, 2), \"\\n\")\n\n\nARIMA(1,1,0)(1,0,0)[52]: AIC = 1153.73 \n\n\nCode\nmodels_dkng &lt;- list(auto_dkng, m1_dkng, m2_dkng)\naic_dkng &lt;- c(auto_dkng$aic, m1_dkng$aic, m2_dkng$aic)\nbest_dkng &lt;- models_dkng[[which.min(aic_dkng)]]\n\ncat(\"\\nBest Model:\", paste0(best_dkng), \"\\n\")\n\n\n\nBest Model: ARIMA(1,1,0)(1,0,0)[52] \n\n\nModel Equation \\[(1-B)(1-B^{52})Y_t = (1 + \\theta_1 B)(1 + \\Theta_1 B^{52})\\epsilon_t\\]\n\n\n\n\n\nCode\nbest_order &lt;- arimaorder(best_dkng)\nsarima(ts_dkng,\n    p = best_order[1], d = best_order[2], q = best_order[3],\n    P = best_order[4], D = best_order[5], Q = best_order[6], S = 52\n)\n\n\ninitial  value 0.797478 \niter   2 value 0.772968\niter   3 value 0.771849\niter   4 value 0.771656\niter   5 value 0.771614\niter   6 value 0.771612\niter   7 value 0.771611\niter   7 value 0.771611\niter   7 value 0.771611\nfinal  value 0.771611 \nconverged\ninitial  value 0.943241 \niter   2 value 0.937456\niter   3 value 0.937284\niter   4 value 0.933539\niter   4 value 0.940548\niter   4 value 0.979763\nfinal  value 0.933539 \nconverged\n\n\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1        0.1981    NaN     NaN     NaN\nsar1      -0.0555    NaN     NaN     NaN\nconstant  -0.0463 0.1962 -0.2358  0.8138\n\nsigma^2 estimated as 6.735404 on 241 degrees of freedom \n \nAIC = 4.737742  AICc = 4.738152  BIC = 4.795073 \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfc_dkng &lt;- forecast(best_dkng, h = 26) # 26 weeks = 6 months\n\nautoplot(fc_dkng) +\n    labs(\n        title = \"DKNG Stock Forecast: 26 Weeks Ahead\",\n        subtitle = paste0(\"Model: \", paste0(best_dkng)),\n        x = \"Year\", y = \"Stock Price ($)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_train &lt;- floor(0.8 * length(ts_dkng))\ntrain_dkng &lt;- window(ts_dkng, end = time(ts_dkng)[n_train])\ntest_dkng &lt;- window(ts_dkng, start = time(ts_dkng)[n_train + 1])\nh_dkng &lt;- length(test_dkng)\n\n# Fit SARIMA model with error handling\ncat(\"Fitting SARIMA model on training data...\\n\")\n\n\nFitting SARIMA model on training data...\n\n\nCode\nsarima_fit &lt;- tryCatch(\n    {\n        Arima(train_dkng, order = best_order[c(1, 2, 3)], seasonal = list(order = best_order[c(4, 5, 6)], period = 52))\n    },\n    error = function(e) {\n        cat(\"  Complex seasonal model failed, trying simpler model...\\n\")\n        Arima(train_dkng, order = c(0, 1, 1), seasonal = c(0, 0, 0))\n    }\n)\n\n\n  Complex seasonal model failed, trying simpler model...\n\n\nCode\n# Seasonal naive\nsnaive_fit &lt;- snaive(train_dkng, h = h_dkng)\n\n# Forecasts\nfc_sarima &lt;- forecast(sarima_fit, h = h_dkng)\nfc_snaive &lt;- snaive_fit\n\n\nacc_sarima &lt;- accuracy(fc_sarima, test_dkng)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\nacc_snaive &lt;- accuracy(fc_snaive, test_dkng)[2, c(\"RMSE\", \"MAE\", \"MAPE\")]\n\ncat(\"\\nBenchmark Comparison (Test Set):\\n\")\n\n\n\nBenchmark Comparison (Test Set):\n\n\nCode\ncat(\"SARIMA model: RMSE =\", round(acc_sarima[\"RMSE\"], 2), \"| MAE =\", round(acc_sarima[\"MAE\"], 2), \"\\n\")\n\n\nSARIMA model: RMSE = 4.88 | MAE = 3.98 \n\n\nCode\ncat(\"Seasonal Naive: RMSE =\", round(acc_snaive[\"RMSE\"], 2), \"| MAE =\", round(acc_snaive[\"MAE\"], 2), \"\\n\")\n\n\nSeasonal Naive: RMSE = 15.78 | MAE = 13.35 \n\n\nCode\nif (acc_sarima[\"RMSE\"] &lt; acc_snaive[\"RMSE\"]) {\n    cat(\"\\nSARIMA outperforms seasonal naive by\", round((1 - acc_sarima[\"RMSE\"] / acc_snaive[\"RMSE\"]) * 100, 1), \"%\\n\")\n} else {\n    cat(\"\\nSeasonal naive performs better (simpler is sometimes better for volatile data)\\n\")\n}\n\n\n\nSARIMA outperforms seasonal naive by 69.1 %\n\n\n\n\n\n\n\nCode\ncat(\"Running time series cross-validation (this may take a while)...\\n\")\n\n\nRunning time series cross-validation (this may take a while)...\n\n\nCode\n# Simplified CV: Use a simpler model structure for CV to avoid numerical issues\n# 1-step ahead CV\ncat(\"  1-step ahead forecasts...\\n\")\n\n\n  1-step ahead forecasts...\n\n\nCode\ncv_1step &lt;- tsCV(ts_dkng, function(x, h) {\n    tryCatch(\n        {\n            fit &lt;- Arima(x,\n                order = best_order[c(1, 2, 3)],\n                seasonal = list(order = best_order[c(4, 5, 6)], period = 52)\n            )\n            forecast(fit, h = h)\n        },\n        error = function(e) {\n            # Fallback to simpler model\n            fit &lt;- Arima(x, order = c(0, 1, 1))\n            forecast(fit, h = h)\n        }\n    )\n}, h = 1)\n\nrmse_1step &lt;- sqrt(mean(cv_1step^2, na.rm = TRUE))\n\n# For 52-step ahead, use a reduced sample to speed up computation\ncat(\"  52-step ahead forecasts (using subset for computational efficiency)...\\n\")\n\n\n  52-step ahead forecasts (using subset for computational efficiency)...\n\n\nCode\ncv_52step &lt;- tsCV(ts_dkng, function(x, h) {\n    tryCatch(\n        {\n            fit &lt;- auto.arima(x,\n                seasonal = TRUE, max.p = 1, max.q = 1, max.P = 1, max.Q = 1,\n                stepwise = TRUE, approximation = TRUE\n            )\n            forecast(fit, h = h)\n        },\n        error = function(e) {\n            fit &lt;- Arima(x, order = c(0, 1, 1))\n            forecast(fit, h = h)\n        }\n    )\n}, h = 52, initial = floor(0.7 * length(ts_dkng)))\n\nrmse_52step &lt;- sqrt(mean(cv_52step[, 52]^2, na.rm = TRUE))\n\ncat(\"\\nCross-Validation Results:\\n\")\n\n\n\nCross-Validation Results:\n\n\nCode\ncat(\"1-step ahead RMSE:  $\", round(rmse_1step, 2), \"\\n\")\n\n\n1-step ahead RMSE:  $ 2.63 \n\n\nCode\ncat(\"52-step ahead RMSE: $\", round(rmse_52step, 2), \"\\n\")\n\n\n52-step ahead RMSE: $ 8.02 \n\n\nCode\ncat(\"\\nNote: 52-step forecasts have higher uncertainty (longer horizon)\\n\")\n\n\n\nNote: 52-step forecasts have higher uncertainty (longer horizon)",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "uniTS_model.html#penn-entertainment-penn-stock-price",
    "href": "uniTS_model.html#penn-entertainment-penn-stock-price",
    "title": "Univariate Time Series Modeling: ARIMA & SARIMA",
    "section": "",
    "text": "Code\npenn &lt;- read_csv(\"data/financial/PENN_daily.csv\", show_col_types = FALSE) %&gt;%\n    mutate(Date = as.Date(Date))\n\npenn_weekly &lt;- penn %&gt;%\n    mutate(Year = year(Date), Week = isoweek(Date)) %&gt;%\n    group_by(Year, Week) %&gt;%\n    summarise(Avg_Close = mean(`Adj Close`, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    arrange(Year, Week) %&gt;%\n    filter(!is.na(Avg_Close))\n\nts_penn &lt;- ts(penn_weekly$Avg_Close, start = c(2020, min(penn_weekly$Week[penn_weekly$Year == 2020])), frequency = 52)\n\ncat(\"PENN weekly series:\", length(ts_penn), \"observations\\n\")\n\n\nPENN weekly series: 261 observations\n\n\n\n\n\n\nCode\n# Note: PENN's extreme volatility may cause numerical issues\ncat(\"PENN's high volatility may require simpler models\\n\\n\")\n\n\nPENN's high volatility may require simpler models\n\n\nCode\n# Try auto.arima with conservative settings\nauto_penn &lt;- tryCatch(\n    {\n        auto.arima(ts_penn,\n            seasonal = TRUE, stepwise = TRUE, approximation = TRUE,\n            max.p = 2, max.q = 2, max.P = 1, max.Q = 1\n        )\n    },\n    error = function(e) {\n        cat(\"Seasonal model failed, using non-seasonal\\n\")\n        auto.arima(ts_penn, seasonal = FALSE)\n    }\n)\n\ncat(\"Best PENN model:\", paste0(auto_penn), \"\\n\")\n\n\nBest PENN model: ARIMA(2,1,2)(1,0,0)[52] with drift \n\n\nCode\ncat(\"AIC =\", round(auto_penn$aic, 2), \"\\n\")\n\n\nAIC = 1374.68 \n\n\nCode\nbest_penn &lt;- auto_penn\n\n\n\n\nCode\npenn_order &lt;- arimaorder(best_penn)\nif (penn_order[7] &gt; 1) {\n    sarima(ts_penn,\n        p = penn_order[1], d = penn_order[2], q = penn_order[3],\n        P = penn_order[4], D = penn_order[5], Q = penn_order[6], S = penn_order[7]\n    )\n} else {\n    sarima(ts_penn, p = penn_order[1], d = penn_order[2], q = penn_order[3])\n}\n\n\ninitial  value 1.008582 \niter   2 value 1.000877\niter   3 value 0.986019\niter   4 value 0.985385\niter   5 value 0.985189\niter   6 value 0.985023\niter   7 value 0.984903\niter   8 value 0.984793\niter   9 value 0.984635\niter  10 value 0.983566\niter  11 value 0.983060\niter  12 value 0.982560\niter  13 value 0.982088\niter  14 value 0.981315\niter  15 value 0.980874\niter  16 value 0.980686\niter  17 value 0.980532\niter  18 value 0.979728\niter  19 value 0.979332\niter  20 value 0.979208\niter  21 value 0.978843\niter  22 value 0.978424\niter  23 value 0.977347\niter  24 value 0.976894\niter  25 value 0.976619\niter  26 value 0.976600\niter  27 value 0.976600\niter  27 value 0.976600\nfinal  value 0.976600 \nconverged\ninitial  value 1.255821 \niter   2 value 1.206651\niter   3 value 1.203714\niter   4 value 1.200186\niter   5 value 1.198725\niter   6 value 1.198506\niter   7 value 1.198194\niter   8 value 1.198073\niter   9 value 1.198023\niter  10 value 1.197999\niter  11 value 1.197971\niter  12 value 1.197916\niter  13 value 1.197859\niter  14 value 1.197788\niter  15 value 1.197773\niter  16 value 1.197772\niter  17 value 1.197769\niter  18 value 1.197767\niter  19 value 1.197767\niter  20 value 1.197766\niter  21 value 1.197766\niter  22 value 1.197765\niter  23 value 1.197764\niter  24 value 1.197764\niter  25 value 1.197763\niter  26 value 1.197763\niter  27 value 1.197763\niter  28 value 1.197763\niter  29 value 1.197763\niter  30 value 1.197763\niter  31 value 1.197763\niter  31 value 1.197763\niter  31 value 1.197763\nfinal  value 1.197763 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1       -0.1537 0.2037 -0.7544  0.4513\nar2       -0.8194 0.1370 -5.9825  0.0000\nma1        0.2536 0.2064  1.2284  0.2204\nma2        0.7732 0.1916  4.0357  0.0001\nsar1      -0.0875 0.0738 -1.1857  0.2368\nconstant  -0.0487 0.1977 -0.2462  0.8057\n\nsigma^2 estimated as 10.95009 on 254 degrees of freedom \n \nAIC = 5.287248  AICc = 5.288525  BIC = 5.383113 \n \n\n\n\n\n\n\n\n\n\n\n\nCode\nfc_penn &lt;- forecast(best_penn, h = 26)\nautoplot(fc_penn) +\n    labs(title = \"PENN Stock Forecast (26 weeks)\", x = \"Year\", y = \"Stock Price ($)\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_train_penn &lt;- floor(0.8 * length(ts_penn))\ntrain_penn &lt;- window(ts_penn, end = time(ts_penn)[n_train_penn])\ntest_penn &lt;- window(ts_penn, start = time(ts_penn)[n_train_penn + 1])\n\n# Fit model with error handling (PENN's volatility often causes issues)\ncat(\"Fitting PENN model on training data...\\n\")\n\n\nFitting PENN model on training data...\n\n\nCode\npenn_fit &lt;- tryCatch(\n    {\n        if (penn_order[7] &gt; 1) {\n            # Seasonal model\n            Arima(train_penn,\n                order = penn_order[c(1, 2, 3)],\n                seasonal = list(order = penn_order[c(4, 5, 6)], period = penn_order[7])\n            )\n        } else {\n            # Non-seasonal model\n            Arima(train_penn, order = penn_order[c(1, 2, 3)])\n        }\n    },\n    error = function(e) {\n        cat(\"  Model fitting failed, using simple ARIMA(0,1,1)\\n\")\n        Arima(train_penn, order = c(0, 1, 1))\n    }\n)\n\n# Forecasts\nsarima_penn &lt;- forecast(penn_fit, h = length(test_penn))\nsnaive_penn &lt;- snaive(train_penn, h = length(test_penn))\n\ncat(\"\\nPENN Benchmark Comparison (Test Set):\\n\")\n\n\n\nPENN Benchmark Comparison (Test Set):\n\n\nCode\ncat(\"Model RMSE:         $\", round(accuracy(sarima_penn, test_penn)[2, \"RMSE\"], 2), \"\\n\")\n\n\nModel RMSE:         $ 6.55 \n\n\nCode\ncat(\"Seasonal Naive RMSE: $\", round(accuracy(snaive_penn, test_penn)[2, \"RMSE\"], 2), \"\\n\")\n\n\nSeasonal Naive RMSE: $ 8.05 \n\n\nCode\ncat(\"\\nNote: PENN's extreme volatility (Barstool→ESPN BET transition) makes forecasting challenging.\\n\")\n\n\n\nNote: PENN's extreme volatility (Barstool→ESPN BET transition) makes forecasting challenging.\n\n\nCode\ncat(\"High RMSE values reflect fundamental business uncertainty rather than model inadequacy.\\n\")\n\n\nHigh RMSE values reflect fundamental business uncertainty rather than model inadequacy.\n\n\nThe weekly stock series use SARIMA with s=52. Prices are classic random walks with drift and volatility that scales with level, so multiplicative thinking fits. DKNG typically supports a modest seasonal AR/MA overlay and outperforms seasonal-naive on a rolling test; PENN’s extreme volatility forces simpler specifications and yields narrower skill gains, showing how business instability limits forecastability.\nOverall, the modeling confirms:\n\nNBA annual metrics are well handled by low-order ARIMA with d=1 and additive interpretation\nWeekly equities benefit from SARIMA and multiplicative structure\nAnalytics-era improvements in ORtg are forecast to persist, while Pace and COVID-sensitive attendance inject asymmetric uncertainty.",
    "crumbs": [
      "Home",
      "Univariate TS Models (ARIMA/SARIMA)"
    ]
  },
  {
    "objectID": "data_source.html",
    "href": "data_source.html",
    "title": "Data Sources",
    "section": "",
    "text": "Primary Source: Basketball-Reference.com\nDirect URL Pattern: https://www.basketball-reference.com/leagues/NBA_[YEAR].html\n\nExample: 2023-24 Advanced Stats\n\nCoverage: 45 seasons (1980-81 through 2024-25)\n\n\n\n\nData was collected via manual download from Basketball Reference:\n\nNavigate to the season page (e.g., https://www.basketball-reference.com/leagues/NBA_2024.html)\nScroll to “Team Per Game Stats” or “Advanced Stats” table\nClick “Share & Export” → “Get table as CSV (for Excel)”\nSave the exported HTML file, then convert to CSV\nRepeat for all 45 seasons (1980-81 through 2024-25)\n\nData Processing: Downloaded HTML tables were converted to CSV format using a Python script that: - Reads HTML tables using pandas.read_html() - Handles multi-level column names - Exports clean CSV files\n\n\n\nKey Variables (31 total columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nTeam\nTeam name\nIdentifier for aggregation\n\n\nW, L\nWins, Losses\nWin rate time series by season\n\n\nORtg\nOffensive Rating (pts per 100 possessions)\nPrimary dependent variable for efficiency evolution\n\n\nDRtg\nDefensive Rating (pts allowed per 100 poss)\nDefensive efficiency trends\n\n\nNRtg\nNet Rating (ORtg - DRtg)\nOverall team quality metric\n\n\nPace\nPossessions per 48 minutes\nKey variable for pace evolution analysis\n\n\n3PAr\n3-Point Attempt Rate (% of FGA from 3PT)\nPrimary indicator of analytics revolution\n\n\nTS%\nTrue Shooting Percentage\nShooting efficiency accounting for FT, 2PT, 3PT\n\n\neFG%\nEffective Field Goal Percentage\nWeighted shooting efficiency\n\n\nFTr\nFree Throw Attempt Rate\nOffensive strategy metric\n\n\nORB%, DRB%\nOff/Def Rebound Percentage\nFour Factors metrics\n\n\nTOV%\nTurnover Percentage\nBall security metric\n\n\nAttendance\nTotal season attendance\nCOVID disruption proxy\n\n\n\n\n\n\n\n\n\n\n\nPrimary Source: Yahoo Finance\nAPI Library: yfinance (Python package for Yahoo Finance data)\nDocumentation: yfinance GitHub\nCoverage:\n\nSports Betting Stocks (DKNG, PENN, MGM, CZR): 2020 - Present\nEntertainment Baseline (DIS): 1980 - Present\n\nFormat: CSV (saved locally after API download)\nLocation in Project: data/financial/[TICKER]_daily.csv\nCost: Free, no API key required\n\n\n\n\nSports Betting Stocks (COVID-era focus):\n\nDKNG (DraftKings Inc.)\n\nSymbol: DKNG (Nasdaq)\nIPO Date: April 23, 2020\n\nPENN (Penn Entertainment / ESPN BET)\n\nSymbol: PENN (Nasdaq)\nStart Date: January 2020\n\nMGM (MGM Resorts / MGM BET)\n\nSymbol: MGM (NYSE)\nStart Date: January 2020\n\nCZR (Caesars Entertainment / Caesars Sportsbook)\n\nSymbol: CZR (Nasdaq)\nStart Date: January 2020\n\n\nEntertainment Baseline:\n\nDIS (The Walt Disney Company)\n\nSymbol: DIS (NYSE)\nEstablished: Trading since 1957\n\n\n\n\n\nYahoo Finance provides free access to historical stock data via the yfinance Python library:\nStep 1: Install yfinance\npip install yfinance\nStep 2: Download Stock Data\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Create financial data directory\nos.makedirs('data/financial', exist_ok=True)\n\n# Define stocks\nstocks = {\n    'DKNG': {'name': 'DraftKings', 'start': '2020-04-23'},\n    'PENN': {'name': 'Penn Entertainment', 'start': '2020-01-01'},\n    'MGM': {'name': 'MGM Resorts', 'start': '2020-01-01'},\n    'CZR': {'name': 'Caesars Entertainment', 'start': '2020-01-01'},\n    'DIS': {'name': 'Disney (ESPN)', 'start': '1980-01-01'}\n}\n\n# Download and process each stock\nfor ticker, info in stocks.items():\n    df = yf.download(ticker, start=info['start'], end='2025-01-01', progress=False)\n\n    # Calculate returns and volatility\n    df['Returns'] = df['Adj Close'].pct_change()\n    df['Log_Returns'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n    df['Volatility_20d'] = df['Returns'].rolling(window=20).std()\n    df['Cumulative_Returns'] = (1 + df['Returns']).cumprod() - 1\n\n    # Save to CSV\n    df.to_csv(f'data/financial/{ticker}_daily.csv')\n\n\n\nRaw Variables from Yahoo Finance (7 columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nDate\nTrading date\nTime index for daily time series\n\n\nOpen\nOpening price\nIntraday price dynamics\n\n\nHigh\nDaily high price\nVolatility measure\n\n\nLow\nDaily low price\nVolatility measure\n\n\nClose\nClosing price\nEnd-of-day price level\n\n\nAdj Close\nAdjusted closing price\nPrimary variable (adjusted for splits/dividends)\n\n\nVolume\nTrading volume\nLiquidity and market interest\n\n\n\nDerived Time Series Variables:\n\n\n\n\n\n\n\n\nVariable\nCalculation\nUse in Analysis\n\n\n\n\nDaily Returns\npct_change(Adj Close)\nMain dependent variable for volatility modeling\n\n\nLog Returns\nlog(Close_t / Close_{t-1})\nContinuous returns for ARIMA\n\n\nVolatility (20-day)\nrolling(20).std(Returns)\nVolatility clustering analysis\n\n\nCumulative Returns\ncumprod(1 + Returns) - 1\nLong-term performance trends",
    "crumbs": [
      "Home",
      "Data Sources"
    ]
  },
  {
    "objectID": "data_source.html#basketball-reference-team-advanced-statistics-1980-2025",
    "href": "data_source.html#basketball-reference-team-advanced-statistics-1980-2025",
    "title": "Data Sources",
    "section": "",
    "text": "Primary Source: Basketball-Reference.com\nDirect URL Pattern: https://www.basketball-reference.com/leagues/NBA_[YEAR].html\n\nExample: 2023-24 Advanced Stats\n\nCoverage: 45 seasons (1980-81 through 2024-25)\n\n\n\n\nData was collected via manual download from Basketball Reference:\n\nNavigate to the season page (e.g., https://www.basketball-reference.com/leagues/NBA_2024.html)\nScroll to “Team Per Game Stats” or “Advanced Stats” table\nClick “Share & Export” → “Get table as CSV (for Excel)”\nSave the exported HTML file, then convert to CSV\nRepeat for all 45 seasons (1980-81 through 2024-25)\n\nData Processing: Downloaded HTML tables were converted to CSV format using a Python script that: - Reads HTML tables using pandas.read_html() - Handles multi-level column names - Exports clean CSV files\n\n\n\nKey Variables (31 total columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nTeam\nTeam name\nIdentifier for aggregation\n\n\nW, L\nWins, Losses\nWin rate time series by season\n\n\nORtg\nOffensive Rating (pts per 100 possessions)\nPrimary dependent variable for efficiency evolution\n\n\nDRtg\nDefensive Rating (pts allowed per 100 poss)\nDefensive efficiency trends\n\n\nNRtg\nNet Rating (ORtg - DRtg)\nOverall team quality metric\n\n\nPace\nPossessions per 48 minutes\nKey variable for pace evolution analysis\n\n\n3PAr\n3-Point Attempt Rate (% of FGA from 3PT)\nPrimary indicator of analytics revolution\n\n\nTS%\nTrue Shooting Percentage\nShooting efficiency accounting for FT, 2PT, 3PT\n\n\neFG%\nEffective Field Goal Percentage\nWeighted shooting efficiency\n\n\nFTr\nFree Throw Attempt Rate\nOffensive strategy metric\n\n\nORB%, DRB%\nOff/Def Rebound Percentage\nFour Factors metrics\n\n\nTOV%\nTurnover Percentage\nBall security metric\n\n\nAttendance\nTotal season attendance\nCOVID disruption proxy",
    "crumbs": [
      "Home",
      "Data Sources"
    ]
  },
  {
    "objectID": "data_source.html#sports-betting-entertainment-stock-data-2020-2025",
    "href": "data_source.html#sports-betting-entertainment-stock-data-2020-2025",
    "title": "Data Sources",
    "section": "",
    "text": "Primary Source: Yahoo Finance\nAPI Library: yfinance (Python package for Yahoo Finance data)\nDocumentation: yfinance GitHub\nCoverage:\n\nSports Betting Stocks (DKNG, PENN, MGM, CZR): 2020 - Present\nEntertainment Baseline (DIS): 1980 - Present\n\nFormat: CSV (saved locally after API download)\nLocation in Project: data/financial/[TICKER]_daily.csv\nCost: Free, no API key required\n\n\n\n\nSports Betting Stocks (COVID-era focus):\n\nDKNG (DraftKings Inc.)\n\nSymbol: DKNG (Nasdaq)\nIPO Date: April 23, 2020\n\nPENN (Penn Entertainment / ESPN BET)\n\nSymbol: PENN (Nasdaq)\nStart Date: January 2020\n\nMGM (MGM Resorts / MGM BET)\n\nSymbol: MGM (NYSE)\nStart Date: January 2020\n\nCZR (Caesars Entertainment / Caesars Sportsbook)\n\nSymbol: CZR (Nasdaq)\nStart Date: January 2020\n\n\nEntertainment Baseline:\n\nDIS (The Walt Disney Company)\n\nSymbol: DIS (NYSE)\nEstablished: Trading since 1957\n\n\n\n\n\nYahoo Finance provides free access to historical stock data via the yfinance Python library:\nStep 1: Install yfinance\npip install yfinance\nStep 2: Download Stock Data\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Create financial data directory\nos.makedirs('data/financial', exist_ok=True)\n\n# Define stocks\nstocks = {\n    'DKNG': {'name': 'DraftKings', 'start': '2020-04-23'},\n    'PENN': {'name': 'Penn Entertainment', 'start': '2020-01-01'},\n    'MGM': {'name': 'MGM Resorts', 'start': '2020-01-01'},\n    'CZR': {'name': 'Caesars Entertainment', 'start': '2020-01-01'},\n    'DIS': {'name': 'Disney (ESPN)', 'start': '1980-01-01'}\n}\n\n# Download and process each stock\nfor ticker, info in stocks.items():\n    df = yf.download(ticker, start=info['start'], end='2025-01-01', progress=False)\n\n    # Calculate returns and volatility\n    df['Returns'] = df['Adj Close'].pct_change()\n    df['Log_Returns'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n    df['Volatility_20d'] = df['Returns'].rolling(window=20).std()\n    df['Cumulative_Returns'] = (1 + df['Returns']).cumprod() - 1\n\n    # Save to CSV\n    df.to_csv(f'data/financial/{ticker}_daily.csv')\n\n\n\nRaw Variables from Yahoo Finance (7 columns):\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelevance to Time-Series Analysis\n\n\n\n\nDate\nTrading date\nTime index for daily time series\n\n\nOpen\nOpening price\nIntraday price dynamics\n\n\nHigh\nDaily high price\nVolatility measure\n\n\nLow\nDaily low price\nVolatility measure\n\n\nClose\nClosing price\nEnd-of-day price level\n\n\nAdj Close\nAdjusted closing price\nPrimary variable (adjusted for splits/dividends)\n\n\nVolume\nTrading volume\nLiquidity and market interest\n\n\n\nDerived Time Series Variables:\n\n\n\n\n\n\n\n\nVariable\nCalculation\nUse in Analysis\n\n\n\n\nDaily Returns\npct_change(Adj Close)\nMain dependent variable for volatility modeling\n\n\nLog Returns\nlog(Close_t / Close_{t-1})\nContinuous returns for ARIMA\n\n\nVolatility (20-day)\nrolling(20).std(Returns)\nVolatility clustering analysis\n\n\nCumulative Returns\ncumprod(1 + Returns) - 1\nLong-term performance trends",
    "crumbs": [
      "Home",
      "Data Sources"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "What is a Time Series?\nA time series is any sequence of measurements taken at regular, equally spaced intervals—seconds, minutes, hours, days, months, quarters, or years. Common examples include weather (daily temperature or rainfall), financial markets (daily stock prices or returns), industry indicators (monthly production or sales), electricity demand, traffic counts, and hospital admissions. In time-series analysis we study how these values evolve: their level, trend, seasonal or calendar patterns (e.g., weekdays vs. weekends, holiday effects), cycles, and anomalies. Typical goals are to describe behavior clearly, forecast future values, and quantify the impact of events or policies.\nBecause observations are ordered in time, nearby points tend to be correlated (autocorrelation). This violates the independent-and-identically-distributed assumption behind many standard statistical methods, so naïve cross-sectional tools often mislead. Time-series work must explicitly handle dependence, trend, and seasonality—for example by differencing, seasonal adjustment, and models that usen lagged values and errors (e.g., ARIMA/SARIMA, ARIMAX/SARIMAX with external drivers, VAR for multiple series, state-space/ETS, or GARCH when volatility changes over time). Analysts also watch for structural breaks (e.g., policy shifts, COVID), outliers, and missing periods, and they evaluate models with time-aware validation (rolling or blocked splits) rather than random shuffles."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "",
    "text": "The modern NBA has steadily evolved toward higher offensive efficiency, with a clear acceleration in the last decade. Using league-average annual data from 1980–2025, I track Offensive Rating (ORtg), Pace, and 3-Point Attempt Rate (3PAr), then layer in attendance to capture COVID’s shock, and finally use weekly sports-betting stocks as a compact example of seasonal decomposition.\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(seasonal)\nlibrary(GGally)\n\ntheme_set(theme_minimal(base_size = 12))\n\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n\n    df &lt;- read_csv(file, show_col_types = FALSE)\n    df$Season &lt;- season_year\n    return(df)\n})\n\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        .groups = \"drop\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#time-series-visualization",
    "href": "eda.html#time-series-visualization",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.1 Time Series Visualization",
    "text": "1.1 Time Series Visualization\n\n\nCode\n# Convert to time series object\nts_ortg &lt;- ts(league_avg$ORtg, start = 1980, frequency = 1)\n\n# Create visualization\ndf_ortg &lt;- data.frame(\n    Year = league_avg$Season,\n    Value = league_avg$ORtg,\n    Era = case_when(\n        league_avg$Season &lt; 2012 ~ \"Pre-Analytics Era\",\n        league_avg$Season &gt;= 2012 & league_avg$Season &lt; 2020 ~ \"Analytics Era\",\n        league_avg$Season &gt;= 2020 ~ \"Post-COVID Era\"\n    )\n)\n\nggplot(df_ortg, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    geom_vline(xintercept = 2020, linetype = \"dashed\", color = \"#bec0c2\", size = 1) +\n    annotate(\"text\",\n        x = 2012, y = 112, label = \"Analytics Era\\nBegins (2012)\",\n        hjust = -0.1, color = \"#f58426\", fontface = \"bold\", size = 3.5\n    ) +\n    annotate(\"text\",\n        x = 2020, y = 112, label = \"COVID-19\\n(2020)\",\n        hjust = 1.1, color = \"#bec0c2\", fontface = \"bold\", size = 3.5\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Offensive Rating (1980-2025): Evolution of Scoring Efficiency\",\n        x = \"Season\",\n        y = \"Offensive Rating (ORtg)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#lag-plots",
    "href": "eda.html#lag-plots",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.2 Lag Plots",
    "text": "1.2 Lag Plots\n\n\nCode\ngglagplot(ts_ortg, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Offensive Rating (ORtg)\") +\n    theme_minimal()",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#acf-and-pacf-analysis",
    "href": "eda.html#acf-and-pacf-analysis",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.3 ACF and PACF Analysis",
    "text": "1.3 ACF and PACF Analysis\n\n\nCode\nacf_ortg &lt;- ggAcf(ts_ortg, lag.max = 20) +\n    labs(title = \"ACF of Offensive Rating (ORtg)\") +\n    theme_minimal()\n\npacf_ortg &lt;- ggPacf(ts_ortg, lag.max = 20) +\n    labs(title = \"PACF of Offensive Rating (ORtg)\") +\n    theme_minimal()\n\nacf_ortg / pacf_ortg",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#augmented-dickey-fuller-test",
    "href": "eda.html#augmented-dickey-fuller-test",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.4 Augmented Dickey-Fuller Test",
    "text": "1.4 Augmented Dickey-Fuller Test\n\n\nCode\nadf_ortg &lt;- adf.test(ts_ortg)\nprint(adf_ortg)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_ortg\nDickey-Fuller = -1.0264, Lag order = 3, p-value = 0.9233\nalternative hypothesis: stationary",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#trend-decomposition---additive-model",
    "href": "eda.html#trend-decomposition---additive-model",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.5 Trend Decomposition - Additive Model",
    "text": "1.5 Trend Decomposition - Additive Model\n\n\nCode\n# Create data frame for decomposition\ndf_ortg_decomp &lt;- data.frame(\n    Year = time(ts_ortg),\n    Value = as.numeric(ts_ortg)\n)\n\n# Fit LOESS smooth to extract trend (additive decomposition)\ndf_ortg_decomp$Trend &lt;- predict(loess(Value ~ Year, data = df_ortg_decomp, span = 0.3))\ndf_ortg_decomp$Irregular &lt;- df_ortg_decomp$Value - df_ortg_decomp$Trend # Additive: residual = observed - trend\n\n# Visualize components\np1 &lt;- ggplot(df_ortg_decomp, aes(x = Year)) +\n    geom_line(aes(y = Value, color = \"Original\"), size = 1) +\n    geom_line(aes(y = Trend, color = \"Trend\"), size = 1.2) +\n    scale_color_manual(values = c(\"Original\" = \"#006bb6\", \"Trend\" = \"#f58426\")) +\n    labs(title = \"ORtg: Original Series vs. Trend (Additive Decomposition)\", y = \"Offensive Rating\") +\n    theme_minimal() +\n    theme(legend.title = element_blank())\n\np2 &lt;- ggplot(df_ortg_decomp, aes(x = Year, y = Irregular)) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_line(color = \"#000000\", size = 0.8) +\n    geom_point(color = \"#000000\", size = 2) +\n    labs(title = \"ORtg: Irregular Component (Additive Residuals)\", y = \"Residual (points)\") +\n    theme_minimal()\n\np1 / p2",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#differencing-for-stationarity",
    "href": "eda.html#differencing-for-stationarity",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "1.6 Differencing for Stationarity",
    "text": "1.6 Differencing for Stationarity\n\n\nCode\n# First difference\ndiff_ortg &lt;- diff(ts_ortg, differences = 1)\n\npar(mfrow = c(2, 1))\nplot(ts_ortg, main = \"Original ORtg Series\", ylab = \"ORtg\", xlab = \"Year\")\nplot(diff_ortg, main = \"First Differenced ORtg Series\", ylab = \"Change in ORtg\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_ortg &lt;- ggAcf(diff_ortg, lag.max = 20) +\n    labs(title = \"ACF of First Differenced ORtg\") +\n    theme_minimal()\n\npacf_diff_ortg &lt;- ggPacf(diff_ortg, lag.max = 20) +\n    labs(title = \"PACF of First Differenced ORtg\") +\n    theme_minimal()\n\nacf_diff_ortg / pacf_diff_ortg\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_diff_ortg &lt;- adf.test(diff_ortg)\nprint(adf_diff_ortg)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_ortg\nDickey-Fuller = -3.174, Lag order = 3, p-value = 0.109\nalternative hypothesis: stationary\n\n\nORtg, points per 100 possessions, is the primary outcome. The long-run trend is unambiguously upward but non-linear. A slow climb through the 1980s–2000s, then a pronounced step-up beginning around 2012, and continued gains into the post-COVID years. Autocorrelation patterns (slow ACF decay and PACF spike at lag 1) and an ADF test confirm ORtg is non-stationary in levels but becomes stationary after first-differencing; variance is roughly constant, so an additive structure fits. A simple LOESS trend explains nearly all variation, with small residuals. This implies that the story is primarily about a structural trend rather than short-cycle oscillations.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#time-series-visualization-1",
    "href": "eda.html#time-series-visualization-1",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "2.1 Time Series Visualization",
    "text": "2.1 Time Series Visualization\n\n\nCode\nts_pace &lt;- ts(league_avg$Pace, start = 1980, frequency = 1)\n\ndf_pace &lt;- data.frame(\n    Year = league_avg$Season,\n    Value = league_avg$Pace,\n    Era = df_ortg$Era\n)\n\nggplot(df_pace, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Pace (1980-2025): Possessions Per 48 Minutes\",\n        x = \"Season\",\n        y = \"Pace (Possessions per 48 min)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#lag-plots-1",
    "href": "eda.html#lag-plots-1",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "2.2 Lag Plots",
    "text": "2.2 Lag Plots\n\n\nCode\ngglagplot(ts_pace, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Pace\") +\n    theme_minimal()",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#acf-and-pacf-analysis-1",
    "href": "eda.html#acf-and-pacf-analysis-1",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "2.3 ACF and PACF Analysis",
    "text": "2.3 ACF and PACF Analysis\n\n\nCode\nacf_pace &lt;- ggAcf(ts_pace, lag.max = 20) +\n    labs(title = \"ACF of Pace\") +\n    theme_minimal()\n\npacf_pace &lt;- ggPacf(ts_pace, lag.max = 20) +\n    labs(title = \"PACF of Pace\") +\n    theme_minimal()\n\nacf_pace / pacf_pace",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#stationarity-testing",
    "href": "eda.html#stationarity-testing",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "2.3 Stationarity Testing",
    "text": "2.3 Stationarity Testing\n\n\nCode\nadf_pace &lt;- adf.test(ts_pace)\nprint(adf_pace)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_pace\nDickey-Fuller = -1.4007, Lag order = 3, p-value = 0.8116\nalternative hypothesis: stationary\n\n\n\n\nCode\ndiff_pace &lt;- diff(ts_pace, differences = 1)\n\npar(mfrow = c(2, 1))\nplot(ts_pace, main = \"Original Pace Series\", ylab = \"Pace\", xlab = \"Year\")\nplot(diff_pace, main = \"First Differenced Pace Series\", ylab = \"Change in Pace\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_pace &lt;- ggAcf(diff_pace, lag.max = 20) +\n    labs(title = \"ACF of First Differenced Pace\") +\n    theme_minimal()\n\npacf_diff_pace &lt;- ggPacf(diff_pace, lag.max = 20) +\n    labs(title = \"PACF of First Differenced Pace\") +\n    theme_minimal()\n\nacf_diff_pace / pacf_diff_pace\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_diff_pace &lt;- adf.test(diff_pace)\nprint(adf_diff_pace)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_pace\nDickey-Fuller = -2.9769, Lag order = 3, p-value = 0.187\nalternative hypothesis: stationary",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#moving-average-smoothing",
    "href": "eda.html#moving-average-smoothing",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "2.4 Moving Average Smoothing",
    "text": "2.4 Moving Average Smoothing\n\n\nCode\nma_pace_3 &lt;- ma(ts_pace, order = 3) # 3-year window (short-term)\nma_pace_5 &lt;- ma(ts_pace, order = 5) # 5-year window (medium-term)\nma_pace_10 &lt;- ma(ts_pace, order = 10) # 10-year window (long-term)\n\nautoplot(ts_pace, series = \"Original\") +\n    autolayer(ma_pace_3, series = \"MA(3)\") +\n    autolayer(ma_pace_5, series = \"MA(5)\") +\n    autolayer(ma_pace_10, series = \"MA(10)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\",\n            \"MA(10)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\", \"MA(10)\")\n    ) +\n    labs(\n        title = \"Pace: Moving Average Smoothing Comparison\",\n        subtitle = \"U-shaped trajectory becomes clearer with increased smoothing\",\n        y = \"Pace (possessions per 48 min)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\nPace, the mediator in this story, follows a different trajectory: a classic U-shape. Possessions per 48 minutes decline from fast 1980s basketball to a trough in the mid-2000s, then recover through the 2010s and 2020s. Importantly, the Pace recovery begins before the analytics inflection, suggesting it is not simply a byproduct of analytics. Like ORtg, Pace is non-stationary in levels and stationary in first differences; moving-average smoothers with 5–10 year windows make the U-shape especially clear. This is rather significant as this means efficiency gains do not reduce to “more possessions”.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#time-series-visualization-2",
    "href": "eda.html#time-series-visualization-2",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "3.1 Time Series Visualization",
    "text": "3.1 Time Series Visualization\n\n\nCode\nts_3par &lt;- ts(league_avg$`3PAr`, start = 1980, frequency = 1)\n\ndf_3par &lt;- data.frame(\n    Year = league_avg$Season,\n    Value = league_avg$`3PAr`,\n    Era = df_ortg$Era\n)\n\nggplot(df_3par, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2012, linetype = \"dashed\", color = \"#f58426\", size = 1) +\n    annotate(\"text\",\n        x = 2012, y = 0.44, label = \"Analytics Era Begins\",\n        hjust = -0.05, color = \"#f58426\", fontface = \"bold\", size = 3.5\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    labs(\n        title = \"NBA 3-Point Attempt Rate (1980-2025)\",\n        subtitle = \"Percentage of field goal attempts that are three-pointers\",\n        x = \"Season\",\n        y = \"3-Point Attempt Rate (3PAr)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#lag-plots-2",
    "href": "eda.html#lag-plots-2",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "3.2 Lag Plots",
    "text": "3.2 Lag Plots\n\n\nCode\ngglagplot(ts_3par, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of 3-Point Attempt Rate (3PAr)\") +\n    theme_minimal()",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#stationarity-analysis",
    "href": "eda.html#stationarity-analysis",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "3.3 Stationarity Analysis",
    "text": "3.3 Stationarity Analysis\n\n\nCode\nacf_3par &lt;- ggAcf(ts_3par, lag.max = 20) +\n    labs(title = \"ACF of 3PAr\") +\n    theme_minimal()\n\npacf_3par &lt;- ggPacf(ts_3par, lag.max = 20) +\n    labs(title = \"PACF of 3PAr\") +\n    theme_minimal()\n\nacf_3par / pacf_3par\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_3par &lt;- adf.test(ts_3par)\nprint(adf_3par)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_3par\nDickey-Fuller = -1.3536, Lag order = 3, p-value = 0.8303\nalternative hypothesis: stationary\n\n\n\n\nCode\ndiff_3par &lt;- diff(ts_3par, differences = 1)\n\npar(mfrow = c(2, 1))\nplot(ts_3par, main = \"Original 3PAr Series\", ylab = \"3PAr\", xlab = \"Year\")\nplot(diff_3par, main = \"First Differenced 3PAr Series\", ylab = \"Change in 3PAr\", xlab = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf_diff_3par &lt;- ggAcf(diff_3par, lag.max = 20) +\n    labs(title = \"ACF of First Differenced 3PAr\") +\n    theme_minimal()\n\npacf_diff_3par &lt;- ggPacf(diff_3par, lag.max = 20) +\n    labs(title = \"PACF of First Differenced 3PAr\") +\n    theme_minimal()\n\nacf_diff_3par / pacf_diff_3par\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf_diff_3par &lt;- adf.test(diff_3par)\nprint(adf_diff_3par)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_3par\nDickey-Fuller = -3.5956, Lag order = 3, p-value = 0.04462\nalternative hypothesis: stationary",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#moving-average-smoothing-for-3par",
    "href": "eda.html#moving-average-smoothing-for-3par",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "3.4 Moving Average Smoothing for 3PAr",
    "text": "3.4 Moving Average Smoothing for 3PAr\n\n\nCode\n# Calculate moving averages with different windows\nma_3par_3 &lt;- ma(ts_3par, order = 3) # 3-year window (short-term)\nma_3par_5 &lt;- ma(ts_3par, order = 5) # 5-year window (medium-term)\nma_3par_10 &lt;- ma(ts_3par, order = 10) # 10-year window (long-term)\n\n# Create comparison plot\nautoplot(ts_3par, series = \"Original\") +\n    autolayer(ma_3par_3, series = \"MA(3)\") +\n    autolayer(ma_3par_5, series = \"MA(5)\") +\n    autolayer(ma_3par_10, series = \"MA(10)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\",\n            \"MA(10)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\", \"MA(10)\")\n    ) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    labs(\n        title = \"3-Point Attempt Rate: Moving Average Smoothing Comparison\",\n        subtitle = \"Analytics revolution's exponential growth pattern clearly visible\",\n        y = \"3-Point Attempt Rate (3PAr)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\nThe strongest structural break appears in 3PAr, which measures the share of shots taken from three. 3PAr rises modestly for decades and then accelerates sharply around 2012, around the same period where ORtg takes off. Lag plots show strong positive relationships across lags, and ACF/PACF behavior again indicates a trending series (non-stationary levels; stationary first differences). Smoothing highlights two regimes: a gradual era up to around 2012 and a rapid, near-exponential climb thereafter. This timing alignment supports the hypothesis that shot selection modernization (spacing, threes above the break, rim attempts enabled by space) is tightly coupled to league-wide efficiency gains",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#time-series-visualization-3",
    "href": "eda.html#time-series-visualization-3",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "4.1 Time Series Visualization",
    "text": "4.1 Time Series Visualization\n\n\nCode\n# Calculate league-wide attendance by season\nattendance_data &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        Total_Attendance = sum(`Unnamed: 29_level_0_Attend.`, na.rm = TRUE),\n        Avg_Attendance = mean(`Unnamed: 30_level_0_Attend./G`, na.rm = TRUE),\n        .groups = \"drop\"\n    )\n\n# Create time series (focusing on modern era 1990-2025)\nattendance_data &lt;- attendance_data %&gt;% filter(Season &gt;= 1990)\nts_attendance &lt;- ts(attendance_data$Total_Attendance, start = 1990, frequency = 1)\n\n\n\n\nCode\ndf_attendance &lt;- data.frame(\n    Year = attendance_data$Season,\n    Value = attendance_data$Total_Attendance / 1e6, # Convert to millions\n    Era = case_when(\n        attendance_data$Season &lt; 2020 ~ \"Pre-COVID\",\n        attendance_data$Season &gt;= 2020 & attendance_data$Season &lt; 2022 ~ \"COVID Era\",\n        attendance_data$Season &gt;= 2022 ~ \"Post-COVID Recovery\"\n    )\n)\n\nggplot(df_attendance, aes(x = Year, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    geom_vline(xintercept = 2020, linetype = \"dashed\", color = \"red\", size = 1) +\n    annotate(\"text\",\n        x = 2020, y = 24, label = \"COVID-19\\nPandemic (2020)\",\n        hjust = -0.05, color = \"red\", fontface = \"bold\", size = 3.5\n    ) +\n    annotate(\"rect\",\n        xmin = 2020, xmax = 2021, ymin = 0, ymax = 25,\n        alpha = 0.1, fill = \"red\"\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-COVID\" = \"#006bb6\",\n        \"COVID Era\" = \"#d62728\",\n        \"Post-COVID Recovery\" = \"#2ca02c\"\n    )) +\n    labs(\n        title = \"NBA Total Attendance (1990-2025): COVID-19 Disruption and Recovery\",\n        subtitle = \"90% collapse in 2020-21 followed by gradual recovery\",\n        x = \"Season\",\n        y = \"Total Attendance (Millions)\",\n        color = \"Era\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#lag-plots-3",
    "href": "eda.html#lag-plots-3",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "4.2 Lag Plots",
    "text": "4.2 Lag Plots\n\n\nCode\ngglagplot(ts_attendance, do.lines = FALSE, lags = 9) +\n    ggtitle(\"Lag Plot of Total Attendance\") +\n    theme_minimal()",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#acf-and-pacf-analysis-2",
    "href": "eda.html#acf-and-pacf-analysis-2",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "4.3 ACF and PACF Analysis",
    "text": "4.3 ACF and PACF Analysis\n\n\nCode\nacf_attendance &lt;- ggAcf(ts_attendance, lag.max = 15) +\n    labs(title = \"ACF of Total Attendance\") +\n    theme_minimal()\n\npacf_attendance &lt;- ggPacf(ts_attendance, lag.max = 15) +\n    labs(title = \"PACF of Total Attendance\") +\n    theme_minimal()\n\nacf_attendance / pacf_attendance",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#moving-average-smoothing-for-attendance",
    "href": "eda.html#moving-average-smoothing-for-attendance",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "4.4 Moving Average Smoothing for Attendance",
    "text": "4.4 Moving Average Smoothing for Attendance\n\n\nCode\n# Calculate moving averages\nma_attendance_3 &lt;- ma(ts_attendance, order = 3)\nma_attendance_5 &lt;- ma(ts_attendance, order = 5)\n\n# Plot comparison\nautoplot(ts_attendance, series = \"Original\") +\n    autolayer(ma_attendance_3, series = \"MA(3)\") +\n    autolayer(ma_attendance_5, series = \"MA(5)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(3)\" = \"#006bb6\",\n            \"MA(5)\" = \"#f58426\"\n        ),\n        breaks = c(\"Original\", \"MA(3)\", \"MA(5)\")\n    ) +\n    labs(\n        title = \"Attendance: Moving Average Smoothing (COVID Shock Visible)\",\n        subtitle = \"Smoothing cannot remove the dramatic 2020-21 disruption\",\n        y = \"Total Attendance (millions)\",\n        x = \"Season\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\nAttendance provides the counterpoint: a stable pre-COVID plateau around ~21–22 million through 2019, a 2020–21 collapse during the bubble/limited-capacity seasons, and a partial recovery that remains below the pre-pandemic ceiling. The sharp, short-window discontinuity is a uncounted for shock rather than a new equilibrium. Even with 3–5 year moving averages, the COVID impact is too large to smooth away",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#data-preparation-and-time-series-creation",
    "href": "eda.html#data-preparation-and-time-series-creation",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.1 Data Preparation and Time Series Creation",
    "text": "5.1 Data Preparation and Time Series Creation\n\n\nCode\n# Load all sports betting stocks\ndkng &lt;- read_csv(\"data/financial/DKNG_daily.csv\", show_col_types = FALSE) %&gt;% mutate(Date = as.Date(Date))\npenn &lt;- read_csv(\"data/financial/PENN_daily.csv\", show_col_types = FALSE) %&gt;% mutate(Date = as.Date(Date))\nmgm &lt;- read_csv(\"data/financial/MGM_daily.csv\", show_col_types = FALSE) %&gt;% mutate(Date = as.Date(Date))\nczr &lt;- read_csv(\"data/financial/CZR_daily.csv\", show_col_types = FALSE) %&gt;% mutate(Date = as.Date(Date))\n\ncat(\"DKNG:\", nrow(dkng), \"days |\", min(dkng$Date), \"to\", max(dkng$Date), \"\\n\")\n\n\nDKNG: 1181 days | 18375 to 20088 \n\n\nCode\ncat(\"PENN:\", nrow(penn), \"days |\", min(penn$Date), \"to\", max(penn$Date), \"\\n\")\n\n\nPENN: 1258 days | 18263 to 20088 \n\n\nCode\ncat(\"MGM:\", nrow(mgm), \"days |\", min(mgm$Date), \"to\", max(mgm$Date), \"\\n\")\n\n\nMGM: 1258 days | 18263 to 20088 \n\n\nCode\ncat(\"CZR:\", nrow(czr), \"days |\", min(czr$Date), \"to\", max(czr$Date), \"\\n\")\n\n\nCZR: 1258 days | 18263 to 20088 \n\n\nCode\n# Create weekly time series for all stocks\ncreate_weekly_ts &lt;- function(df, ticker) {\n    weekly &lt;- df %&gt;%\n        mutate(Year = year(Date), Week = week(Date)) %&gt;%\n        group_by(Year, Week) %&gt;%\n        summarise(Avg_Close = mean(`Adj Close`, na.rm = TRUE), .groups = \"drop\") %&gt;%\n        arrange(Year, Week)\n\n    start_year &lt;- min(weekly$Year)\n    start_week &lt;- weekly %&gt;%\n        filter(Year == start_year) %&gt;%\n        pull(Week) %&gt;%\n        min()\n    ts(weekly$Avg_Close, start = c(start_year, start_week), frequency = 52)\n}\n\nts_dkng &lt;- create_weekly_ts(dkng, \"DKNG\")\nts_penn &lt;- create_weekly_ts(penn, \"PENN\")\nts_mgm &lt;- create_weekly_ts(mgm, \"MGM\")\nts_czr &lt;- create_weekly_ts(czr, \"CZR\")",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#comparative-visualization-all-four-betting-stocks",
    "href": "eda.html#comparative-visualization-all-four-betting-stocks",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.2 Comparative Visualization: All Four Betting Stocks",
    "text": "5.2 Comparative Visualization: All Four Betting Stocks\n\n\nCode\n# Combine all stocks for comparison (normalize to starting price = 100)\nautoplot(ts_dkng / as.numeric(ts_dkng)[1] * 100, series = \"DKNG\") +\n    autolayer(ts_penn / as.numeric(ts_penn)[1] * 100, series = \"PENN\") +\n    autolayer(ts_mgm / as.numeric(ts_mgm)[1] * 100, series = \"MGM\") +\n    autolayer(ts_czr / as.numeric(ts_czr)[1] * 100, series = \"CZR\") +\n    scale_color_manual(values = c(\"DKNG\" = \"#006bb6\", \"PENN\" = \"#f58426\", \"MGM\" = \"#00a94f\", \"CZR\" = \"#c8102e\")) +\n    labs(\n        title = \"Sports Betting Stocks: Normalized Performance (2020-2024)\",\n        subtitle = \"Indexed to 100 at each stock's start date | Boom-bust-stabilization pattern\",\n        y = \"Normalized Price (Start = 100)\", x = \"Year\", color = \"Stock\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\"), legend.position = \"bottom\")",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#dkng-detailed-analysis",
    "href": "eda.html#dkng-detailed-analysis",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.3 DKNG Detailed Analysis",
    "text": "5.3 DKNG Detailed Analysis\n\n\nCode\nautoplot(ts_dkng) +\n    annotate(\"rect\", xmin = 2021, xmax = 2021.5, ymin = 0, ymax = 70, alpha = 0.1, fill = \"orange\") +\n    annotate(\"text\", x = 2021.25, y = 65, label = \"Peak Boom\", color = \"orange\", fontface = \"bold\", size = 3) +\n    labs(\n        title = \"DraftKings (DKNG) Weekly Stock Price (2020-2024)\",\n        subtitle = \"IPO boom during COVID → correction → stabilization\",\n        x = \"Year\", y = \"Avg Weekly Adj Close ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\", size = 14))",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#seasonal-decomposition-dkng",
    "href": "eda.html#seasonal-decomposition-dkng",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.4 Seasonal Decomposition (DKNG)",
    "text": "5.4 Seasonal Decomposition (DKNG)\n\n\nCode\n# Multiplicative decomposition (appropriate for stock prices)\ndecomp_dkng &lt;- decompose(ts_dkng, type = \"multiplicative\")\n\n# Plot decomposition\nautoplot(decomp_dkng) +\n    labs(title = \"DKNG Stock: Seasonal Decomposition (Multiplicative Model)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(face = \"bold\", size = 14))",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#moving-average-smoothing-dkng",
    "href": "eda.html#moving-average-smoothing-dkng",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.5 Moving Average Smoothing (DKNG)",
    "text": "5.5 Moving Average Smoothing (DKNG)\n\n\nCode\n# Calculate moving averages (using weeks)\nma_dkng_4 &lt;- ma(ts_dkng, order = 4) # Monthly smoothing (~4 weeks)\nma_dkng_13 &lt;- ma(ts_dkng, order = 13) # Quarterly smoothing (~13 weeks)\nma_dkng_52 &lt;- ma(ts_dkng, order = 52) # Annual smoothing (52 weeks)\n\nautoplot(ts_dkng, series = \"Original\") +\n    autolayer(ma_dkng_4, series = \"MA(4 weeks)\") +\n    autolayer(ma_dkng_13, series = \"MA(13 weeks)\") +\n    autolayer(ma_dkng_52, series = \"MA(52 weeks)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(4 weeks)\" = \"#006bb6\",\n            \"MA(13 weeks)\" = \"#f58426\",\n            \"MA(52 weeks)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(4 weeks)\", \"MA(13 weeks)\", \"MA(52 weeks)\")\n    ) +\n    labs(\n        title = \"DKNG Stock: Moving Average Smoothing Comparison\",\n        subtitle = \"Different windows reveal trading cycles vs long-term trends\",\n        y = \"Stock Price ($)\",\n        x = \"Year\",\n        color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#acf-and-lag-plots-dkng",
    "href": "eda.html#acf-and-lag-plots-dkng",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.6 ACF and Lag Plots (DKNG)",
    "text": "5.6 ACF and Lag Plots (DKNG)\n\n\nCode\nacf_dkng &lt;- ggAcf(ts_dkng, lag.max = 52) +\n    labs(title = \"ACF of DKNG Weekly Stock Price\") +\n    theme_minimal()\n\npacf_dkng &lt;- ggPacf(ts_dkng, lag.max = 52) +\n    labs(title = \"PACF of DKNG Weekly Stock Price\") +\n    theme_minimal()\n\nacf_dkng / pacf_dkng",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#penn-analysis",
    "href": "eda.html#penn-analysis",
    "title": "Exploratory Data Analysis: Offensive Efficiency in the Modern NBA",
    "section": "5.7 PENN Analysis",
    "text": "5.7 PENN Analysis\n\n5.7.1 PENN Time Series Visualization\n\n\nCode\nautoplot(ts_penn) +\n    annotate(\"rect\", xmin = 2020, xmax = 2021.5, ymin = 0, ymax = 140, alpha = 0.1, fill = \"red\") +\n    annotate(\"text\", x = 2020, y = 130, label = \"Covid-19\", color = \"red\", fontface = \"bold\", size = 3) +\n    labs(\n        title = \"Penn Entertainment (PENN) Weekly Stock Price (2020-2024)\",\n        x = \"Year\", y = \"Avg Weekly Adj Close ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n5.7.2 Seasonal Decomposition (PENN)\n\n\nCode\n# Multiplicative decomposition\ndecomp_penn &lt;- decompose(ts_penn, type = \"multiplicative\")\n\nautoplot(decomp_penn) +\n    labs(title = \"PENN Stock: Seasonal Decomposition (Multiplicative Model)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\n\n\n5.7.3 Moving Average Smoothing (PENN)\n\n\nCode\nma_penn_4 &lt;- ma(ts_penn, order = 4)\nma_penn_13 &lt;- ma(ts_penn, order = 13)\nma_penn_52 &lt;- ma(ts_penn, order = 52)\n\nautoplot(ts_penn, series = \"Original\") +\n    autolayer(ma_penn_4, series = \"MA(4 weeks)\") +\n    autolayer(ma_penn_13, series = \"MA(13 weeks)\") +\n    autolayer(ma_penn_52, series = \"MA(52 weeks)\") +\n    scale_color_manual(\n        values = c(\n            \"Original\" = \"gray60\",\n            \"MA(4 weeks)\" = \"#006bb6\",\n            \"MA(13 weeks)\" = \"#f58426\",\n            \"MA(52 weeks)\" = \"#000000\"\n        ),\n        breaks = c(\"Original\", \"MA(4 weeks)\", \"MA(13 weeks)\", \"MA(52 weeks)\")\n    ) +\n    labs(\n        title = \"PENN Stock: Moving Average Smoothing Comparison\",\n        subtitle = \"Even annual smoothing cannot hide the structural collapse\",\n        y = \"Stock Price ($)\", x = \"Year\", color = \"Series\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n5.7.4 ACF and Lag Plots (PENN)\n\n\nCode\nacf_penn &lt;- ggAcf(ts_penn, lag.max = 52) +\n    labs(title = \"ACF of PENN Weekly Stock Price\") +\n    theme_minimal()\n\npacf_penn &lt;- ggPacf(ts_penn, lag.max = 52) +\n    labs(title = \"PACF of PENN Weekly Stock Price\") +\n    theme_minimal()\n\nacf_penn / pacf_penn\n\n\n\n\n\n\n\n\n\nBecause annual NBA series are effectively non-seasonal, I include weekly sports-betting equities to demonstrate seasonality and multiplicative decomposition. DraftKings (DKNG), Penn (PENN), MGM, and Caesars (CZR) all show pandemic-era boom-bust dynamics on weekly data. Prices are non-stationary in levels, stationary in differences, and has volatility that scales with price; implying a multiplicative model is necessary for decomposition. DKNG exhibits a large run-up, correction, and stabilization while PENN shows a sharper hype-driven spike and deeper collapse.\nPulling the findings together: ORtg, Pace, 3PAr, and Attendance are all non-stationary in levels and become stationary after first differences (d = 1). Therefore, additive decomposition is appropriate for the NBA metrics , while multiplicative decomposition fits the weekly equities. Short and medium moving-average windows clarify regime shifts: the 2012 analytics inflection in ORtg/3PAr, the mid-2000s trough and rebound in Pace, and the COVID intervention in Attendance.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "The Era of Efficiency: Analytics, COVID, and the Modern NBA",
    "section": "",
    "text": "Over the last two decades, the National Basketball Association (NBA) has undergone a historic transformation in how the game is played and measured. The rise of analytics has redefined decision-making, from shot selection to player valuation, creating a league increasingly optimized for pace, spacing, and efficiency. Traditional mid-range play has given way to data-driven offenses that favor the three-point shot and fast-break opportunities1. Yet this evolution has not been linear: external shocks such as the COVID-19 pandemic and rule changes have periodically disrupted the sport’s equilibrium.\nThis project seeks to quantify and contextualize the NBA’s evolution toward efficiency; particularly how the league’s statistical DNA has shifted under the influence of analytics, and how sudden disruptions like the 2020 “bubble” season temporarily rewired its dynamics. Using time-series analysis, the study traces the interplay between pace, three-point attempt rate, and offensive efficiency to reveal both long-term structural change and short-term volatility.\n\n\nBasketball today is not merely an athletic competition but a living laboratory of applied data science. The league’s embrace of analytics mirrors a broader transformation in modern industries: decisions increasingly rest on quantitative evidence rather than intuition. The shift from mid-range isolation to high efficiency, high variance offense encapsulates how optimization, technology, and information access shape human behavior, specifically in sports.\n\n\n\nThe Big Picture\n\n\nWith the framework above, I aim to tell the story of how basketball’s offensive efficiency evolved under the dual pressures of analytics and disruption. The analysis begins with a long-run view of the game’s transformation: tracing how three-point attempts, shot selection, and pace redefined offensive output from 1980 through 2025. By examining league wide efficiency, field-goal percentages, and possessions per 48 minutes, I hope to capture how the modern NBA diverged from its slower, mid-range past into a data-optimized era of high variance offense.\nFrom there, the focus shifts to the Analytics Revolution, a period beginning around 2012 when front offices embraced quantitative models and player-tracking data. Here, patterns in shot distance, efficiency metrics, and team-level statistics reveal how strategy and roster construction began to align with mathematical optimization. These structural changes not only reshaped offensive philosophy but also institutionalized analytics as a core component of competitive advantage.\nThe next chapter explores the COVID-19 shock, a natural experiment that disrupted decades of rhythm. Empty arenas, travel restrictions, and condensed schedules created conditions to test how context, apart from talent or tactics, influences performance. Comparing attendance patterns, scoring volatility, and pace before, during, and after the pandemic provides a rare window into basketball’s psychological and environmental dimensions.\nFinally, the analysis turns toward the future. By modeling post-2023 trends using both traditional and modern time-series methods, the goal is to forecast whether the league has stabilized in a new equilibrium or continues to evolve. From evolution and optimization to disruption and re-emergence the project aims to capture how data, environment, and adaptation intertwine to define the modern game.\n\n\n\nEarly quantitative basketball research established the foundation for efficiency metrics such as effective field goal percentage (eFG%) and true shooting percentage (TS%)2. Subsequent work formalized offensive rating and pace-adjusted measures that underpin today’s analytics frameworks3. In this study, those established metrics anchor a longitudinal view of the NBA from 1980–2025, allowing us to trace how efficiency evolved and whether recent shifts reflect gradual adaptation or discrete regime changes.\nA central thread is shot-selection optimization. Spatial analyses show that perimeter-oriented offenses and rim attempts yield higher expected value than mid-range play4. Building on that evidence, we follow the league’s shot mix over time and date its key inflection points using structural-break diagnostics. Forecast comparisons around those dated transitions then indicate whether the modern shot profile has stabilized or is still moving toward further concentration in high-value zones.\nTempo and spacing economics form the second pillar. Prior work links ball movement, spacing, and faster pace to scoring efficiency in the modern game1. Rather than treating pace and efficiency as static correlates, we examine their dynamic ordering: do changes in tempo precede shifts in efficiency, or the reverse? A multivariate framework (including Granger-style causality tests and rolling correlations) lets us see how this relationship strengthened from the pre-analytics period into the analytics era, clarifying whether “playing faster” is a driver, a consequence, or part of a feedback loop with shot quality.\nThe third pillar addresses COVID-era disruptions. Empty-arena conditions in 2020 weakened traditional home-court effects and scrambled normal rhythms. We treat the bubble and capacity-limited seasons as an exogenous intervention, quantify the immediate impact on pace, efficiency, and scoring variance, and then measure persistence and recovery. By juxtaposing pre-2020 fitted behavior with realized outcomes through 2022, we can distinguish a temporary shock from a lasting structural shift.\nTogether, these components extend prior literature that established what changed (shot mix, tempo strategies, COVID effects) but seldom mapped how and when the transitions unfolded or whether they endured. Concretely, we (i) date the analytics inflection with objective break tests; (ii) test whether rising three-point volume leads or follows efficiency gains; (iii) model low-order serial dependence and feedback that cross-sectional designs omit; and (iv) isolate the pandemic’s impulse and its decay. Treating efficiency, pace, and shot selection as interconnected trajectories reveals the timing, coupling, and durability of the forces reshaping the modern NBA.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#the-big-picture",
    "href": "intro.html#the-big-picture",
    "title": "The Era of Efficiency: Analytics, COVID, and the Modern NBA",
    "section": "",
    "text": "Basketball today is not merely an athletic competition but a living laboratory of applied data science. The league’s embrace of analytics mirrors a broader transformation in modern industries: decisions increasingly rest on quantitative evidence rather than intuition. The shift from mid-range isolation to high efficiency, high variance offense encapsulates how optimization, technology, and information access shape human behavior, specifically in sports.\n\n\n\nThe Big Picture\n\n\nWith the framework above, I aim to tell the story of how basketball’s offensive efficiency evolved under the dual pressures of analytics and disruption. The analysis begins with a long-run view of the game’s transformation: tracing how three-point attempts, shot selection, and pace redefined offensive output from 1980 through 2025. By examining league wide efficiency, field-goal percentages, and possessions per 48 minutes, I hope to capture how the modern NBA diverged from its slower, mid-range past into a data-optimized era of high variance offense.\nFrom there, the focus shifts to the Analytics Revolution, a period beginning around 2012 when front offices embraced quantitative models and player-tracking data. Here, patterns in shot distance, efficiency metrics, and team-level statistics reveal how strategy and roster construction began to align with mathematical optimization. These structural changes not only reshaped offensive philosophy but also institutionalized analytics as a core component of competitive advantage.\nThe next chapter explores the COVID-19 shock, a natural experiment that disrupted decades of rhythm. Empty arenas, travel restrictions, and condensed schedules created conditions to test how context, apart from talent or tactics, influences performance. Comparing attendance patterns, scoring volatility, and pace before, during, and after the pandemic provides a rare window into basketball’s psychological and environmental dimensions.\nFinally, the analysis turns toward the future. By modeling post-2023 trends using both traditional and modern time-series methods, the goal is to forecast whether the league has stabilized in a new equilibrium or continues to evolve. From evolution and optimization to disruption and re-emergence the project aims to capture how data, environment, and adaptation intertwine to define the modern game.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#literature-review",
    "href": "intro.html#literature-review",
    "title": "The Era of Efficiency: Analytics, COVID, and the Modern NBA",
    "section": "",
    "text": "Early quantitative basketball research established the foundation for efficiency metrics such as effective field goal percentage (eFG%) and true shooting percentage (TS%)2. Subsequent work formalized offensive rating and pace-adjusted measures that underpin today’s analytics frameworks3. In this study, those established metrics anchor a longitudinal view of the NBA from 1980–2025, allowing us to trace how efficiency evolved and whether recent shifts reflect gradual adaptation or discrete regime changes.\nA central thread is shot-selection optimization. Spatial analyses show that perimeter-oriented offenses and rim attempts yield higher expected value than mid-range play4. Building on that evidence, we follow the league’s shot mix over time and date its key inflection points using structural-break diagnostics. Forecast comparisons around those dated transitions then indicate whether the modern shot profile has stabilized or is still moving toward further concentration in high-value zones.\nTempo and spacing economics form the second pillar. Prior work links ball movement, spacing, and faster pace to scoring efficiency in the modern game1. Rather than treating pace and efficiency as static correlates, we examine their dynamic ordering: do changes in tempo precede shifts in efficiency, or the reverse? A multivariate framework (including Granger-style causality tests and rolling correlations) lets us see how this relationship strengthened from the pre-analytics period into the analytics era, clarifying whether “playing faster” is a driver, a consequence, or part of a feedback loop with shot quality.\nThe third pillar addresses COVID-era disruptions. Empty-arena conditions in 2020 weakened traditional home-court effects and scrambled normal rhythms. We treat the bubble and capacity-limited seasons as an exogenous intervention, quantify the immediate impact on pace, efficiency, and scoring variance, and then measure persistence and recovery. By juxtaposing pre-2020 fitted behavior with realized outcomes through 2022, we can distinguish a temporary shock from a lasting structural shift.\nTogether, these components extend prior literature that established what changed (shot mix, tempo strategies, COVID effects) but seldom mapped how and when the transitions unfolded or whether they endured. Concretely, we (i) date the analytics inflection with objective break tests; (ii) test whether rising three-point volume leads or follows efficiency gains; (iii) model low-order serial dependence and feedback that cross-sectional designs omit; and (iv) isolate the pandemic’s impulse and its decay. Treating efficiency, pace, and shot selection as interconnected trajectories reveals the timing, coupling, and durability of the forces reshaping the modern NBA.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "data_viz.html",
    "href": "data_viz.html",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "",
    "text": "This analysis tells the story of the most dramatic strategic transformation in NBA history, particularly the shift from mid-range-heavy “iso-ball” to the analytics-optimized “Moreyball” offense that dominates today. Through the visualizations below, we can see how data-driven decision making fundamentally reshaped basketball strategy over 45 years.\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(magick)\nlibrary(viridis)\nlibrary(patchwork)\nlibrary(cowplot)\n\ntheme_set(theme_minimal(base_size = 12))",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#the-analytics-revolution-rise-of-the-three-pointer-1980-2025",
    "href": "data_viz.html#the-analytics-revolution-rise-of-the-three-pointer-1980-2025",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "1. The Analytics Revolution: Rise of the Three-Pointer (1980-2025)",
    "text": "1. The Analytics Revolution: Rise of the Three-Pointer (1980-2025)\n\n\nCode\nlibrary(stringr)\n\nall_adv_files &lt;- list.files(\"data/adv_stats\", pattern = \"*.csv\", full.names = TRUE)\n\nall_adv_data &lt;- map_df(all_adv_files, function(file) {\n    season_str &lt;- str_extract(basename(file), \"\\\\d{4}-\\\\d{2}\")\n    season_year &lt;- as.numeric(str_sub(season_str, 1, 4)) + 1\n\n    df &lt;- read_csv(file, show_col_types = FALSE)\n\n    df$Season &lt;- season_year\n    return(df)\n})\n\nleague_avg &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        ORtg = mean(`Unnamed: 10_level_0_ORtg`, na.rm = TRUE),\n        DRtg = mean(`Unnamed: 11_level_0_DRtg`, na.rm = TRUE),\n        Pace = mean(`Unnamed: 13_level_0_Pace`, na.rm = TRUE),\n        `3PAr` = mean(`Unnamed: 15_level_0_3PAr`, na.rm = TRUE),\n        `TS%` = mean(`Unnamed: 16_level_0_TS%`, na.rm = TRUE),\n        `eFG%` = mean(`Offense Four Factors_eFG%`, na.rm = TRUE),\n        .groups = \"drop\"\n    )\n\nleague_avg &lt;- league_avg %&gt;%\n    mutate(\n        Era = case_when(\n            Season &lt; 2012 ~ \"Pre-Analytics Era\",\n            Season &gt;= 2012 & Season &lt; 2020 ~ \"Analytics Era\",\n            Season &gt;= 2020 ~ \"Post-COVID Era\"\n        )\n    )\n\nfig_3par &lt;- plot_ly(league_avg,\n    x = ~Season, y = ~`3PAr`,\n    color = ~Era,\n    colors = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    ),\n    type = \"scatter\", mode = \"lines+markers\",\n    marker = list(size = 6),\n    line = list(width = 3),\n    hovertemplate = paste(\n        \"&lt;b&gt;Season:&lt;/b&gt; %{x}&lt;br&gt;\",\n        \"&lt;b&gt;3PAr:&lt;/b&gt; %{y:.0%}&lt;br&gt;\",\n        \"&lt;extra&gt;&lt;/extra&gt;\"\n    )\n) %&gt;%\n    layout(\n        title = list(\n            text = \"The Analytics Revolution: 3-Point Attempt Rate (1980-2025)\",\n            font = list(size = 15, weight = \"bold\")\n        ),\n        xaxis = list(title = \"Season\"),\n        yaxis = list(title = \"3-Point Attempt Rate (3PA / FGA)\", tickformat = \".0%\"),\n        hovermode = \"closest\",\n        template = \"plotly_white\",\n        annotations = list(\n            list(\n                x = 2012, y = 0.44, text = \"Analytics Era Begins\",\n                showarrow = FALSE,\n                font = list(size = 8, color = \"#f58426\", weight = \"bold\"),\n                xanchor = \"left\", xshift = 5\n            )\n        ),\n        shapes = list(\n            list(\n                type = \"line\", x0 = 2012, x1 = 2012, y0 = 0, y1 = 1,\n                line = list(color = \"#f58426\", width = 2, dash = \"dash\"),\n                yref = \"paper\"\n            )\n        )\n    )\n\nfig_3par\n\n\n\n\n\n\nFor three decades, from 1980 to 2011, NBA teams treated the three-pointer as a supplementary weapon rather than a foundational strategy, with attempt rates hovering consistently between 20% and 28%. Between 1995 and 1997, the rate peaked at 21% due to the league temporarily shortening the three-point line. However, from 1997 to 1998, we see a clear decline in attempts as the league reverted to the original distance. During this era, the mid-range jumper, the signature shot of basketball mastery taught in gyms from youth leagues to the professional ranks, remained dominant. But in 2012, Houston Rockets GM Daryl Morey’s analytics department did the math and exposed a harsh truth: mid-range shots, averaging roughly 0.8 points per attempt, were the least efficient in basketball, while three-pointers yielded significantly higher returns. Observing the visualization, we can clearly identify a structural break around 2012, as three-point attempt rates surge from roughly 28% to over 42% by 2025. Yet this shift raises a crucial question: If teams started shooing more threes, did it actually make them better or just different?",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#efficiency-evolution-four-decades-of-offensive-rating-pace-and-shooting",
    "href": "data_viz.html#efficiency-evolution-four-decades-of-offensive-rating-pace-and-shooting",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "2. Efficiency Evolution: Four Decades of Offensive Rating, Pace, and Shooting",
    "text": "2. Efficiency Evolution: Four Decades of Offensive Rating, Pace, and Shooting\n\n\nCode\nefficiency_long &lt;- league_avg %&gt;%\n    select(Season, ORtg, Pace, `TS%`, `eFG%`, Era) %&gt;%\n    pivot_longer(\n        cols = c(ORtg, Pace, `TS%`, `eFG%`),\n        names_to = \"Metric\",\n        values_to = \"Value\"\n    )\n\nefficiency_facet &lt;- ggplot(efficiency_long, aes(x = Season, y = Value, color = Era)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"loess\", se = TRUE, alpha = 0.2, color = \"black\", linetype = \"dashed\") +\n    facet_wrap(~Metric,\n        scales = \"free_y\", ncol = 2,\n        labeller = labeller(Metric = c(\n            \"ORtg\" = \"Offensive Rating (pts per 100 poss)\",\n            \"Pace\" = \"Pace (possessions per 48 min)\",\n            \"TS%\" = \"True Shooting %\",\n            \"eFG%\" = \"Effective FG%\"\n        ))\n    ) +\n    scale_color_manual(values = c(\n        \"Pre-Analytics Era\" = \"#006bb6\",\n        \"Analytics Era\" = \"#f58426\",\n        \"Post-COVID Era\" = \"#bec0c2\"\n    )) +\n    labs(\n        title = \"NBA Efficiency Metrics: 45-Year Evolution (1980-2025)\",\n        subtitle = \"Offensive rating climbed steadily; pace declined then rebounded; shooting efficiency surged post-2012\",\n        x = \"Season\",\n        y = \"Metric Value\",\n        color = \"Era\",\n        caption = \"Data: Basketball Reference | Black dashed line: LOESS smoothing\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 16),\n        plot.subtitle = element_text(size = 11, color = \"gray40\"),\n        strip.text = element_text(face = \"bold\", size = 11),\n        legend.position = \"bottom\",\n        panel.grid.minor = element_blank()\n    )\n\nefficiency_facet\n\n\n\n\n\n\n\n\n\nObserving the visualization, we can see that attempting more three pointers did, in fact, make teams measurably better. The chart tells a story spanning 45 years of performance gains driven by strategic optimization. Offensive Rating rose by roughly 11%, from 104 in 1980 to 115 in 2025, with the sharpest improvements occurring after 2012, precisely when three point attempt rates began to surge. We also see True Shooting Percentage climb from 53% to 58%, reinforcing the conclusion that teams became more efficient scorers by optimizing the quality of their shots. The Pace metric follows a U-shaped trajectory, reflecting the evolution of play styles over time. It went from the fast, run and gun tempo of the 1980s, to the slowed isolation heavy 2000s, and finally rebounding post 2012 as teams embraced a more controlled yet efficient rhythm. Lastly, the rise in Effective Field Goal Percentage suggests that these improvements weren’t merely the result of drawing more fouls. Teams didn’t shoot more threes by accident; they made a deliberate, data driven decision to sacrifice mid range shots in exchange for more threes and attempts at the rim.",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#death-of-the-midrange-shot-zone-trends-2004-2025",
    "href": "data_viz.html#death-of-the-midrange-shot-zone-trends-2004-2025",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "3. Death of the Midrange: Shot Zone Trends (2004-2025)",
    "text": "3. Death of the Midrange: Shot Zone Trends (2004-2025)\n\n\nCode\nshot_files &lt;- list.files(\"data/shot_location\", pattern = \"NBA_.*_Shots.csv\", full.names = TRUE)\n\nextract_season &lt;- function(filename) {\n    year_str &lt;- str_extract(basename(filename), \"\\\\d{4}\")\n    return(as.numeric(year_str))\n}\n\nshot_data_sample &lt;- map_df(shot_files, function(file) {\n    season_year &lt;- extract_season(file)\n\n    df &lt;- read_csv(file, show_col_types = FALSE, n_max = 50000)\n    df$Season &lt;- season_year\n    return(df)\n})\n\nzone_distribution &lt;- shot_data_sample %&gt;%\n    filter(!is.na(BASIC_ZONE)) %&gt;%\n    group_by(Season, BASIC_ZONE) %&gt;%\n    summarise(\n        Shot_Count = n(),\n        .groups = \"drop\"\n    ) %&gt;%\n    group_by(Season) %&gt;%\n    mutate(\n        Shot_Percentage = Shot_Count / sum(Shot_Count) * 100\n    ) %&gt;%\n    ungroup()\n\nkey_zones &lt;- c(\n    \"Mid-Range\", \"Restricted Area\", \"Above the Break 3\",\n    \"Left Corner 3\", \"Right Corner 3\"\n)\n\nzone_trends &lt;- zone_distribution %&gt;%\n    filter(BASIC_ZONE %in% key_zones) %&gt;%\n    mutate(\n        Zone_Category = case_when(\n            BASIC_ZONE == \"Mid-Range\" ~ \"Mid-Range (≈8–22 ft)\",\n            BASIC_ZONE == \"Restricted Area\" ~ \"At Rim\",\n            BASIC_ZONE %in% c(\"Left Corner 3\", \"Right Corner 3\") ~ \"Corner 3s\",\n            BASIC_ZONE == \"Above the Break 3\" ~ \"Non-Corner 3s (Arc)\",\n            TRUE ~ BASIC_ZONE\n        )\n    ) %&gt;%\n    group_by(Season, Zone_Category) %&gt;%\n    summarise(Shot_Percentage = sum(Shot_Percentage), .groups = \"drop\")\n\nzone_trends &lt;- zone_trends %&gt;%\n    mutate(\n        tooltip_text = paste0(\n            \"Season: \", Season, \"\\n\",\n            \"Zone: \", Zone_Category, \"\\n\",\n            \"Percentage: \", round(Shot_Percentage, 0), \"%\"\n        )\n    )\n\nrect_data &lt;- data.frame(\n    xmin = 2012, xmax = 2015,\n    ymin = 0, ymax = 45\n)\n\nmidrange_line_plot &lt;- ggplot(zone_trends, aes(\n    x = Season, y = Shot_Percentage,\n    color = Zone_Category,\n    linetype = Zone_Category,\n    text = tooltip_text\n)) +\n    geom_rect(\n        data = rect_data, inherit.aes = FALSE,\n        aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),\n        fill = \"#f58426\", alpha = 0.1\n    ) +\n    geom_line(size = 1.5) +\n    geom_point(size = 2.5) +\n    annotate(\"text\",\n        x = 2013.5, y = 42, label = \"Analytics\\nRevolution\", fontface = \"bold\", color = \"#f58426\"\n    ) +\n    scale_color_manual(values = c(\n        \"Mid-Range (≈8–22 ft)\" = \"#bec0c2\",\n        \"At Rim\" = \"#006bb6\",\n        \"Corner 3s\" = \"#f58426\",\n        \"Non-Corner 3s (Arc)\" = \"#000000\"\n    )) +\n    scale_linetype_manual(values = c(\n        \"Mid-Range (≈8–22 ft)\" = \"solid\",\n        \"At Rim\"               = \"solid\",\n        \"Corner 3s\"            = \"dashed\",\n        \"Non-Corner 3s (Arc)\"  = \"dotted\"\n    )) +\n    labs(\n        title = \"The Death of the Midrange: Shot Zone Trends (2004–2025)\",\n        subtitle = \"Mid-range declined while arc and corner 3s surged; at-rim remained relatively stable\",\n        x = \"Season\", y = \"Percentage of Total Shots (%)\",\n        color = \"Shot Zone\", linetype = \"Shot Zone\"\n    ) +\n    theme_minimal(base_size = 10) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 13),\n        plot.subtitle = element_text(size = 9, color = \"gray40\"),\n        axis.title = element_text(size = 9),\n        axis.text = element_text(size = 8),\n        legend.position = \"bottom\",\n        legend.title = element_text(face = \"bold\", size = 9),\n        legend.text = element_text(size = 8),\n        panel.grid.minor = element_blank()\n    ) +\n    scale_y_continuous(limits = c(0, 45), labels = function(x) paste0(x, \"%\"))\n\np &lt;- ggplotly(midrange_line_plot, tooltip = \"text\")\n\np %&gt;% style(mode = \"lines+markers\")\n\n\n\n\n\n\nWe can clearly see the trade-off in the visualization above. Mid-range shots, which accounted for about 35% of all attempts in 2004, collapsed to just 13% by 2025. Meanwhile, corner threes doubled, and above-the-arc threes surged from 13% to 34%. Shots at the rim remained relatively stable throughout this period. Around the 2015–2016 season, three-point attempts beyond the arc surpassed mid-range shots for the first time in NBA history. This shift reflects teams’ evolving approach: attack the rim for high-percentage looks, draw fouls or create putback opportunities, or shoot threes for higher expected value",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#court-shot-charts-evolution-of-shot-selection-2004-2024-w.-focus-on-boston-celtics",
    "href": "data_viz.html#court-shot-charts-evolution-of-shot-selection-2004-2024-w.-focus-on-boston-celtics",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "4. Court Shot Charts: Evolution of Shot Selection (2004-2024) w. Focus on Boston Celtics",
    "text": "4. Court Shot Charts: Evolution of Shot Selection (2004-2024) w. Focus on Boston Celtics\n\n\nCode\nsource(\"NBA_shots_tutorial.R\")\n\nshots_2004 &lt;- shot_data_sample %&gt;%\n    filter(Season == 2004, !is.na(LOC_X), !is.na(LOC_Y), LOC_Y &lt;= 47)\n\nzone_check_2004 &lt;- shots_2004 %&gt;%\n    filter(!is.na(BASIC_ZONE)) %&gt;%\n    group_by(BASIC_ZONE) %&gt;%\n    summarise(Count = n(), .groups = \"drop\") %&gt;%\n    mutate(Percentage = Count / sum(Count) * 100) %&gt;%\n    arrange(desc(Count))\n\nyears_to_plot &lt;- c(2004, 2008, 2012, 2016, 2019, 2024)\n\ncreate_court_for_year &lt;- function(year) {\n    shots_year &lt;- shot_data_sample %&gt;%\n        filter(Season == year, !is.na(LOC_X), !is.na(LOC_Y), LOC_Y &lt;= 47, !is.na(BASIC_ZONE))\n\n    shots_year &lt;- shots_year %&gt;%\n        mutate(\n            Zone_Category = case_when(\n                BASIC_ZONE == \"Mid-Range\" ~ \"Mid-Range\",\n                BASIC_ZONE == \"Restricted Area\" ~ \"At Rim\",\n                BASIC_ZONE %in% c(\"Left Corner 3\", \"Right Corner 3\") ~ \"Corner 3s\",\n                BASIC_ZONE == \"Above the Break 3\" ~ \"Above Arc 3s\",\n                TRUE ~ \"Other\"\n            )\n        )\n\n    if (nrow(shots_year) &gt; 3000) {\n        set.seed(42) # For reproducibility\n        shots_year &lt;- shots_year %&gt;% sample_n(3000)\n    }\n\n    subtitle &lt;- case_when(\n        year == 2004 ~ \"Pre-Analytics\",\n        year == 2008 ~ \"Early Transition\",\n        year == 2012 ~ \"Moreyball Begins\",\n        year == 2016 ~ \"Warriors Dynasty\",\n        year == 2019 ~ \"Pre-COVID\",\n        year == 2024 ~ \"Modern Era\",\n        TRUE ~ \"\"\n    )\n\n    plot_court(court_themes$dark, use_short_three = FALSE) +\n        geom_point(\n            data = shots_year,\n            aes(x = LOC_X, y = LOC_Y, color = Zone_Category),\n            size = 1.2,\n            alpha = 0.6\n        ) +\n        scale_color_manual(\n            values = c(\n                \"Mid-Range\" = \"#d62728\",\n                \"At Rim\" = \"#1f77b4\",\n                \"Corner 3s\" = \"#ff7f0e\",\n                \"Above Arc 3s\" = \"#2ca02c\",\n                \"Other\" = \"gray70\"\n            ),\n            name = \"Zone\"\n        ) +\n        labs(\n            title = as.character(year),\n            subtitle = subtitle\n        ) +\n        theme(\n            plot.title = element_text(hjust = 0.5, size = 18, face = \"bold\", color = \"white\"),\n            plot.subtitle = element_text(hjust = 0.5, size = 13, color = \"gray80\"),\n            plot.background = element_rect(fill = \"#000004\", color = \"gray30\", size = 1),\n            panel.background = element_rect(fill = \"#000004\"),\n            legend.position = \"right\",\n            legend.background = element_rect(fill = \"#000004\"),\n            legend.title = element_text(size = 11, face = \"bold\", color = \"white\"),\n            legend.text = element_text(size = 10, color = \"white\"),\n            legend.key = element_rect(fill = \"#000004\"),\n            legend.key.size = unit(0.6, \"cm\")\n        )\n}\n\ncourt_plots &lt;- lapply(years_to_plot, create_court_for_year)\n\n\n\n2004 - Pre-Analytics2008 - Early Transition2012 - Moreyball Begins2016 - Warriors Dynasty2019 - Pre-COVID2024 - Modern Era\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nteams_2024 &lt;- shot_data_sample %&gt;%\n    filter(Season == 2024, !is.na(TEAM_NAME)) %&gt;%\n    group_by(TEAM_NAME) %&gt;%\n    summarise(Count = n(), .groups = \"drop\") %&gt;%\n    arrange(desc(Count))\n\ncreate_celtics_court &lt;- function(year) {\n    celtics_shots &lt;- shot_data_sample %&gt;%\n        filter(\n            Season == year,\n            TEAM_NAME == \"Boston Celtics\",\n            !is.na(LOC_X), !is.na(LOC_Y),\n            LOC_Y &lt;= 47\n        )\n\n    subtitle &lt;- case_when(\n        year == 2004 ~ \"Big 3 Era Begins\",\n        year == 2008 ~ \"Championship Season\",\n        year == 2012 ~ \"Late Big 3 Era\",\n        year == 2016 ~ \"Rebuilding Year\",\n        year == 2019 ~ \"Tatum/Brown Era\",\n        year == 2025 ~ \"Modern Celtics (2024)\",\n        TRUE ~ \"\"\n    )\n\n    plot_court(court_themes$dark, use_short_three = FALSE) +\n        geom_point(\n            data = celtics_shots,\n            aes(x = LOC_X, y = LOC_Y, color = SHOT_MADE, fill = SHOT_MADE),\n            size = 1.5,\n            shape = 21,\n            stroke = 0.5,\n            alpha = 0.6\n        ) +\n        scale_color_manual(\n            values = c(\"TRUE\" = \"green4\", \"FALSE\" = \"red3\"),\n            labels = c(\"TRUE\" = \"Made\", \"FALSE\" = \"Missed\")\n        ) +\n        scale_fill_manual(\n            values = c(\"TRUE\" = \"green2\", \"FALSE\" = \"red2\"),\n            labels = c(\"TRUE\" = \"Made\", \"FALSE\" = \"Missed\")\n        ) +\n        labs(\n            title = paste(\"Boston Celtics\"),\n            subtitle = subtitle\n        ) +\n        theme(\n            plot.title = element_text(hjust = 0.5, size = 18, face = \"bold\", color = \"white\"),\n            plot.subtitle = element_text(hjust = 0.5, size = 13, color = \"gray80\"),\n            plot.background = element_rect(fill = \"#000004\", color = \"gray30\", size = 1),\n            panel.background = element_rect(fill = \"#000004\"),\n            legend.position = \"right\",\n            legend.background = element_rect(fill = \"#000004\"),\n            legend.title = element_blank(),\n            legend.text = element_text(size = 10, color = \"white\", face = \"bold\"),\n            legend.key = element_rect(fill = \"#000004\"),\n            legend.key.size = unit(0.6, \"cm\")\n        )\n}\n\nceltics_years &lt;- c(2004, 2008, 2012, 2016, 2019, 2025)\nceltics_plots &lt;- lapply(celtics_years, create_celtics_court)\n\n\n\n200420082012201620192024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the first visualization, we can see that in the pre-analytics era there was a heavy concentration of mid-range shots. By 2012, however, we begin to see the influence of “Moreyball” reshaping the league, as shot distributions tighten around above-the-arc three-pointers. As we move into the modern era, mid-range attempts become increasingly sparse, while above-the-arc threes grow more frequent and form even tighter clusters. The outliers in the visualization represent the greatest shooter of all time, Stephen Curry, whose style helped redefine offensive strategy. This transformation is further illustrated by the Boston Celtics: in 2024, they recorded the highest volume of above-the-arc three-point attempts in team history. Their strategic embrace of analytics and shot optimization directly contributed to their success; culminating in their NBA Finals victory.",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "data_viz.html#the-covid-19-disruption-attendance-collapse-and-sports-betting-volatility",
    "href": "data_viz.html#the-covid-19-disruption-attendance-collapse-and-sports-betting-volatility",
    "title": "NBA’s Analytics Revolution - From Traditional to Modern Basketball (1980-2025)",
    "section": "5. The COVID-19 Disruption: Attendance Collapse and Sports Betting Volatility",
    "text": "5. The COVID-19 Disruption: Attendance Collapse and Sports Betting Volatility\n\n\nCode\nattendance_data &lt;- all_adv_data %&gt;%\n    group_by(Season) %&gt;%\n    summarise(\n        Total_Attendance = sum(`Unnamed: 29_level_0_Attend.`, na.rm = TRUE),\n        Avg_Attendance = mean(`Unnamed: 30_level_0_Attend./G`, na.rm = TRUE),\n        .groups = \"drop\"\n    ) %&gt;%\n    filter(Season &gt;= 2000)\n\ndkng &lt;- read_csv(\"data/financial/DKNG_daily.csv\", show_col_types = FALSE)\n\ndkng &lt;- dkng %&gt;%\n    mutate(\n        Date = as.Date(Date),\n        Year = year(Date)\n    )\n\ndkng_yearly &lt;- dkng %&gt;%\n    group_by(Year) %&gt;%\n    summarise(\n        Avg_Close = mean(`Adj Close`, na.rm = TRUE),\n        Volatility = sd(Returns, na.rm = TRUE) * sqrt(252),\n        .groups = \"drop\"\n    )\nattendance_plot &lt;- ggplot(attendance_data, aes(x = Season, y = Total_Attendance / 1e6)) +\n    geom_line(color = \"#006bb6\", size = 1.5) +\n    geom_point(color = \"#006bb6\", size = 3) +\n    annotate(\"rect\",\n        xmin = 2020, xmax = 2021, ymin = 0, ymax = 25,\n        alpha = 0.2, fill = \"#f58426\"\n    ) +\n    annotate(\"text\",\n        x = 2020.5, y = 24,\n        label = \"COVID-19\",\n        size = 4, fontface = \"bold\", color = \"#f58426\"\n    ) +\n    labs(\n        title = \"NBA Attendance Collapse\",\n        x = \"Season\",\n        y = \"Total Attendance (Millions)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        panel.grid.minor = element_blank()\n    ) +\n    scale_y_continuous(labels = scales::comma, limits = c(0, 25))\n\ndkng_plot &lt;- ggplot(dkng_yearly, aes(x = Year, y = Avg_Close)) +\n    geom_line(color = \"#f58426\", size = 1.5) +\n    geom_point(color = \"#f58426\", size = 3) +\n    annotate(\"text\",\n        x = 2020, y = 35,\n        label = \"DKNG IPO\",\n        size = 3.5, fontface = \"bold\", color = \"#f58426\", vjust = 1\n    ) +\n    labs(\n        title = \"DraftKings (DKNG) Stock Price\",\n        x = \"Year\",\n        y = \"Average Adj Close Price ($)\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\", size = 14),\n        panel.grid.minor = element_blank()\n    )\n\ncombined_plot &lt;- attendance_plot | dkng_plot\n\ncombined_plot + plot_annotation(\n    title = \"COVID-19 Impact: Attendance Collapse vs Sports Betting Boom\",\n    theme = theme(\n        plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\")\n    )\n)\n\n\n\n\n\n\n\n\n\nMarch 2020 presented basketball with an unprecedented event. NBA attendance collapsed by 90% virtually overnight as the season was suspended following Rudy Gobert’s positive COVID-19 test. This was followed by the Orlando bubble season with zero fans, and then the 2020–21 campaign with limited capacity. The league’s normal rhythms and fan energy were completely disrupted. However, while the NBA paused, online sports betting exploded. DraftKings went public in April 2020, and its stock price surged as online betting became legalized across more states. If anything, the pandemic accelerated, rather than slowed, the connection between basketball and analytics, as betting markets quickly became the primary way many fans engaged with the sport.",
    "crumbs": [
      "Home",
      "Data Visualization"
    ]
  },
  {
    "objectID": "DLTS.html",
    "href": "DLTS.html",
    "title": "Mamba",
    "section": "",
    "text": "Mamba",
    "crumbs": [
      "Home",
      "Deep Learning for TS"
    ]
  }
]