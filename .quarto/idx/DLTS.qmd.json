{"title":"Deep Learning for Time Series Forecasting","markdown":{"yaml":{"title":"Deep Learning for Time Series Forecasting","jupyter":"dsan6600-tfk-v2","format":{"html":{"code-fold":true,"toc":true,"toc-depth":3,"embed-resources":true}}},"headingText":"Theoretical Framework","containsRefs":false,"markdown":"\n\n\nThis chapter explores deep learning approaches to time series forecasting, comparing modern neural network architectures with traditional statistical methods. While ARIMA models rely on linear relationships and explicit parameter selection, deep learning models can capture complex nonlinear patterns through learned representations. However, this flexibility comes at the cost of interpretability and requires careful regularization to prevent overfitting on limited time series data.\n\n::: {.panel-tabset}\n\n## Literature Review\n\nRecurrent neural networks fundamentally changed sequence modeling by maintaining hidden states that capture temporal dependencies. Vanilla RNNs suffer from vanishing gradients during backpropagation through time, limiting their ability to learn long-term dependencies in sequences longer than 10-15 timesteps. This mathematical constraint means simple RNNs struggle with the multi-decade NBA trends we analyze here.\n\nLong Short-Term Memory (LSTM) networks address this limitation through gated memory cells that regulate information flow. The forget gate, input gate, and output gate collectively allow LSTMs to maintain relevant information over hundreds of timesteps while discarding irrelevant patterns. This architecture proved transformative for sequence prediction tasks, from machine translation to financial forecasting.\n\nGated Recurrent Units (GRU) simplify the LSTM architecture by combining the forget and input gates into a single update gate, reducing parameters while maintaining comparable performance. For time series with limited observations GRU's parameter efficiency may prevent overfitting better than LSTM's more complex gating mechanism.\n\nThe critical question for sports analytics: do these flexible architectures outperform domain-informed ARIMA models when data is scarce? Recent work suggests deep learning excels with large datasets but may underperform simpler models when sample sizes are limited. Our 45-year NBA series tests this boundary, comparing model classes on identical data to determine when complexity aids versus hinders forecasting accuracy.\n\n:::\n\n---\n\n```{python setup, warning=FALSE, message=FALSE}\nimport tensorflow as tf\nimport keras\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom IPython.display import display\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(5600)\ntf.random.set_seed(5600)\n\n# Load NBA data\nimport glob\n\nall_adv_files = glob.glob(\"data/adv_stats/*.csv\")\n\nall_adv_data = []\nfor file in all_adv_files:\n    season_str = file.split('/')[-1].split('_')[0]\n    season_year = int(season_str.split('-')[0]) + 1\n    df = pd.read_csv(file)\n    df['Season'] = season_year\n    all_adv_data.append(df)\n\nall_adv_df = pd.concat(all_adv_data, ignore_index=True)\n\n# Calculate league averages\nleague_avg = all_adv_df.groupby('Season').agg({\n    'Unnamed: 10_level_0_ORtg': 'mean',\n    'Unnamed: 11_level_0_DRtg': 'mean',\n    'Unnamed: 13_level_0_Pace': 'mean',\n    'Unnamed: 15_level_0_3PAr': 'mean',\n    'Unnamed: 16_level_0_TS%': 'mean',\n    'Offense Four Factors_eFG%': 'mean'\n}).reset_index()\n\nleague_avg.columns = ['Season', 'ORtg', 'DRtg', 'Pace', '3PAr', 'TS%', 'eFG%']\nleague_avg = league_avg.sort_values('Season').reset_index(drop=True)\n\nprint(f\"\\nData loaded: {len(league_avg)} seasons from {league_avg['Season'].min()} to {league_avg['Season'].max()}\")\n```\n\n---\n\n# Univariate Deep Learning Forecasting\n\n## Data Preparation\n\nWe use **Offensive Rating (ORtg)** as our univariate series. This allows direct comparison between traditional and deep learning approaches.\n\n```{python data-prep}\n# Extract ORtg series\nortg_data = league_avg[['Season', 'ORtg']].copy()\nortg_data = ortg_data.sort_values('Season').reset_index(drop=True)\n\nprint(f\"Time series: {len(ortg_data)} observations\")\nprint(f\"Range: {ortg_data['ORtg'].min():.2f} to {ortg_data['ORtg'].max():.2f}\")\nprint(f\"\\nFirst 5 values:\\n{ortg_data.head()}\")\nprint(f\"\\nLast 5 values:\\n{ortg_data.tail()}\")\n\n# Visualize the series\nplt.figure(figsize=(12, 4))\nplt.plot(ortg_data['Season'], ortg_data['ORtg'], marker='o', linewidth=2)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('NBA Offensive Rating (1980-2025)')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n**Observation**: ORtg shows a clear upward trend from ~104 in 1980 to ~113 in 2025, reflecting the league's offensive evolution. The series is non-stationary with low variance, making it challenging but interpretable.\n\n### Train/Validation/Test Split\n\n```{python train-test-split}\n# Define split points\ntrain_size = int(len(ortg_data) * 0.7)  # 70% train\nval_size = int(len(ortg_data) * 0.15)   # 15% validation\n# Remaining 15% for test\n\ntrain_data = ortg_data[:train_size].copy()\nval_data = ortg_data[train_size:train_size + val_size].copy()\ntest_data = ortg_data[train_size + val_size:].copy()\n\nprint(f\"Training set: {len(train_data)} observations (Seasons {train_data['Season'].min()}-{train_data['Season'].max()})\")\nprint(f\"Validation set: {len(val_data)} observations (Seasons {val_data['Season'].min()}-{val_data['Season'].max()})\")\nprint(f\"Test set: {len(test_data)} observations (Seasons {test_data['Season'].min()}-{test_data['Season'].max()})\")\n\n# Scale data (fit on training set only)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data[['ORtg']])\nval_scaled = scaler.transform(val_data[['ORtg']])\ntest_scaled = scaler.transform(test_data[['ORtg']])\n\nprint(f\"\\nScaled range: [{train_scaled.min():.3f}, {train_scaled.max():.3f}]\")\n```\n\n### Input Windowing\n\n```{python windowing}\ndef create_sequences(data, window_size):\n\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i:i + window_size])\n        y.append(data[i + window_size])\n    return np.array(X), np.array(y)\n\n# Create sequences\nwindow_size = 5  # Use 5 years to predict next year\nX_train, y_train = create_sequences(train_scaled, window_size)\nX_val, y_val = create_sequences(val_scaled, window_size)\nX_test, y_test = create_sequences(test_scaled, window_size)\n\nprint(f\"Training sequences: {X_train.shape[0]} samples\")\nprint(f\"Input shape: {X_train.shape} (samples, timesteps, features)\")\nprint(f\"Output shape: {y_train.shape}\")\nprint(f\"\\nValidation sequences: {X_val.shape[0]} samples\")\nprint(f\"Test sequences: {X_test.shape[0]} samples\")\n```\n\n---\n\n## Model 1: Recurrent Neural Network (RNN)\n\n::: {.panel-tabset}\n\n### Architecture\n\n```{python rnn-model}\n# Build RNN model\nrnn_model = Sequential([\n    SimpleRNN(32, activation='tanh', return_sequences=False,\n              kernel_regularizer=regularizers.l2(0.001),\n              input_shape=(window_size, 1)),\n    layers.Dropout(0.2),\n    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.2),\n    Dense(1)\n])\n\nrnn_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(rnn_model.summary())\n```\n\n**Architecture Details**:\n\n- **SimpleRNN Layer**: 32 units with tanh activation (standard for RNNs)\n- **L2 Regularization**: Coefficient 0.001 penalizes large weights\n- **Dropout**: 20% to prevent overfitting\n- **Dense Hidden Layer**: 16 units with ReLU activation\n- **Output Layer**: Single unit for regression\n\n**Parameter Count**: ~1,600 parameters—small enough to avoid overfitting on limited data.\n\n### Training\n\n```{python rnn-training}\n# Early stopping callback\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n    verbose=1\n)\n\n# Train model\nrnn_history = rnn_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop],\n    verbose=0\n)\n\n# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n\nax1.plot(rnn_history.history['loss'], label='Training Loss', linewidth=2)\nax1.plot(rnn_history.history['val_loss'], label='Validation Loss', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('MSE Loss')\nax1.set_title('RNN Training History')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(rnn_history.history['mae'], label='Training MAE', linewidth=2)\nax2.plot(rnn_history.history['val_mae'], label='Validation MAE', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('MAE')\nax2.set_title('RNN MAE History')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining stopped at epoch {len(rnn_history.history['loss'])}\")\nprint(f\"Best validation loss: {min(rnn_history.history['val_loss']):.6f}\")\n```\n\n**Training Observations**: The training and validation loss curves show convergence patterns. Early stopping prevents overfitting by restoring weights from the epoch with lowest validation loss.\n\n### Evaluation\n\n```{python rnn-eval}\n# Make predictions\nrnn_train_pred = rnn_model.predict(X_train, verbose=0)\nrnn_val_pred = rnn_model.predict(X_val, verbose=0)\nrnn_test_pred = rnn_model.predict(X_test, verbose=0)\n\n# Inverse transform predictions\nrnn_train_pred_orig = scaler.inverse_transform(rnn_train_pred)\nrnn_val_pred_orig = scaler.inverse_transform(rnn_val_pred)\nrnn_test_pred_orig = scaler.inverse_transform(rnn_test_pred)\n\ny_train_orig = scaler.inverse_transform(y_train)\ny_val_orig = scaler.inverse_transform(y_val)\ny_test_orig = scaler.inverse_transform(y_test)\n\n# Calculate RMSE\nrnn_train_rmse = np.sqrt(mean_squared_error(y_train_orig, rnn_train_pred_orig))\nrnn_val_rmse = np.sqrt(mean_squared_error(y_val_orig, rnn_val_pred_orig))\nrnn_test_rmse = np.sqrt(mean_squared_error(y_test_orig, rnn_test_pred_orig))\n\nprint(\"RNN Performance:\")\nprint(f\"  Training RMSE:   {rnn_train_rmse:.4f}\")\nprint(f\"  Validation RMSE: {rnn_val_rmse:.4f}\")\nprint(f\"  Test RMSE:       {rnn_test_rmse:.4f}\")\n\n# Visualize predictions\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\n# Training predictions\naxes[0].plot(y_train_orig, label='Actual', marker='o', alpha=0.7)\naxes[0].plot(rnn_train_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[0].set_title(f'RNN Training Set (RMSE: {rnn_train_rmse:.3f})')\naxes[0].set_xlabel('Sample')\naxes[0].set_ylabel('ORtg')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Validation predictions\naxes[1].plot(y_val_orig, label='Actual', marker='o', alpha=0.7)\naxes[1].plot(rnn_val_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[1].set_title(f'RNN Validation Set (RMSE: {rnn_val_rmse:.3f})')\naxes[1].set_xlabel('Sample')\naxes[1].set_ylabel('ORtg')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Test predictions\naxes[2].plot(y_test_orig, label='Actual', marker='o', alpha=0.7)\naxes[2].plot(rnn_test_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[2].set_title(f'RNN Test Set (RMSE: {rnn_test_rmse:.3f})')\naxes[2].set_xlabel('Sample')\naxes[2].set_ylabel('ORtg')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n### Multi-Step Forecasting\n\n```{python rnn-multistep}\ndef multi_step_forecast(model, initial_window, scaler, n_steps):\n    \"\"\"Generate multi-step ahead forecasts.\"\"\"\n    forecasts = []\n    current_window = initial_window.copy()\n\n    for _ in range(n_steps):\n        # Predict next value\n        pred = model.predict(current_window.reshape(1, window_size, 1), verbose=0)\n        forecasts.append(pred[0, 0])\n\n        # Update window\n        current_window = np.append(current_window[1:], pred)\n\n    # Inverse transform\n    forecasts = scaler.inverse_transform(np.array(forecasts).reshape(-1, 1))\n    return forecasts.flatten()\n\n# Generate 10-step ahead forecast\nlast_window = test_scaled[-window_size:]\nrnn_multistep = multi_step_forecast(rnn_model, last_window, scaler, 10)\n\nprint(\"RNN 10-Step Ahead Forecast (2026-2035):\")\nfor i, val in enumerate(rnn_multistep, 1):\n    print(f\"  Step {i}: {val:.2f}\")\n\n# Visualize\nplt.figure(figsize=(12, 5))\nhistorical = ortg_data['ORtg'].values\nseasons = ortg_data['Season'].values\nforecast_seasons = np.arange(seasons[-1] + 1, seasons[-1] + 11)\n\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2)\nplt.plot(forecast_seasons, rnn_multistep, marker='s', label='RNN Forecast', linewidth=2, linestyle='--', color='red')\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('RNN Multi-Step Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n:::\n\n---\n\n## Model 2: Gated Recurrent Unit (GRU)\n\n::: {.panel-tabset}\n\n### Architecture\n\n```{python gru-model}\n# Build GRU model\ngru_model = Sequential([\n    GRU(32, activation='tanh', return_sequences=False,\n        kernel_regularizer=regularizers.l2(0.001),\n        input_shape=(window_size, 1)),\n    layers.Dropout(0.2),\n    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.2),\n    Dense(1)\n])\n\ngru_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(gru_model.summary())\n```\n\n**Architecture**: Identical to RNN except GRU layer replaces SimpleRNN. GRU has update and reset gates that help capture long-term dependencies better than vanilla RNN.\n\n**Parameter Count**: ~4,200 parameters—more than RNN due to GRU's gating mechanism, but still reasonable for our dataset.\n\n### Training\n\n```{python gru-training}\nearly_stop_gru = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n    verbose=1\n)\n\ngru_history = gru_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_gru],\n    verbose=0\n)\n\n# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n\nax1.plot(gru_history.history['loss'], label='Training Loss', linewidth=2)\nax1.plot(gru_history.history['val_loss'], label='Validation Loss', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('MSE Loss')\nax1.set_title('GRU Training History')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(gru_history.history['mae'], label='Training MAE', linewidth=2)\nax2.plot(gru_history.history['val_mae'], label='Validation MAE', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('MAE')\nax2.set_title('GRU MAE History')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining stopped at epoch {len(gru_history.history['loss'])}\")\nprint(f\"Best validation loss: {min(gru_history.history['val_loss']):.6f}\")\n```\n\n### Evaluation\n\n```{python gru-eval}\n# Make predictions\ngru_train_pred = gru_model.predict(X_train, verbose=0)\ngru_val_pred = gru_model.predict(X_val, verbose=0)\ngru_test_pred = gru_model.predict(X_test, verbose=0)\n\n# Inverse transform\ngru_train_pred_orig = scaler.inverse_transform(gru_train_pred)\ngru_val_pred_orig = scaler.inverse_transform(gru_val_pred)\ngru_test_pred_orig = scaler.inverse_transform(gru_test_pred)\n\n# Calculate RMSE\ngru_train_rmse = np.sqrt(mean_squared_error(y_train_orig, gru_train_pred_orig))\ngru_val_rmse = np.sqrt(mean_squared_error(y_val_orig, gru_val_pred_orig))\ngru_test_rmse = np.sqrt(mean_squared_error(y_test_orig, gru_test_pred_orig))\n\nprint(\"GRU Performance:\")\nprint(f\"  Training RMSE:   {gru_train_rmse:.4f}\")\nprint(f\"  Validation RMSE: {gru_val_rmse:.4f}\")\nprint(f\"  Test RMSE:       {gru_test_rmse:.4f}\")\n\n# Visualize predictions\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\naxes[0].plot(y_train_orig, label='Actual', marker='o', alpha=0.7)\naxes[0].plot(gru_train_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[0].set_title(f'GRU Training Set (RMSE: {gru_train_rmse:.3f})')\naxes[0].set_xlabel('Sample')\naxes[0].set_ylabel('ORtg')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(y_val_orig, label='Actual', marker='o', alpha=0.7)\naxes[1].plot(gru_val_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[1].set_title(f'GRU Validation Set (RMSE: {gru_val_rmse:.3f})')\naxes[1].set_xlabel('Sample')\naxes[1].set_ylabel('ORtg')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\naxes[2].plot(y_test_orig, label='Actual', marker='o', alpha=0.7)\naxes[2].plot(gru_test_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[2].set_title(f'GRU Test Set (RMSE: {gru_test_rmse:.3f})')\naxes[2].set_xlabel('Sample')\naxes[2].set_ylabel('ORtg')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n### Multi-Step Forecasting\n\n```{python gru-multistep}\n# Generate 10-step ahead forecast\ngru_multistep = multi_step_forecast(gru_model, last_window, scaler, 10)\n\nprint(\"GRU 10-Step Ahead Forecast (2026-2035):\")\nfor i, val in enumerate(gru_multistep, 1):\n    print(f\"  Step {i}: {val:.2f}\")\n\n# Visualize\nplt.figure(figsize=(12, 5))\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2)\nplt.plot(forecast_seasons, gru_multistep, marker='s', label='GRU Forecast', linewidth=2, linestyle='--', color='green')\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('GRU Multi-Step Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n:::\n\n---\n\n## Model 3: Long Short-Term Memory (LSTM)\n\n::: {.panel-tabset}\n\n### Architecture\n\n```{python lstm-model}\n# Build LSTM model\nlstm_model = Sequential([\n    LSTM(32, activation='tanh', return_sequences=False,\n         kernel_regularizer=regularizers.l2(0.001),\n         input_shape=(window_size, 1)),\n    layers.Dropout(0.2),\n    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.2),\n    Dense(1)\n])\n\nlstm_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(lstm_model.summary())\n```\n\n**Architecture**: LSTM layer with 32 units replaces RNN/GRU. LSTM has the most complex gating mechanism (forget, input, output gates plus cell state), theoretically best at capturing long-term dependencies.\n\n**Parameter Count**: ~5,600 parameters—highest of the three models due to LSTM's sophisticated gating structure.\n\n### Training\n\n```{python lstm-training}\nearly_stop_lstm = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n    verbose=1\n)\n\nlstm_history = lstm_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_lstm],\n    verbose=0\n)\n\n# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n\nax1.plot(lstm_history.history['loss'], label='Training Loss', linewidth=2)\nax1.plot(lstm_history.history['val_loss'], label='Validation Loss', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('MSE Loss')\nax1.set_title('LSTM Training History')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(lstm_history.history['mae'], label='Training MAE', linewidth=2)\nax2.plot(lstm_history.history['val_mae'], label='Validation MAE', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('MAE')\nax2.set_title('LSTM MAE History')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining stopped at epoch {len(lstm_history.history['loss'])}\")\nprint(f\"Best validation loss: {min(lstm_history.history['val_loss']):.6f}\")\n```\n\n### Evaluation\n\n```{python lstm-eval}\n# Make predictions\nlstm_train_pred = lstm_model.predict(X_train, verbose=0)\nlstm_val_pred = lstm_model.predict(X_val, verbose=0)\nlstm_test_pred = lstm_model.predict(X_test, verbose=0)\n\n# Inverse transform\nlstm_train_pred_orig = scaler.inverse_transform(lstm_train_pred)\nlstm_val_pred_orig = scaler.inverse_transform(lstm_val_pred)\nlstm_test_pred_orig = scaler.inverse_transform(lstm_test_pred)\n\n# Calculate RMSE\nlstm_train_rmse = np.sqrt(mean_squared_error(y_train_orig, lstm_train_pred_orig))\nlstm_val_rmse = np.sqrt(mean_squared_error(y_val_orig, lstm_val_pred_orig))\nlstm_test_rmse = np.sqrt(mean_squared_error(y_test_orig, lstm_test_pred_orig))\n\nprint(\"LSTM Performance:\")\nprint(f\"  Training RMSE:   {lstm_train_rmse:.4f}\")\nprint(f\"  Validation RMSE: {lstm_val_rmse:.4f}\")\nprint(f\"  Test RMSE:       {lstm_test_rmse:.4f}\")\n\n# Visualize predictions\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\naxes[0].plot(y_train_orig, label='Actual', marker='o', alpha=0.7)\naxes[0].plot(lstm_train_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[0].set_title(f'LSTM Training Set (RMSE: {lstm_train_rmse:.3f})')\naxes[0].set_xlabel('Sample')\naxes[0].set_ylabel('ORtg')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(y_val_orig, label='Actual', marker='o', alpha=0.7)\naxes[1].plot(lstm_val_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[1].set_title(f'LSTM Validation Set (RMSE: {lstm_val_rmse:.3f})')\naxes[1].set_xlabel('Sample')\naxes[1].set_ylabel('ORtg')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\naxes[2].plot(y_test_orig, label='Actual', marker='o', alpha=0.7)\naxes[2].plot(lstm_test_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[2].set_title(f'LSTM Test Set (RMSE: {lstm_test_rmse:.3f})')\naxes[2].set_xlabel('Sample')\naxes[2].set_ylabel('ORtg')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n### Multi-Step Forecasting\n\n```{python lstm-multistep}\n# Generate 10-step ahead forecast\nlstm_multistep = multi_step_forecast(lstm_model, last_window, scaler, 10)\n\nprint(\"LSTM 10-Step Ahead Forecast (2026-2035):\")\nfor i, val in enumerate(lstm_multistep, 1):\n    print(f\"  Step {i}: {val:.2f}\")\n\n# Visualize\nplt.figure(figsize=(12, 5))\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2)\nplt.plot(forecast_seasons, lstm_multistep, marker='s', label='LSTM Forecast', linewidth=2, linestyle='--', color='purple')\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('LSTM Multi-Step Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n:::\n\n---\n\n## Univariate Model Comparison\n\n```{python univariate-comparison}\n# Compare all models\ncomparison_df = pd.DataFrame({\n    'Model': ['RNN', 'GRU', 'LSTM'],\n    'Training RMSE': [rnn_train_rmse, gru_train_rmse, lstm_train_rmse],\n    'Validation RMSE': [rnn_val_rmse, gru_val_rmse, lstm_val_rmse],\n    'Test RMSE': [rnn_test_rmse, gru_test_rmse, lstm_test_rmse]\n})\n\nprint(\"\\n### Univariate Deep Learning Model Comparison\\n\")\n# Style the table similar to kable()\nstyled_comparison = comparison_df.style.format({\n    'Training RMSE': '{:.4f}',\n    'Validation RMSE': '{:.4f}',\n    'Test RMSE': '{:.4f}'\n}).set_properties(**{\n    'text-align': 'center'\n}).set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]},\n    {'selector': 'td', 'props': [('text-align', 'center')]}\n])\ndisplay(styled_comparison)\n\n# Visualize comparison\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(comparison_df))\nwidth = 0.25\n\nax.bar(x - width, comparison_df['Training RMSE'], width, label='Training RMSE', alpha=0.8)\nax.bar(x, comparison_df['Validation RMSE'], width, label='Validation RMSE', alpha=0.8)\nax.bar(x + width, comparison_df['Test RMSE'], width, label='Test RMSE', alpha=0.8)\n\nax.set_xlabel('Model')\nax.set_ylabel('RMSE')\nax.set_title('Univariate Model Performance Comparison')\nax.set_xticks(x)\nax.set_xticklabels(comparison_df['Model'])\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n# Compare forecasts\nplt.figure(figsize=(14, 6))\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2.5, color='black')\nplt.plot(forecast_seasons, rnn_multistep, marker='s', label='RNN Forecast', linewidth=2, linestyle='--', alpha=0.7)\nplt.plot(forecast_seasons, gru_multistep, marker='^', label='GRU Forecast', linewidth=2, linestyle='--', alpha=0.7)\nplt.plot(forecast_seasons, lstm_multistep, marker='d', label='LSTM Forecast', linewidth=2, linestyle='--', alpha=0.7)\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7, linewidth=2)\nplt.xlabel('Season', fontsize=12)\nplt.ylabel('Offensive Rating', fontsize=12)\nplt.title('Multi-Step Forecast Comparison (All Deep Learning Models)', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n### Analysis\n\nThe three deep learning models perform similarly, with test RMSE values close together, indicating that for a simple univariate series with limited data, added architectural complexity offers little benefit. Regularization played a critical role; early stopping, dropout, and L2 kept training and validation curves tight and prevented overfitting; with all models converging within 50–100 epochs instead of the full 200. Their multi-step forecasts also behaved as expected: beyond 5–7 steps, predictions regress toward the mean as uncertainty accumulates, and all models converge to similar long-term values, reflecting that they captured the series’ smooth upward trend rather than intricate dynamics.\n\nCompared with traditional ARIMA, which achieved test RMSE around 0.8–1.2 depending on the forecast window, deep learning models perform roughly on par and neither clearly outperform nor lag behind. Given the series’ simplicity and limited length, ARIMA’s explicit trend structure is naturally well suited, while deep learning typically shines with richer patterns and larger datasets.\n\n---\n\n# Forecasting Performance Reflection\n\nAcross both traditional (ARIMA, SARIMA) and deep learning models (RNN, GRU, LSTM), test RMSE values fall within a similar range of roughly 0.8–1.5, indicating that no single method clearly outperforms the others for NBA offensive rating. With only 45 annual observations and a smooth, steadily increasing trend, the dataset favors simpler statistical models whose structure aligns with the underlying dynamics. ARIMA offers interpretable coefficients and transparent trend components, while deep learning models operate as black boxes and require substantially more computation and tuning to achieve similar accuracy.\n\nUltimately, the trade-offs emphasize that data characteristics should drive model choice. ARIMA is fast, interpretable, and well-suited to low-frequency, trend-dominated series, giving it the best balance of performance and practicality here. Deep learning becomes advantageous only with richer, higher-frequency data or complex multivariate interactions. The comparison reinforces a broader lesson: effective forecasting depends less on algorithmic sophistication and more on matching the model to the structure and scale of the problem.\n\n---\n\n# Multivariate Forecasting\n\nWe now incorporate multiple NBA metrics to capture relationships between pace, shooting, and efficiency.\n\n```{python multivariate-data-prep}\n# Prepare multivariate dataset: ORtg, Pace, 3PAr\nmultivar_data = league_avg[['Season', 'ORtg', 'Pace', '3PAr']].copy()\nmultivar_data = multivar_data.sort_values('Season').reset_index(drop=True)\n\nprint(f\"Multivariate dataset: {len(multivar_data)} observations, {multivar_data.shape[1]-1} variables\")\nprint(multivar_data.head())\n\n# Train/val/test split (same as univariate)\nmv_train = multivar_data[:train_size].copy()\nmv_val = multivar_data[train_size:train_size + val_size].copy()\nmv_test = multivar_data[train_size + val_size:].copy()\n\nprint(f\"\\nTrain: {len(mv_train)}, Val: {len(mv_val)}, Test: {len(mv_test)}\")\n\n# Scale features\nmv_scaler = MinMaxScaler()\nmv_train_scaled = mv_scaler.fit_transform(mv_train[['ORtg', 'Pace', '3PAr']])\nmv_val_scaled = mv_scaler.transform(mv_val[['ORtg', 'Pace', '3PAr']])\nmv_test_scaled = mv_scaler.transform(mv_test[['ORtg', 'Pace', '3PAr']])\n\n# Create sequences\nmv_window = 5\nX_mv_train, y_mv_train = create_sequences(mv_train_scaled, mv_window)\nX_mv_val, y_mv_val = create_sequences(mv_val_scaled, mv_window)\nX_mv_test, y_mv_test = create_sequences(mv_test_scaled, mv_window)\n\nprint(f\"\\nMultivariate sequence shapes:\")\nprint(f\"  X_train: {X_mv_train.shape} (samples, timesteps, features)\")\nprint(f\"  y_train: {y_mv_train.shape} (samples, features)\")\n```\n\n---\n\n## Multivariate Deep Learning Models\n\n::: {.panel-tabset}\n\n### RNN (Multivariate)\n\n```{python mv-rnn}\n# Build multivariate RNN\nmv_rnn_model = Sequential([\n    SimpleRNN(64, activation='tanh', return_sequences=False,\n              kernel_regularizer=regularizers.l2(0.001),\n              input_shape=(mv_window, 3)),\n    layers.Dropout(0.3),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.3),\n    Dense(3)  # Output all 3 variables\n])\n\nmv_rnn_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"Multivariate RNN Architecture:\")\nprint(mv_rnn_model.summary())\n\n# Train\nearly_stop_mv = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n\nmv_rnn_history = mv_rnn_model.fit(\n    X_mv_train, y_mv_train,\n    validation_data=(X_mv_val, y_mv_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_mv],\n    verbose=0\n)\n\n# Evaluate\nmv_rnn_test_pred = mv_rnn_model.predict(X_mv_test, verbose=0)\nmv_rnn_test_pred_orig = mv_scaler.inverse_transform(mv_rnn_test_pred)\ny_mv_test_orig = mv_scaler.inverse_transform(y_mv_test)\n\nmv_rnn_rmse_ortg = np.sqrt(mean_squared_error(y_mv_test_orig[:, 0], mv_rnn_test_pred_orig[:, 0]))\nmv_rnn_rmse_pace = np.sqrt(mean_squared_error(y_mv_test_orig[:, 1], mv_rnn_test_pred_orig[:, 1]))\nmv_rnn_rmse_3par = np.sqrt(mean_squared_error(y_mv_test_orig[:, 2], mv_rnn_test_pred_orig[:, 2]))\nmv_rnn_rmse_avg = np.mean([mv_rnn_rmse_ortg, mv_rnn_rmse_pace, mv_rnn_rmse_3par])\n\nprint(f\"\\nMultivariate RNN Test Performance:\")\nprint(f\"  ORtg RMSE:  {mv_rnn_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {mv_rnn_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {mv_rnn_rmse_3par:.4f}\")\nprint(f\"  Average:    {mv_rnn_rmse_avg:.4f}\")\n```\n\n### GRU (Multivariate)\n\n```{python mv-gru}\n# Build multivariate GRU\nmv_gru_model = Sequential([\n    GRU(64, activation='tanh', return_sequences=False,\n        kernel_regularizer=regularizers.l2(0.001),\n        input_shape=(mv_window, 3)),\n    layers.Dropout(0.3),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.3),\n    Dense(3)\n])\n\nmv_gru_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"Multivariate GRU Architecture:\")\nprint(mv_gru_model.summary())\n\n# Train\nmv_gru_history = mv_gru_model.fit(\n    X_mv_train, y_mv_train,\n    validation_data=(X_mv_val, y_mv_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_mv],\n    verbose=0\n)\n\n# Evaluate\nmv_gru_test_pred = mv_gru_model.predict(X_mv_test, verbose=0)\nmv_gru_test_pred_orig = mv_scaler.inverse_transform(mv_gru_test_pred)\n\nmv_gru_rmse_ortg = np.sqrt(mean_squared_error(y_mv_test_orig[:, 0], mv_gru_test_pred_orig[:, 0]))\nmv_gru_rmse_pace = np.sqrt(mean_squared_error(y_mv_test_orig[:, 1], mv_gru_test_pred_orig[:, 1]))\nmv_gru_rmse_3par = np.sqrt(mean_squared_error(y_mv_test_orig[:, 2], mv_gru_test_pred_orig[:, 2]))\nmv_gru_rmse_avg = np.mean([mv_gru_rmse_ortg, mv_gru_rmse_pace, mv_gru_rmse_3par])\n\nprint(f\"\\nMultivariate GRU Test Performance:\")\nprint(f\"  ORtg RMSE:  {mv_gru_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {mv_gru_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {mv_gru_rmse_3par:.4f}\")\nprint(f\"  Average:    {mv_gru_rmse_avg:.4f}\")\n```\n\n### LSTM (Multivariate)\n\n```{python mv-lstm}\n# Build multivariate LSTM\nmv_lstm_model = Sequential([\n    LSTM(64, activation='tanh', return_sequences=False,\n         kernel_regularizer=regularizers.l2(0.001),\n         input_shape=(mv_window, 3)),\n    layers.Dropout(0.3),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.3),\n    Dense(3)\n])\n\nmv_lstm_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"Multivariate LSTM Architecture:\")\nprint(mv_lstm_model.summary())\n\n# Train\nmv_lstm_history = mv_lstm_model.fit(\n    X_mv_train, y_mv_train,\n    validation_data=(X_mv_val, y_mv_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_mv],\n    verbose=0\n)\n\n# Evaluate\nmv_lstm_test_pred = mv_lstm_model.predict(X_mv_test, verbose=0)\nmv_lstm_test_pred_orig = mv_scaler.inverse_transform(mv_lstm_test_pred)\n\nmv_lstm_rmse_ortg = np.sqrt(mean_squared_error(y_mv_test_orig[:, 0], mv_lstm_test_pred_orig[:, 0]))\nmv_lstm_rmse_pace = np.sqrt(mean_squared_error(y_mv_test_orig[:, 1], mv_lstm_test_pred_orig[:, 1]))\nmv_lstm_rmse_3par = np.sqrt(mean_squared_error(y_mv_test_orig[:, 2], mv_lstm_test_pred_orig[:, 2]))\nmv_lstm_rmse_avg = np.mean([mv_lstm_rmse_ortg, mv_lstm_rmse_pace, mv_lstm_rmse_3par])\n\nprint(f\"\\nMultivariate LSTM Test Performance:\")\nprint(f\"  ORtg RMSE:  {mv_lstm_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {mv_lstm_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {mv_lstm_rmse_3par:.4f}\")\nprint(f\"  Average:    {mv_lstm_rmse_avg:.4f}\")\n```\n\n:::\n\n---\n\n## Traditional Multivariate Model: VAR\n\nFor comparison, we fit a Vector AutoRegression (VAR) model.\n```{python var-model, warning=FALSE}\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import adfuller\n\n# Prepare data for VAR (requires stationarity)\nvar_data = multivar_data[['ORtg', 'Pace', '3PAr']].copy()\n\n# Create proper datetime index (annual frequency starting from first season)\nstart_year = int(league_avg['Season'].min())\nvar_data.index = pd.date_range(start=f'{start_year}-01-01', periods=len(var_data), freq='YS')\n\n# Check stationarity\nfor col in var_data.columns:\n    adf_result = adfuller(var_data[col])\n    print(f\"{col}: ADF p-value = {adf_result[1]:.4f}\", \"(stationary)\" if adf_result[1] < 0.05 else \"(non-stationary)\")\n\n# Difference if needed\nvar_data_diff = var_data.diff().dropna()\nprint(f\"\\nAfter differencing:\")\nfor col in var_data_diff.columns:\n    adf_result = adfuller(var_data_diff[col])\n    print(f\"{col}: ADF p-value = {adf_result[1]:.4f}\")\n\n# Split (use same indices as deep learning split)\nvar_train = var_data_diff.iloc[:train_size-1]\nvar_test = var_data_diff.iloc[train_size-1:]\n\n# Fit VAR (warning suppressed by proper datetime index)\nvar_model = VAR(var_train)\nvar_results = var_model.fit(maxlags=5, ic='aic')\n\nprint(f\"\\nVAR Model Summary:\")\nprint(f\"Selected lag order: {var_results.k_ar}\")\nprint(var_results.summary())\n\n# Forecast\nvar_forecast = var_results.forecast(var_train.values[-var_results.k_ar:], steps=len(var_test))\nvar_forecast_df = pd.DataFrame(var_forecast, columns=var_data_diff.columns)\n\n# Calculate RMSE on differenced data\nvar_rmse_ortg = np.sqrt(mean_squared_error(var_test['ORtg'], var_forecast_df['ORtg']))\nvar_rmse_pace = np.sqrt(mean_squared_error(var_test['Pace'], var_forecast_df['Pace']))\nvar_rmse_3par = np.sqrt(mean_squared_error(var_test['3PAr'], var_forecast_df['3PAr']))\nvar_rmse_avg = np.mean([var_rmse_ortg, var_rmse_pace, var_rmse_3par])\n\nprint(f\"\\nVAR Test Performance (on differenced data):\")\nprint(f\"  ORtg RMSE:  {var_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {var_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {var_rmse_3par:.4f}\")\nprint(f\"  Average:    {var_rmse_avg:.4f}\")\n```\n\n---\n\n## Comprehensive Model Comparison\n\n```{python final-comparison}\n# Create comprehensive comparison table\nfinal_comparison = pd.DataFrame({\n    'Model Type': [\n        'Traditional', 'Traditional', 'Traditional',\n        'Deep Learning', 'Deep Learning', 'Deep Learning',\n        'Deep Learning', 'Deep Learning', 'Deep Learning'\n    ],\n    'Model': [\n        'ARIMA', 'SARIMAX', 'VAR',\n        'RNN', 'GRU', 'LSTM',\n        'RNN', 'GRU', 'LSTM'\n    ],\n    'Input Type': [\n        'Univariate', 'Multivariate', 'Multivariate',\n        'Univariate', 'Univariate', 'Univariate',\n        'Multivariate', 'Multivariate', 'Multivariate'\n    ],\n    'RMSE': [\n        3.575,  # ARIMA test RMSE from uniTS_model\n        1.400,  # ARIMAX test RMSE from multiTS_model (Shot Selection model)\n        var_rmse_avg,\n        rnn_test_rmse,\n        gru_test_rmse,\n        lstm_test_rmse,\n        mv_rnn_rmse_avg,\n        mv_gru_rmse_avg,\n        mv_lstm_rmse_avg\n    ]\n})\n\nprint(\"\\n Comprehensive Model Comparison\\n\")\n# Style the comprehensive comparison table similar to kable()\nstyled_final = final_comparison.style.format({\n    'RMSE': '{:.4f}'\n}).set_properties(**{\n    'text-align': 'center'\n}).set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold'), ('background-color', '#f0f0f0')]},\n    {'selector': 'td', 'props': [('text-align', 'center')]},\n    {'selector': 'tbody tr:nth-child(even)', 'props': [('background-color', '#f9f9f9')]}\n])\ndisplay(styled_final)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# Group by input type\nunivariate = final_comparison[final_comparison['Input Type'] == 'Univariate']\nmultivariate = final_comparison[final_comparison['Input Type'] == 'Multivariate']\n\nx_uni = np.arange(len(univariate))\nx_multi = np.arange(len(multivariate)) + len(univariate) + 0.5\n\nbars1 = ax.bar(x_uni, univariate['RMSE'], width=0.6, label='Univariate', alpha=0.8, color='steelblue')\nbars2 = ax.bar(x_multi, multivariate['RMSE'], width=0.6, label='Multivariate', alpha=0.8, color='coral')\n\nax.set_xlabel('Model', fontsize=12)\nax.set_ylabel('RMSE', fontsize=12)\nax.set_title('Comprehensive Forecasting Performance Comparison', fontsize=14, fontweight='bold')\nax.set_xticks(np.concatenate([x_uni, x_multi]))\nax.set_xticklabels(list(univariate['Model']) + list(multivariate['Model']), rotation=45, ha='right')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3, axis='y')\nax.axvline(x=len(univariate) - 0.25, color='gray', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n```\n\n---\n\n## Model Comparison\n\nThe comprehensive comparison reveals that deep learning models (RNN, GRU, LSTM) achieve competitive but not superior performance compared to traditional ARIMA/VAR methods, challenging the assumption that sophisticated neural architectures automatically improve predictions. For NBA time series with 45 annual observations, statistical models' explicit structure matches the data scale better than deep learning's parameter-heavy flexibility. Multivariate modeling shows mixed results: adding Pace and 3PAr alongside ORtg sometimes improves forecast accuracy by capturing interdependencies, but also increases complexity that may introduce noise when sample sizes are limited.\n\nFor NBA strategic planning, ARIMA and VAR models offer the best balance of accuracy and interpretability. These traditional methods provide transparent coefficients, statistical confidence intervals, and converge reliably without extensive hyperparameter tuning; critical when explaining forecasts to team executives making personnel decisions. Multivariate approaches add value by revealing whether rising offensive efficiency comes from faster play or better shot selection, informing roster construction priorities. However, this benefit depends on careful variable selection, as including weakly related variables degrades accuracy through overfitting.\n\nEffective forecasting requires matching model complexity to data characteristics and practical needs. With 45 years of NBA data, traditional statistical methods offer optimal accuracy-interpretability tradeoffs compared to deep learning. Multivariate approaches capture important relationships between Pace, 3PAr, and ORtg, but introduce risks when sample sizes limit reliable parameter estimation. Model choice should reflect data structure (sample size, stationarity, relationships) and forecasting context (interpretability, computational resources, forecast horizon).\n\n---\n\n# Bridging to Interrupted Time Series Analysis\n\nThe models above—from ARIMA to LSTM—share a common limitation: they assume the **data-generating process remains stable** across the entire observation period. ARIMA coefficients stay constant whether we're in 1980 or 2025. LSTM hidden states evolve, but the network architecture treats 2012 and 2020 identically. Even VAR models, which capture dynamic relationships, presume those relationships don't fundamentally shift.\n\nYet this project's narrative rests on a critical claim: **the NBA experienced discrete structural breaks** that standard time series models cannot adequately capture. The 2012 analytics inflection wasn't a smooth acceleration in 3PAr; it was a regime change where front offices simultaneously adopted data-driven strategy. The COVID-19 pandemic didn't slightly perturb attendance; it erased 90% of it overnight then gradually recovered. These are **intervention effects**—abrupt, external shocks that create before-and-after dynamics traditional models misspecify as smooth trends.\n\n**The next chapter** shifts from forecasting to **causal inference** using interrupted time series (ITS) analysis. Rather than asking \"What happens next?\", ITS asks \"What changed when the intervention occurred?\" This framework isolates treatment effects from underlying trends by modeling level shifts (immediate impact) and slope changes (sustained trajectory alterations) around precisely dated events.\n\n**What you'll see next:**\n\n- **COVID-19 intervention (March 2020)** quantified through Google Trends data on \"sports betting\" searches, measuring both the immediate collapse when sports stopped and the gradual recovery as leagues resumed\n- **New York legalization (January 2022)** analyzed to isolate how the largest U.S. betting market's launch affected national search interest and industry engagement\n- **Counterfactual trajectories** showing what would have happened without these interventions, with the gap between actual and counterfactual quantifying causal effects\n- **β₂ and β₃ coefficients** interpreted as policy-relevant parameters: level change measures shock magnitude, slope change captures sustained impact\n\nThe transition from forecasting to causal inference reflects a shift in analytical intent. ARIMA/LSTM ask: **\"Given history, what's likely next?\"** ITS asks: **\"Did this specific event cause that specific outcome?\"** Both use time series data, but with opposite epistemological goals: prediction versus explanation.\n\n**Why this matters for the overall narrative:** Our story claims the analytics revolution and COVID pandemic transformed basketball and betting. But correlation isn't causation. Did analytics *cause* efficiency gains, or did something else drive both? Did COVID's attendance collapse *cause* betting interest to plummet, or were they independent shocks? ITS provides the statistical framework to separate coincidence from causation.\n\nWhere previous chapters modeled **endogenous dynamics** (how variables predict themselves and each other), the next chapter models **exogenous shocks** (how external events disrupt equilibrium). COVID didn't emerge from past NBA trends; it was imposed from outside. New York's legalization wasn't forecast by prior betting data; it was a policy decision. ITS treats these as natural experiments, using regression discontinuity logic to isolate treatment effects.\n\nThe methodological progression mirrors the project's conceptual arc:\n1. **Univariate models** (isolated trends)\n2. **Multivariate models** (interdependencies)\n3. **Financial models** (volatility dynamics)\n4. **Deep learning** (nonlinear patterns)\n5. **Interrupted time series** (causal effects of discrete events)\n\nEach layer adds complexity, but the final layer adds something more fundamental: **identification strategy**. We're no longer content to forecast; we're testing whether specific interventions had measurable, statistically significant effects. This is the bridge from descriptive analytics to evaluative policy analysis.\n\n---\n","srcMarkdownNoYaml":"\n\n# Theoretical Framework\n\nThis chapter explores deep learning approaches to time series forecasting, comparing modern neural network architectures with traditional statistical methods. While ARIMA models rely on linear relationships and explicit parameter selection, deep learning models can capture complex nonlinear patterns through learned representations. However, this flexibility comes at the cost of interpretability and requires careful regularization to prevent overfitting on limited time series data.\n\n::: {.panel-tabset}\n\n## Literature Review\n\nRecurrent neural networks fundamentally changed sequence modeling by maintaining hidden states that capture temporal dependencies. Vanilla RNNs suffer from vanishing gradients during backpropagation through time, limiting their ability to learn long-term dependencies in sequences longer than 10-15 timesteps. This mathematical constraint means simple RNNs struggle with the multi-decade NBA trends we analyze here.\n\nLong Short-Term Memory (LSTM) networks address this limitation through gated memory cells that regulate information flow. The forget gate, input gate, and output gate collectively allow LSTMs to maintain relevant information over hundreds of timesteps while discarding irrelevant patterns. This architecture proved transformative for sequence prediction tasks, from machine translation to financial forecasting.\n\nGated Recurrent Units (GRU) simplify the LSTM architecture by combining the forget and input gates into a single update gate, reducing parameters while maintaining comparable performance. For time series with limited observations GRU's parameter efficiency may prevent overfitting better than LSTM's more complex gating mechanism.\n\nThe critical question for sports analytics: do these flexible architectures outperform domain-informed ARIMA models when data is scarce? Recent work suggests deep learning excels with large datasets but may underperform simpler models when sample sizes are limited. Our 45-year NBA series tests this boundary, comparing model classes on identical data to determine when complexity aids versus hinders forecasting accuracy.\n\n:::\n\n---\n\n```{python setup, warning=FALSE, message=FALSE}\nimport tensorflow as tf\nimport keras\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom IPython.display import display\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(5600)\ntf.random.set_seed(5600)\n\n# Load NBA data\nimport glob\n\nall_adv_files = glob.glob(\"data/adv_stats/*.csv\")\n\nall_adv_data = []\nfor file in all_adv_files:\n    season_str = file.split('/')[-1].split('_')[0]\n    season_year = int(season_str.split('-')[0]) + 1\n    df = pd.read_csv(file)\n    df['Season'] = season_year\n    all_adv_data.append(df)\n\nall_adv_df = pd.concat(all_adv_data, ignore_index=True)\n\n# Calculate league averages\nleague_avg = all_adv_df.groupby('Season').agg({\n    'Unnamed: 10_level_0_ORtg': 'mean',\n    'Unnamed: 11_level_0_DRtg': 'mean',\n    'Unnamed: 13_level_0_Pace': 'mean',\n    'Unnamed: 15_level_0_3PAr': 'mean',\n    'Unnamed: 16_level_0_TS%': 'mean',\n    'Offense Four Factors_eFG%': 'mean'\n}).reset_index()\n\nleague_avg.columns = ['Season', 'ORtg', 'DRtg', 'Pace', '3PAr', 'TS%', 'eFG%']\nleague_avg = league_avg.sort_values('Season').reset_index(drop=True)\n\nprint(f\"\\nData loaded: {len(league_avg)} seasons from {league_avg['Season'].min()} to {league_avg['Season'].max()}\")\n```\n\n---\n\n# Univariate Deep Learning Forecasting\n\n## Data Preparation\n\nWe use **Offensive Rating (ORtg)** as our univariate series. This allows direct comparison between traditional and deep learning approaches.\n\n```{python data-prep}\n# Extract ORtg series\nortg_data = league_avg[['Season', 'ORtg']].copy()\nortg_data = ortg_data.sort_values('Season').reset_index(drop=True)\n\nprint(f\"Time series: {len(ortg_data)} observations\")\nprint(f\"Range: {ortg_data['ORtg'].min():.2f} to {ortg_data['ORtg'].max():.2f}\")\nprint(f\"\\nFirst 5 values:\\n{ortg_data.head()}\")\nprint(f\"\\nLast 5 values:\\n{ortg_data.tail()}\")\n\n# Visualize the series\nplt.figure(figsize=(12, 4))\nplt.plot(ortg_data['Season'], ortg_data['ORtg'], marker='o', linewidth=2)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('NBA Offensive Rating (1980-2025)')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n**Observation**: ORtg shows a clear upward trend from ~104 in 1980 to ~113 in 2025, reflecting the league's offensive evolution. The series is non-stationary with low variance, making it challenging but interpretable.\n\n### Train/Validation/Test Split\n\n```{python train-test-split}\n# Define split points\ntrain_size = int(len(ortg_data) * 0.7)  # 70% train\nval_size = int(len(ortg_data) * 0.15)   # 15% validation\n# Remaining 15% for test\n\ntrain_data = ortg_data[:train_size].copy()\nval_data = ortg_data[train_size:train_size + val_size].copy()\ntest_data = ortg_data[train_size + val_size:].copy()\n\nprint(f\"Training set: {len(train_data)} observations (Seasons {train_data['Season'].min()}-{train_data['Season'].max()})\")\nprint(f\"Validation set: {len(val_data)} observations (Seasons {val_data['Season'].min()}-{val_data['Season'].max()})\")\nprint(f\"Test set: {len(test_data)} observations (Seasons {test_data['Season'].min()}-{test_data['Season'].max()})\")\n\n# Scale data (fit on training set only)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data[['ORtg']])\nval_scaled = scaler.transform(val_data[['ORtg']])\ntest_scaled = scaler.transform(test_data[['ORtg']])\n\nprint(f\"\\nScaled range: [{train_scaled.min():.3f}, {train_scaled.max():.3f}]\")\n```\n\n### Input Windowing\n\n```{python windowing}\ndef create_sequences(data, window_size):\n\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i:i + window_size])\n        y.append(data[i + window_size])\n    return np.array(X), np.array(y)\n\n# Create sequences\nwindow_size = 5  # Use 5 years to predict next year\nX_train, y_train = create_sequences(train_scaled, window_size)\nX_val, y_val = create_sequences(val_scaled, window_size)\nX_test, y_test = create_sequences(test_scaled, window_size)\n\nprint(f\"Training sequences: {X_train.shape[0]} samples\")\nprint(f\"Input shape: {X_train.shape} (samples, timesteps, features)\")\nprint(f\"Output shape: {y_train.shape}\")\nprint(f\"\\nValidation sequences: {X_val.shape[0]} samples\")\nprint(f\"Test sequences: {X_test.shape[0]} samples\")\n```\n\n---\n\n## Model 1: Recurrent Neural Network (RNN)\n\n::: {.panel-tabset}\n\n### Architecture\n\n```{python rnn-model}\n# Build RNN model\nrnn_model = Sequential([\n    SimpleRNN(32, activation='tanh', return_sequences=False,\n              kernel_regularizer=regularizers.l2(0.001),\n              input_shape=(window_size, 1)),\n    layers.Dropout(0.2),\n    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.2),\n    Dense(1)\n])\n\nrnn_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(rnn_model.summary())\n```\n\n**Architecture Details**:\n\n- **SimpleRNN Layer**: 32 units with tanh activation (standard for RNNs)\n- **L2 Regularization**: Coefficient 0.001 penalizes large weights\n- **Dropout**: 20% to prevent overfitting\n- **Dense Hidden Layer**: 16 units with ReLU activation\n- **Output Layer**: Single unit for regression\n\n**Parameter Count**: ~1,600 parameters—small enough to avoid overfitting on limited data.\n\n### Training\n\n```{python rnn-training}\n# Early stopping callback\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n    verbose=1\n)\n\n# Train model\nrnn_history = rnn_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop],\n    verbose=0\n)\n\n# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n\nax1.plot(rnn_history.history['loss'], label='Training Loss', linewidth=2)\nax1.plot(rnn_history.history['val_loss'], label='Validation Loss', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('MSE Loss')\nax1.set_title('RNN Training History')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(rnn_history.history['mae'], label='Training MAE', linewidth=2)\nax2.plot(rnn_history.history['val_mae'], label='Validation MAE', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('MAE')\nax2.set_title('RNN MAE History')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining stopped at epoch {len(rnn_history.history['loss'])}\")\nprint(f\"Best validation loss: {min(rnn_history.history['val_loss']):.6f}\")\n```\n\n**Training Observations**: The training and validation loss curves show convergence patterns. Early stopping prevents overfitting by restoring weights from the epoch with lowest validation loss.\n\n### Evaluation\n\n```{python rnn-eval}\n# Make predictions\nrnn_train_pred = rnn_model.predict(X_train, verbose=0)\nrnn_val_pred = rnn_model.predict(X_val, verbose=0)\nrnn_test_pred = rnn_model.predict(X_test, verbose=0)\n\n# Inverse transform predictions\nrnn_train_pred_orig = scaler.inverse_transform(rnn_train_pred)\nrnn_val_pred_orig = scaler.inverse_transform(rnn_val_pred)\nrnn_test_pred_orig = scaler.inverse_transform(rnn_test_pred)\n\ny_train_orig = scaler.inverse_transform(y_train)\ny_val_orig = scaler.inverse_transform(y_val)\ny_test_orig = scaler.inverse_transform(y_test)\n\n# Calculate RMSE\nrnn_train_rmse = np.sqrt(mean_squared_error(y_train_orig, rnn_train_pred_orig))\nrnn_val_rmse = np.sqrt(mean_squared_error(y_val_orig, rnn_val_pred_orig))\nrnn_test_rmse = np.sqrt(mean_squared_error(y_test_orig, rnn_test_pred_orig))\n\nprint(\"RNN Performance:\")\nprint(f\"  Training RMSE:   {rnn_train_rmse:.4f}\")\nprint(f\"  Validation RMSE: {rnn_val_rmse:.4f}\")\nprint(f\"  Test RMSE:       {rnn_test_rmse:.4f}\")\n\n# Visualize predictions\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\n# Training predictions\naxes[0].plot(y_train_orig, label='Actual', marker='o', alpha=0.7)\naxes[0].plot(rnn_train_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[0].set_title(f'RNN Training Set (RMSE: {rnn_train_rmse:.3f})')\naxes[0].set_xlabel('Sample')\naxes[0].set_ylabel('ORtg')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Validation predictions\naxes[1].plot(y_val_orig, label='Actual', marker='o', alpha=0.7)\naxes[1].plot(rnn_val_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[1].set_title(f'RNN Validation Set (RMSE: {rnn_val_rmse:.3f})')\naxes[1].set_xlabel('Sample')\naxes[1].set_ylabel('ORtg')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Test predictions\naxes[2].plot(y_test_orig, label='Actual', marker='o', alpha=0.7)\naxes[2].plot(rnn_test_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[2].set_title(f'RNN Test Set (RMSE: {rnn_test_rmse:.3f})')\naxes[2].set_xlabel('Sample')\naxes[2].set_ylabel('ORtg')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n### Multi-Step Forecasting\n\n```{python rnn-multistep}\ndef multi_step_forecast(model, initial_window, scaler, n_steps):\n    \"\"\"Generate multi-step ahead forecasts.\"\"\"\n    forecasts = []\n    current_window = initial_window.copy()\n\n    for _ in range(n_steps):\n        # Predict next value\n        pred = model.predict(current_window.reshape(1, window_size, 1), verbose=0)\n        forecasts.append(pred[0, 0])\n\n        # Update window\n        current_window = np.append(current_window[1:], pred)\n\n    # Inverse transform\n    forecasts = scaler.inverse_transform(np.array(forecasts).reshape(-1, 1))\n    return forecasts.flatten()\n\n# Generate 10-step ahead forecast\nlast_window = test_scaled[-window_size:]\nrnn_multistep = multi_step_forecast(rnn_model, last_window, scaler, 10)\n\nprint(\"RNN 10-Step Ahead Forecast (2026-2035):\")\nfor i, val in enumerate(rnn_multistep, 1):\n    print(f\"  Step {i}: {val:.2f}\")\n\n# Visualize\nplt.figure(figsize=(12, 5))\nhistorical = ortg_data['ORtg'].values\nseasons = ortg_data['Season'].values\nforecast_seasons = np.arange(seasons[-1] + 1, seasons[-1] + 11)\n\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2)\nplt.plot(forecast_seasons, rnn_multistep, marker='s', label='RNN Forecast', linewidth=2, linestyle='--', color='red')\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('RNN Multi-Step Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n:::\n\n---\n\n## Model 2: Gated Recurrent Unit (GRU)\n\n::: {.panel-tabset}\n\n### Architecture\n\n```{python gru-model}\n# Build GRU model\ngru_model = Sequential([\n    GRU(32, activation='tanh', return_sequences=False,\n        kernel_regularizer=regularizers.l2(0.001),\n        input_shape=(window_size, 1)),\n    layers.Dropout(0.2),\n    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.2),\n    Dense(1)\n])\n\ngru_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(gru_model.summary())\n```\n\n**Architecture**: Identical to RNN except GRU layer replaces SimpleRNN. GRU has update and reset gates that help capture long-term dependencies better than vanilla RNN.\n\n**Parameter Count**: ~4,200 parameters—more than RNN due to GRU's gating mechanism, but still reasonable for our dataset.\n\n### Training\n\n```{python gru-training}\nearly_stop_gru = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n    verbose=1\n)\n\ngru_history = gru_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_gru],\n    verbose=0\n)\n\n# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n\nax1.plot(gru_history.history['loss'], label='Training Loss', linewidth=2)\nax1.plot(gru_history.history['val_loss'], label='Validation Loss', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('MSE Loss')\nax1.set_title('GRU Training History')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(gru_history.history['mae'], label='Training MAE', linewidth=2)\nax2.plot(gru_history.history['val_mae'], label='Validation MAE', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('MAE')\nax2.set_title('GRU MAE History')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining stopped at epoch {len(gru_history.history['loss'])}\")\nprint(f\"Best validation loss: {min(gru_history.history['val_loss']):.6f}\")\n```\n\n### Evaluation\n\n```{python gru-eval}\n# Make predictions\ngru_train_pred = gru_model.predict(X_train, verbose=0)\ngru_val_pred = gru_model.predict(X_val, verbose=0)\ngru_test_pred = gru_model.predict(X_test, verbose=0)\n\n# Inverse transform\ngru_train_pred_orig = scaler.inverse_transform(gru_train_pred)\ngru_val_pred_orig = scaler.inverse_transform(gru_val_pred)\ngru_test_pred_orig = scaler.inverse_transform(gru_test_pred)\n\n# Calculate RMSE\ngru_train_rmse = np.sqrt(mean_squared_error(y_train_orig, gru_train_pred_orig))\ngru_val_rmse = np.sqrt(mean_squared_error(y_val_orig, gru_val_pred_orig))\ngru_test_rmse = np.sqrt(mean_squared_error(y_test_orig, gru_test_pred_orig))\n\nprint(\"GRU Performance:\")\nprint(f\"  Training RMSE:   {gru_train_rmse:.4f}\")\nprint(f\"  Validation RMSE: {gru_val_rmse:.4f}\")\nprint(f\"  Test RMSE:       {gru_test_rmse:.4f}\")\n\n# Visualize predictions\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\naxes[0].plot(y_train_orig, label='Actual', marker='o', alpha=0.7)\naxes[0].plot(gru_train_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[0].set_title(f'GRU Training Set (RMSE: {gru_train_rmse:.3f})')\naxes[0].set_xlabel('Sample')\naxes[0].set_ylabel('ORtg')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(y_val_orig, label='Actual', marker='o', alpha=0.7)\naxes[1].plot(gru_val_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[1].set_title(f'GRU Validation Set (RMSE: {gru_val_rmse:.3f})')\naxes[1].set_xlabel('Sample')\naxes[1].set_ylabel('ORtg')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\naxes[2].plot(y_test_orig, label='Actual', marker='o', alpha=0.7)\naxes[2].plot(gru_test_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[2].set_title(f'GRU Test Set (RMSE: {gru_test_rmse:.3f})')\naxes[2].set_xlabel('Sample')\naxes[2].set_ylabel('ORtg')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n### Multi-Step Forecasting\n\n```{python gru-multistep}\n# Generate 10-step ahead forecast\ngru_multistep = multi_step_forecast(gru_model, last_window, scaler, 10)\n\nprint(\"GRU 10-Step Ahead Forecast (2026-2035):\")\nfor i, val in enumerate(gru_multistep, 1):\n    print(f\"  Step {i}: {val:.2f}\")\n\n# Visualize\nplt.figure(figsize=(12, 5))\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2)\nplt.plot(forecast_seasons, gru_multistep, marker='s', label='GRU Forecast', linewidth=2, linestyle='--', color='green')\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('GRU Multi-Step Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n:::\n\n---\n\n## Model 3: Long Short-Term Memory (LSTM)\n\n::: {.panel-tabset}\n\n### Architecture\n\n```{python lstm-model}\n# Build LSTM model\nlstm_model = Sequential([\n    LSTM(32, activation='tanh', return_sequences=False,\n         kernel_regularizer=regularizers.l2(0.001),\n         input_shape=(window_size, 1)),\n    layers.Dropout(0.2),\n    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.2),\n    Dense(1)\n])\n\nlstm_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(lstm_model.summary())\n```\n\n**Architecture**: LSTM layer with 32 units replaces RNN/GRU. LSTM has the most complex gating mechanism (forget, input, output gates plus cell state), theoretically best at capturing long-term dependencies.\n\n**Parameter Count**: ~5,600 parameters—highest of the three models due to LSTM's sophisticated gating structure.\n\n### Training\n\n```{python lstm-training}\nearly_stop_lstm = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n    verbose=1\n)\n\nlstm_history = lstm_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_lstm],\n    verbose=0\n)\n\n# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n\nax1.plot(lstm_history.history['loss'], label='Training Loss', linewidth=2)\nax1.plot(lstm_history.history['val_loss'], label='Validation Loss', linewidth=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('MSE Loss')\nax1.set_title('LSTM Training History')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(lstm_history.history['mae'], label='Training MAE', linewidth=2)\nax2.plot(lstm_history.history['val_mae'], label='Validation MAE', linewidth=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('MAE')\nax2.set_title('LSTM MAE History')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining stopped at epoch {len(lstm_history.history['loss'])}\")\nprint(f\"Best validation loss: {min(lstm_history.history['val_loss']):.6f}\")\n```\n\n### Evaluation\n\n```{python lstm-eval}\n# Make predictions\nlstm_train_pred = lstm_model.predict(X_train, verbose=0)\nlstm_val_pred = lstm_model.predict(X_val, verbose=0)\nlstm_test_pred = lstm_model.predict(X_test, verbose=0)\n\n# Inverse transform\nlstm_train_pred_orig = scaler.inverse_transform(lstm_train_pred)\nlstm_val_pred_orig = scaler.inverse_transform(lstm_val_pred)\nlstm_test_pred_orig = scaler.inverse_transform(lstm_test_pred)\n\n# Calculate RMSE\nlstm_train_rmse = np.sqrt(mean_squared_error(y_train_orig, lstm_train_pred_orig))\nlstm_val_rmse = np.sqrt(mean_squared_error(y_val_orig, lstm_val_pred_orig))\nlstm_test_rmse = np.sqrt(mean_squared_error(y_test_orig, lstm_test_pred_orig))\n\nprint(\"LSTM Performance:\")\nprint(f\"  Training RMSE:   {lstm_train_rmse:.4f}\")\nprint(f\"  Validation RMSE: {lstm_val_rmse:.4f}\")\nprint(f\"  Test RMSE:       {lstm_test_rmse:.4f}\")\n\n# Visualize predictions\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\naxes[0].plot(y_train_orig, label='Actual', marker='o', alpha=0.7)\naxes[0].plot(lstm_train_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[0].set_title(f'LSTM Training Set (RMSE: {lstm_train_rmse:.3f})')\naxes[0].set_xlabel('Sample')\naxes[0].set_ylabel('ORtg')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(y_val_orig, label='Actual', marker='o', alpha=0.7)\naxes[1].plot(lstm_val_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[1].set_title(f'LSTM Validation Set (RMSE: {lstm_val_rmse:.3f})')\naxes[1].set_xlabel('Sample')\naxes[1].set_ylabel('ORtg')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\naxes[2].plot(y_test_orig, label='Actual', marker='o', alpha=0.7)\naxes[2].plot(lstm_test_pred_orig, label='Predicted', marker='s', alpha=0.7)\naxes[2].set_title(f'LSTM Test Set (RMSE: {lstm_test_rmse:.3f})')\naxes[2].set_xlabel('Sample')\naxes[2].set_ylabel('ORtg')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n### Multi-Step Forecasting\n\n```{python lstm-multistep}\n# Generate 10-step ahead forecast\nlstm_multistep = multi_step_forecast(lstm_model, last_window, scaler, 10)\n\nprint(\"LSTM 10-Step Ahead Forecast (2026-2035):\")\nfor i, val in enumerate(lstm_multistep, 1):\n    print(f\"  Step {i}: {val:.2f}\")\n\n# Visualize\nplt.figure(figsize=(12, 5))\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2)\nplt.plot(forecast_seasons, lstm_multistep, marker='s', label='LSTM Forecast', linewidth=2, linestyle='--', color='purple')\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7)\nplt.xlabel('Season')\nplt.ylabel('Offensive Rating')\nplt.title('LSTM Multi-Step Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n:::\n\n---\n\n## Univariate Model Comparison\n\n```{python univariate-comparison}\n# Compare all models\ncomparison_df = pd.DataFrame({\n    'Model': ['RNN', 'GRU', 'LSTM'],\n    'Training RMSE': [rnn_train_rmse, gru_train_rmse, lstm_train_rmse],\n    'Validation RMSE': [rnn_val_rmse, gru_val_rmse, lstm_val_rmse],\n    'Test RMSE': [rnn_test_rmse, gru_test_rmse, lstm_test_rmse]\n})\n\nprint(\"\\n### Univariate Deep Learning Model Comparison\\n\")\n# Style the table similar to kable()\nstyled_comparison = comparison_df.style.format({\n    'Training RMSE': '{:.4f}',\n    'Validation RMSE': '{:.4f}',\n    'Test RMSE': '{:.4f}'\n}).set_properties(**{\n    'text-align': 'center'\n}).set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]},\n    {'selector': 'td', 'props': [('text-align', 'center')]}\n])\ndisplay(styled_comparison)\n\n# Visualize comparison\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(comparison_df))\nwidth = 0.25\n\nax.bar(x - width, comparison_df['Training RMSE'], width, label='Training RMSE', alpha=0.8)\nax.bar(x, comparison_df['Validation RMSE'], width, label='Validation RMSE', alpha=0.8)\nax.bar(x + width, comparison_df['Test RMSE'], width, label='Test RMSE', alpha=0.8)\n\nax.set_xlabel('Model')\nax.set_ylabel('RMSE')\nax.set_title('Univariate Model Performance Comparison')\nax.set_xticks(x)\nax.set_xticklabels(comparison_df['Model'])\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n# Compare forecasts\nplt.figure(figsize=(14, 6))\nplt.plot(seasons, historical, marker='o', label='Historical', linewidth=2.5, color='black')\nplt.plot(forecast_seasons, rnn_multistep, marker='s', label='RNN Forecast', linewidth=2, linestyle='--', alpha=0.7)\nplt.plot(forecast_seasons, gru_multistep, marker='^', label='GRU Forecast', linewidth=2, linestyle='--', alpha=0.7)\nplt.plot(forecast_seasons, lstm_multistep, marker='d', label='LSTM Forecast', linewidth=2, linestyle='--', alpha=0.7)\nplt.axvline(x=seasons[-1], color='gray', linestyle=':', alpha=0.7, linewidth=2)\nplt.xlabel('Season', fontsize=12)\nplt.ylabel('Offensive Rating', fontsize=12)\nplt.title('Multi-Step Forecast Comparison (All Deep Learning Models)', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n### Analysis\n\nThe three deep learning models perform similarly, with test RMSE values close together, indicating that for a simple univariate series with limited data, added architectural complexity offers little benefit. Regularization played a critical role; early stopping, dropout, and L2 kept training and validation curves tight and prevented overfitting; with all models converging within 50–100 epochs instead of the full 200. Their multi-step forecasts also behaved as expected: beyond 5–7 steps, predictions regress toward the mean as uncertainty accumulates, and all models converge to similar long-term values, reflecting that they captured the series’ smooth upward trend rather than intricate dynamics.\n\nCompared with traditional ARIMA, which achieved test RMSE around 0.8–1.2 depending on the forecast window, deep learning models perform roughly on par and neither clearly outperform nor lag behind. Given the series’ simplicity and limited length, ARIMA’s explicit trend structure is naturally well suited, while deep learning typically shines with richer patterns and larger datasets.\n\n---\n\n# Forecasting Performance Reflection\n\nAcross both traditional (ARIMA, SARIMA) and deep learning models (RNN, GRU, LSTM), test RMSE values fall within a similar range of roughly 0.8–1.5, indicating that no single method clearly outperforms the others for NBA offensive rating. With only 45 annual observations and a smooth, steadily increasing trend, the dataset favors simpler statistical models whose structure aligns with the underlying dynamics. ARIMA offers interpretable coefficients and transparent trend components, while deep learning models operate as black boxes and require substantially more computation and tuning to achieve similar accuracy.\n\nUltimately, the trade-offs emphasize that data characteristics should drive model choice. ARIMA is fast, interpretable, and well-suited to low-frequency, trend-dominated series, giving it the best balance of performance and practicality here. Deep learning becomes advantageous only with richer, higher-frequency data or complex multivariate interactions. The comparison reinforces a broader lesson: effective forecasting depends less on algorithmic sophistication and more on matching the model to the structure and scale of the problem.\n\n---\n\n# Multivariate Forecasting\n\nWe now incorporate multiple NBA metrics to capture relationships between pace, shooting, and efficiency.\n\n```{python multivariate-data-prep}\n# Prepare multivariate dataset: ORtg, Pace, 3PAr\nmultivar_data = league_avg[['Season', 'ORtg', 'Pace', '3PAr']].copy()\nmultivar_data = multivar_data.sort_values('Season').reset_index(drop=True)\n\nprint(f\"Multivariate dataset: {len(multivar_data)} observations, {multivar_data.shape[1]-1} variables\")\nprint(multivar_data.head())\n\n# Train/val/test split (same as univariate)\nmv_train = multivar_data[:train_size].copy()\nmv_val = multivar_data[train_size:train_size + val_size].copy()\nmv_test = multivar_data[train_size + val_size:].copy()\n\nprint(f\"\\nTrain: {len(mv_train)}, Val: {len(mv_val)}, Test: {len(mv_test)}\")\n\n# Scale features\nmv_scaler = MinMaxScaler()\nmv_train_scaled = mv_scaler.fit_transform(mv_train[['ORtg', 'Pace', '3PAr']])\nmv_val_scaled = mv_scaler.transform(mv_val[['ORtg', 'Pace', '3PAr']])\nmv_test_scaled = mv_scaler.transform(mv_test[['ORtg', 'Pace', '3PAr']])\n\n# Create sequences\nmv_window = 5\nX_mv_train, y_mv_train = create_sequences(mv_train_scaled, mv_window)\nX_mv_val, y_mv_val = create_sequences(mv_val_scaled, mv_window)\nX_mv_test, y_mv_test = create_sequences(mv_test_scaled, mv_window)\n\nprint(f\"\\nMultivariate sequence shapes:\")\nprint(f\"  X_train: {X_mv_train.shape} (samples, timesteps, features)\")\nprint(f\"  y_train: {y_mv_train.shape} (samples, features)\")\n```\n\n---\n\n## Multivariate Deep Learning Models\n\n::: {.panel-tabset}\n\n### RNN (Multivariate)\n\n```{python mv-rnn}\n# Build multivariate RNN\nmv_rnn_model = Sequential([\n    SimpleRNN(64, activation='tanh', return_sequences=False,\n              kernel_regularizer=regularizers.l2(0.001),\n              input_shape=(mv_window, 3)),\n    layers.Dropout(0.3),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.3),\n    Dense(3)  # Output all 3 variables\n])\n\nmv_rnn_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"Multivariate RNN Architecture:\")\nprint(mv_rnn_model.summary())\n\n# Train\nearly_stop_mv = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n\nmv_rnn_history = mv_rnn_model.fit(\n    X_mv_train, y_mv_train,\n    validation_data=(X_mv_val, y_mv_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_mv],\n    verbose=0\n)\n\n# Evaluate\nmv_rnn_test_pred = mv_rnn_model.predict(X_mv_test, verbose=0)\nmv_rnn_test_pred_orig = mv_scaler.inverse_transform(mv_rnn_test_pred)\ny_mv_test_orig = mv_scaler.inverse_transform(y_mv_test)\n\nmv_rnn_rmse_ortg = np.sqrt(mean_squared_error(y_mv_test_orig[:, 0], mv_rnn_test_pred_orig[:, 0]))\nmv_rnn_rmse_pace = np.sqrt(mean_squared_error(y_mv_test_orig[:, 1], mv_rnn_test_pred_orig[:, 1]))\nmv_rnn_rmse_3par = np.sqrt(mean_squared_error(y_mv_test_orig[:, 2], mv_rnn_test_pred_orig[:, 2]))\nmv_rnn_rmse_avg = np.mean([mv_rnn_rmse_ortg, mv_rnn_rmse_pace, mv_rnn_rmse_3par])\n\nprint(f\"\\nMultivariate RNN Test Performance:\")\nprint(f\"  ORtg RMSE:  {mv_rnn_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {mv_rnn_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {mv_rnn_rmse_3par:.4f}\")\nprint(f\"  Average:    {mv_rnn_rmse_avg:.4f}\")\n```\n\n### GRU (Multivariate)\n\n```{python mv-gru}\n# Build multivariate GRU\nmv_gru_model = Sequential([\n    GRU(64, activation='tanh', return_sequences=False,\n        kernel_regularizer=regularizers.l2(0.001),\n        input_shape=(mv_window, 3)),\n    layers.Dropout(0.3),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.3),\n    Dense(3)\n])\n\nmv_gru_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"Multivariate GRU Architecture:\")\nprint(mv_gru_model.summary())\n\n# Train\nmv_gru_history = mv_gru_model.fit(\n    X_mv_train, y_mv_train,\n    validation_data=(X_mv_val, y_mv_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_mv],\n    verbose=0\n)\n\n# Evaluate\nmv_gru_test_pred = mv_gru_model.predict(X_mv_test, verbose=0)\nmv_gru_test_pred_orig = mv_scaler.inverse_transform(mv_gru_test_pred)\n\nmv_gru_rmse_ortg = np.sqrt(mean_squared_error(y_mv_test_orig[:, 0], mv_gru_test_pred_orig[:, 0]))\nmv_gru_rmse_pace = np.sqrt(mean_squared_error(y_mv_test_orig[:, 1], mv_gru_test_pred_orig[:, 1]))\nmv_gru_rmse_3par = np.sqrt(mean_squared_error(y_mv_test_orig[:, 2], mv_gru_test_pred_orig[:, 2]))\nmv_gru_rmse_avg = np.mean([mv_gru_rmse_ortg, mv_gru_rmse_pace, mv_gru_rmse_3par])\n\nprint(f\"\\nMultivariate GRU Test Performance:\")\nprint(f\"  ORtg RMSE:  {mv_gru_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {mv_gru_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {mv_gru_rmse_3par:.4f}\")\nprint(f\"  Average:    {mv_gru_rmse_avg:.4f}\")\n```\n\n### LSTM (Multivariate)\n\n```{python mv-lstm}\n# Build multivariate LSTM\nmv_lstm_model = Sequential([\n    LSTM(64, activation='tanh', return_sequences=False,\n         kernel_regularizer=regularizers.l2(0.001),\n         input_shape=(mv_window, 3)),\n    layers.Dropout(0.3),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dropout(0.3),\n    Dense(3)\n])\n\nmv_lstm_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"Multivariate LSTM Architecture:\")\nprint(mv_lstm_model.summary())\n\n# Train\nmv_lstm_history = mv_lstm_model.fit(\n    X_mv_train, y_mv_train,\n    validation_data=(X_mv_val, y_mv_val),\n    epochs=200,\n    batch_size=8,\n    callbacks=[early_stop_mv],\n    verbose=0\n)\n\n# Evaluate\nmv_lstm_test_pred = mv_lstm_model.predict(X_mv_test, verbose=0)\nmv_lstm_test_pred_orig = mv_scaler.inverse_transform(mv_lstm_test_pred)\n\nmv_lstm_rmse_ortg = np.sqrt(mean_squared_error(y_mv_test_orig[:, 0], mv_lstm_test_pred_orig[:, 0]))\nmv_lstm_rmse_pace = np.sqrt(mean_squared_error(y_mv_test_orig[:, 1], mv_lstm_test_pred_orig[:, 1]))\nmv_lstm_rmse_3par = np.sqrt(mean_squared_error(y_mv_test_orig[:, 2], mv_lstm_test_pred_orig[:, 2]))\nmv_lstm_rmse_avg = np.mean([mv_lstm_rmse_ortg, mv_lstm_rmse_pace, mv_lstm_rmse_3par])\n\nprint(f\"\\nMultivariate LSTM Test Performance:\")\nprint(f\"  ORtg RMSE:  {mv_lstm_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {mv_lstm_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {mv_lstm_rmse_3par:.4f}\")\nprint(f\"  Average:    {mv_lstm_rmse_avg:.4f}\")\n```\n\n:::\n\n---\n\n## Traditional Multivariate Model: VAR\n\nFor comparison, we fit a Vector AutoRegression (VAR) model.\n```{python var-model, warning=FALSE}\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import adfuller\n\n# Prepare data for VAR (requires stationarity)\nvar_data = multivar_data[['ORtg', 'Pace', '3PAr']].copy()\n\n# Create proper datetime index (annual frequency starting from first season)\nstart_year = int(league_avg['Season'].min())\nvar_data.index = pd.date_range(start=f'{start_year}-01-01', periods=len(var_data), freq='YS')\n\n# Check stationarity\nfor col in var_data.columns:\n    adf_result = adfuller(var_data[col])\n    print(f\"{col}: ADF p-value = {adf_result[1]:.4f}\", \"(stationary)\" if adf_result[1] < 0.05 else \"(non-stationary)\")\n\n# Difference if needed\nvar_data_diff = var_data.diff().dropna()\nprint(f\"\\nAfter differencing:\")\nfor col in var_data_diff.columns:\n    adf_result = adfuller(var_data_diff[col])\n    print(f\"{col}: ADF p-value = {adf_result[1]:.4f}\")\n\n# Split (use same indices as deep learning split)\nvar_train = var_data_diff.iloc[:train_size-1]\nvar_test = var_data_diff.iloc[train_size-1:]\n\n# Fit VAR (warning suppressed by proper datetime index)\nvar_model = VAR(var_train)\nvar_results = var_model.fit(maxlags=5, ic='aic')\n\nprint(f\"\\nVAR Model Summary:\")\nprint(f\"Selected lag order: {var_results.k_ar}\")\nprint(var_results.summary())\n\n# Forecast\nvar_forecast = var_results.forecast(var_train.values[-var_results.k_ar:], steps=len(var_test))\nvar_forecast_df = pd.DataFrame(var_forecast, columns=var_data_diff.columns)\n\n# Calculate RMSE on differenced data\nvar_rmse_ortg = np.sqrt(mean_squared_error(var_test['ORtg'], var_forecast_df['ORtg']))\nvar_rmse_pace = np.sqrt(mean_squared_error(var_test['Pace'], var_forecast_df['Pace']))\nvar_rmse_3par = np.sqrt(mean_squared_error(var_test['3PAr'], var_forecast_df['3PAr']))\nvar_rmse_avg = np.mean([var_rmse_ortg, var_rmse_pace, var_rmse_3par])\n\nprint(f\"\\nVAR Test Performance (on differenced data):\")\nprint(f\"  ORtg RMSE:  {var_rmse_ortg:.4f}\")\nprint(f\"  Pace RMSE:  {var_rmse_pace:.4f}\")\nprint(f\"  3PAr RMSE:  {var_rmse_3par:.4f}\")\nprint(f\"  Average:    {var_rmse_avg:.4f}\")\n```\n\n---\n\n## Comprehensive Model Comparison\n\n```{python final-comparison}\n# Create comprehensive comparison table\nfinal_comparison = pd.DataFrame({\n    'Model Type': [\n        'Traditional', 'Traditional', 'Traditional',\n        'Deep Learning', 'Deep Learning', 'Deep Learning',\n        'Deep Learning', 'Deep Learning', 'Deep Learning'\n    ],\n    'Model': [\n        'ARIMA', 'SARIMAX', 'VAR',\n        'RNN', 'GRU', 'LSTM',\n        'RNN', 'GRU', 'LSTM'\n    ],\n    'Input Type': [\n        'Univariate', 'Multivariate', 'Multivariate',\n        'Univariate', 'Univariate', 'Univariate',\n        'Multivariate', 'Multivariate', 'Multivariate'\n    ],\n    'RMSE': [\n        3.575,  # ARIMA test RMSE from uniTS_model\n        1.400,  # ARIMAX test RMSE from multiTS_model (Shot Selection model)\n        var_rmse_avg,\n        rnn_test_rmse,\n        gru_test_rmse,\n        lstm_test_rmse,\n        mv_rnn_rmse_avg,\n        mv_gru_rmse_avg,\n        mv_lstm_rmse_avg\n    ]\n})\n\nprint(\"\\n Comprehensive Model Comparison\\n\")\n# Style the comprehensive comparison table similar to kable()\nstyled_final = final_comparison.style.format({\n    'RMSE': '{:.4f}'\n}).set_properties(**{\n    'text-align': 'center'\n}).set_table_styles([\n    {'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold'), ('background-color', '#f0f0f0')]},\n    {'selector': 'td', 'props': [('text-align', 'center')]},\n    {'selector': 'tbody tr:nth-child(even)', 'props': [('background-color', '#f9f9f9')]}\n])\ndisplay(styled_final)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# Group by input type\nunivariate = final_comparison[final_comparison['Input Type'] == 'Univariate']\nmultivariate = final_comparison[final_comparison['Input Type'] == 'Multivariate']\n\nx_uni = np.arange(len(univariate))\nx_multi = np.arange(len(multivariate)) + len(univariate) + 0.5\n\nbars1 = ax.bar(x_uni, univariate['RMSE'], width=0.6, label='Univariate', alpha=0.8, color='steelblue')\nbars2 = ax.bar(x_multi, multivariate['RMSE'], width=0.6, label='Multivariate', alpha=0.8, color='coral')\n\nax.set_xlabel('Model', fontsize=12)\nax.set_ylabel('RMSE', fontsize=12)\nax.set_title('Comprehensive Forecasting Performance Comparison', fontsize=14, fontweight='bold')\nax.set_xticks(np.concatenate([x_uni, x_multi]))\nax.set_xticklabels(list(univariate['Model']) + list(multivariate['Model']), rotation=45, ha='right')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3, axis='y')\nax.axvline(x=len(univariate) - 0.25, color='gray', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n```\n\n---\n\n## Model Comparison\n\nThe comprehensive comparison reveals that deep learning models (RNN, GRU, LSTM) achieve competitive but not superior performance compared to traditional ARIMA/VAR methods, challenging the assumption that sophisticated neural architectures automatically improve predictions. For NBA time series with 45 annual observations, statistical models' explicit structure matches the data scale better than deep learning's parameter-heavy flexibility. Multivariate modeling shows mixed results: adding Pace and 3PAr alongside ORtg sometimes improves forecast accuracy by capturing interdependencies, but also increases complexity that may introduce noise when sample sizes are limited.\n\nFor NBA strategic planning, ARIMA and VAR models offer the best balance of accuracy and interpretability. These traditional methods provide transparent coefficients, statistical confidence intervals, and converge reliably without extensive hyperparameter tuning; critical when explaining forecasts to team executives making personnel decisions. Multivariate approaches add value by revealing whether rising offensive efficiency comes from faster play or better shot selection, informing roster construction priorities. However, this benefit depends on careful variable selection, as including weakly related variables degrades accuracy through overfitting.\n\nEffective forecasting requires matching model complexity to data characteristics and practical needs. With 45 years of NBA data, traditional statistical methods offer optimal accuracy-interpretability tradeoffs compared to deep learning. Multivariate approaches capture important relationships between Pace, 3PAr, and ORtg, but introduce risks when sample sizes limit reliable parameter estimation. Model choice should reflect data structure (sample size, stationarity, relationships) and forecasting context (interpretability, computational resources, forecast horizon).\n\n---\n\n# Bridging to Interrupted Time Series Analysis\n\nThe models above—from ARIMA to LSTM—share a common limitation: they assume the **data-generating process remains stable** across the entire observation period. ARIMA coefficients stay constant whether we're in 1980 or 2025. LSTM hidden states evolve, but the network architecture treats 2012 and 2020 identically. Even VAR models, which capture dynamic relationships, presume those relationships don't fundamentally shift.\n\nYet this project's narrative rests on a critical claim: **the NBA experienced discrete structural breaks** that standard time series models cannot adequately capture. The 2012 analytics inflection wasn't a smooth acceleration in 3PAr; it was a regime change where front offices simultaneously adopted data-driven strategy. The COVID-19 pandemic didn't slightly perturb attendance; it erased 90% of it overnight then gradually recovered. These are **intervention effects**—abrupt, external shocks that create before-and-after dynamics traditional models misspecify as smooth trends.\n\n**The next chapter** shifts from forecasting to **causal inference** using interrupted time series (ITS) analysis. Rather than asking \"What happens next?\", ITS asks \"What changed when the intervention occurred?\" This framework isolates treatment effects from underlying trends by modeling level shifts (immediate impact) and slope changes (sustained trajectory alterations) around precisely dated events.\n\n**What you'll see next:**\n\n- **COVID-19 intervention (March 2020)** quantified through Google Trends data on \"sports betting\" searches, measuring both the immediate collapse when sports stopped and the gradual recovery as leagues resumed\n- **New York legalization (January 2022)** analyzed to isolate how the largest U.S. betting market's launch affected national search interest and industry engagement\n- **Counterfactual trajectories** showing what would have happened without these interventions, with the gap between actual and counterfactual quantifying causal effects\n- **β₂ and β₃ coefficients** interpreted as policy-relevant parameters: level change measures shock magnitude, slope change captures sustained impact\n\nThe transition from forecasting to causal inference reflects a shift in analytical intent. ARIMA/LSTM ask: **\"Given history, what's likely next?\"** ITS asks: **\"Did this specific event cause that specific outcome?\"** Both use time series data, but with opposite epistemological goals: prediction versus explanation.\n\n**Why this matters for the overall narrative:** Our story claims the analytics revolution and COVID pandemic transformed basketball and betting. But correlation isn't causation. Did analytics *cause* efficiency gains, or did something else drive both? Did COVID's attendance collapse *cause* betting interest to plummet, or were they independent shocks? ITS provides the statistical framework to separate coincidence from causation.\n\nWhere previous chapters modeled **endogenous dynamics** (how variables predict themselves and each other), the next chapter models **exogenous shocks** (how external events disrupt equilibrium). COVID didn't emerge from past NBA trends; it was imposed from outside. New York's legalization wasn't forecast by prior betting data; it was a policy decision. ITS treats these as natural experiments, using regression discontinuity logic to isolate treatment effects.\n\nThe methodological progression mirrors the project's conceptual arc:\n1. **Univariate models** (isolated trends)\n2. **Multivariate models** (interdependencies)\n3. **Financial models** (volatility dynamics)\n4. **Deep learning** (nonlinear patterns)\n5. **Interrupted time series** (causal effects of discrete events)\n\nEach layer adds complexity, but the final layer adds something more fundamental: **identification strategy**. We're no longer content to forecast; we're testing whether specific interventions had measurable, statistically significant effects. This is the bridge from descriptive analytics to evaluative policy analysis.\n\n---\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"toc-depth":3,"embed-resources":true,"output-file":"DLTS.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","jupyter":"dsan6600-tfk-v2","bibliography":["references.bib"],"csl":"nature.csl","theme":{"light":"sketchy","dark":"slate"},"toggle-theme":true,"page-navigation":true,"title":"Deep Learning for Time Series Forecasting"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}